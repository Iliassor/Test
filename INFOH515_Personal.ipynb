{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Ls1jlNnPhejtOHHAii7SC5veUPHUrIzR",
      "authorship_tag": "ABX9TyM0F7Fsb/VujWXZ6PscvCky",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Iliassor/Test/blob/main/INFOH515_Personal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf\n",
        "!pip install nltk\n",
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "import fitz  # PyMuPDF\n",
        "import re\n",
        "import spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRFP9oqUWrsk",
        "outputId": "59af37bb-66ee-42ed-8bde-7104459a1727"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.11/dist-packages (1.25.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Parameters ===\n",
        "pdf_path = \"/content/drive/MyDrive/Input Data/Dimitris Sacharidis/1. Introduction and Map Reduce/1-intro.pdf\"\n",
        "chunk_size = 200  # can change to 200–500 later\n",
        "output_text_path = \"/content/drive/MyDrive/Output Data/1-intro_cleaned_chunks.txt\""
      ],
      "metadata": {
        "id": "wZdU5zAnQci-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_pymupdf(filepath):\n",
        "    doc = fitz.open(filepath)\n",
        "    all_text = []\n",
        "    for page in doc:\n",
        "        text = page.get_text(\"text\")  # \"text\" keeps layout\n",
        "        if text:\n",
        "            all_text.append(text)\n",
        "    return \"\\n\".join(all_text)"
      ],
      "metadata": {
        "id": "5xUuo2ShW4ty"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text = extract_text_pymupdf(pdf_path)\n",
        "print(raw_text)  # print first 1000 characters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvmoWkm1ukaK",
        "outputId": "706d9d50-d1c1-4163-c2a8-794cebe0d17e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO-H515: BIG DATA: DISTRIBUTED MANAGEMENT\n",
            "Lecture 1: Introduction, HDFS, Map/Reduce\n",
            "Dimitris Sacharidis\n",
            "\n",
            "LECTURE OUTLINE\n",
            "What is Big Data?\n",
            "How is Big Data Used?\n",
            "Big Datacenters\n",
            "HDFS\n",
            "Map/Reduce\n",
            "1\n",
            "\n",
            "WHAT IS BIG DATA?\n",
            "\n",
            "“Big data is like teenage sex:\n",
            "everyone talks about it,\n",
            "nobody really knows how to do it,\n",
            "everyone thinks everyone else is doing it,\n",
            "so everyone claims they are doing it …”\n",
            "—Dan Ariely\n",
            "3\n",
            "\n",
            "BIG DATA\n",
            "DATA ANALYTICS\n",
            "DATA SCIENCE\n",
            "THE BIG THREE\n",
            "4\n",
            "\n",
            "BIG DATA\n",
            "THE BIG THREE\n",
            "4\n",
            "\n",
            "BIG DATA\n",
            "Some History\n",
            "THE BIG THREE\n",
            "4\n",
            "\n",
            "Source: The Economist, \n",
            "February 25, 2010\n",
            "\n",
            "The Data Deluge\n",
            "“ EIGHTEEN months ago, Li & Fung, a firm that manages supply chains \n",
            "for retailers, saw 100 gigabytes of information flow through its \n",
            "network each day. Now the amount has increased tenfold. During \n",
            "2009, American drone aircraft flying over Iraq and Afghanistan sent \n",
            "back around 24 years' worth of video footage. New models being \n",
            "deployed this year will produce ten times as many data streams as \n",
            "their predecessors, and those in 2011 will produce 30 times as many.\n",
            "Source: The Economist, \n",
            "February 25, 2010\n",
            "\n",
            "The Data Deluge\n",
            "“ Everywhere you look, the quantity of information in the world is \n",
            "soaring. According to one estimate, mankind created 150 exabytes \n",
            "(billion gigabytes) of data in 2005. This year, it will create 1,200 \n",
            "exabytes. Merely keeping up with this flood, and storing the bits that \n",
            "might be useful, is difficult enough. Analysing it, to spot patterns and \n",
            "extract useful information, is harder still.\n",
            "Source: The Economist, \n",
            "February 25, 2010\n",
            "\n",
            "The Data Deluge\n",
            "“ By the 2030s, computer data storage may surpass 1 yottabyte (1024), the \n",
            "largest number with an official metric prefix.\n",
            "The International Bureau of Weights and Measures (BIPM) in Paris \n",
            "recommends new names—ronna and quecca—as prefixes for 1027 and 1030.\n",
            "1 gigabyte  = 109 bytes\n",
            "1 terabyte  = 1012 bytes\n",
            "1 petabyte = 1015 bytes\n",
            "1 exabyte   = 1018 bytes\n",
            "1 zetabyte   = 1021 bytes\n",
            "1 yottabyte   = 1024 bytes\n",
            "1 ronnabyte   = 1027 bytes\n",
            "1 queccabyte   = 1030 bytes\n",
            "https://www.science.org/content/article/you-know-kilo-mega-and-giga-metric-system-ready-ronna-and-quecca\n",
            "\n",
            "Is this really problematic?\n",
            "Assume you’re google at the beginning of the 2000’s \n",
            "and you want to index the web\n",
            "\n",
            "Is this really problematic?\n",
            "“ Although it is hard to accurately determine the size of the Web at any \n",
            "point in time, it is safe to say that it consists of hundreds of billions of \n",
            "individual documents and that it continues to grow. If we assume the \n",
            "Web to contain 100 billion documents, with an average document size \n",
            "of 4 kB (after compression), the Web is about 400 TB …\n",
            "… The size of the resulting inverted (web search) index depends on the \n",
            "specific implementation, but it tends to be on the same order of \n",
            "magnitude as the original repository. \n",
            "Source: The Datacenter as a Computer\n",
            "               Luiz André Barroso (Google)\n",
            "               Jimmy Clidaras (Google)\n",
            "               Urs Hölzle (Google)\n",
            " \n",
            "\n",
            "BIG DATA: SOME DEFINITIONS (1/4)\n",
            "Big Data is a term encompassing the use of techniques to capture, pro-\n",
            "cess, analyse and visualize potentially large datasets in a reasonable\n",
            "timeframe not accessible to standard IT technologies. By extension,\n",
            "the platform, tools and software used for this purpose are collectively\n",
            "called Big Data technologies.\n",
            "“\n",
            "– Nessi1\n",
            "1The European Technology Platform dedicated to Software, Services and Data\n",
            "5\n",
            "\n",
            "BIG DATA: SOME DEFINITIONS (2/4)\n",
            "Big data refers to datasets whose size is beyond the ability of typical\n",
            "database software tools to capture, store, manage, and analyze. This\n",
            "definition is intentionally subjective and incorporates a moving defi-\n",
            "nition of how big a dataset needs to be in order to be considered big\n",
            "data … We assume that, as technology advances over time, the size of\n",
            "datasets that qualify as big data will also increase.\n",
            "“\n",
            "– MCKinsey Global Institute 2\n",
            "2Big data: The next frontier for innovation, competition, and productivity, June 2011\n",
            "6\n",
            "\n",
            "BIG DATA: SOME DEFINITIONS (3/4)\n",
            "Big data are data sets that are so voluminous and complex that tra-\n",
            "ditional data processing application software are inadequate to deal\n",
            "with them. Big data challenges include capturing data, data storage,\n",
            "data analysis, search, sharing, transfer, visualization, querying, updat-\n",
            "ing and information privacy.\n",
            "Lately, the term big data tends to refer to the use of predictive analyt-\n",
            "ics, user behavior analytics, or certain other advanced data analytics\n",
            "methods that extract value from data, and seldom to a particular size\n",
            "of data set.\n",
            "“\n",
            "—Wikipedia\n",
            "7\n",
            "\n",
            "BIG DATA: SOME DEFINITIONS (4/4)\n",
            "Big data refers to extremely large datasets that cannot be easily man-\n",
            "aged, processed, or analyzed using traditional data processing meth-\n",
            "ods. It is characterized by the three V’s: volume, variety, and velocity,\n",
            "which describe the sheer amount of data, the diversity of data types,\n",
            "and the speed at which it is generated. Big data technologies enable\n",
            "businesses and researchers to gain insights and make data-driven de-\n",
            "cisions from these vast, dynamic datasets.\n",
            "“\n",
            "—ChatGPT (02/2025)\n",
            "8\n",
            "\n",
            "BIG DATA\n",
            "DATA ANALYTICS\n",
            "DATA SCIENCE\n",
            "THE BIG THREE\n",
            "9\n",
            "\n",
            "DATA ANALYTICS\n",
            "THE BIG THREE\n",
            "9\n",
            "\n",
            "DATA ANALYTICS: A DEFINITION\n",
            "Data analysis, also known as analysis of data or data analytics, is\n",
            "a process of inspecting, cleansing, transforming, and modeling data\n",
            "with the goal of discovering useful information, suggesting conclu-\n",
            "sions, and supporting decision-making.\n",
            "“\n",
            "—Wikipedia\n",
            "10\n",
            "\n",
            "11\n",
            "\n",
            "BIG DATA\n",
            "DATA ANALYTICS\n",
            "DATA SCIENCE\n",
            "THE BIG THREE\n",
            "12\n",
            "\n",
            "DATA SCIENCE\n",
            "THE BIG THREE\n",
            "12\n",
            "\n",
            "DATA SCIENCE: SOME DEFINITIONS (1/3)\n",
            "Data science is the study of the generalizable extraction of knowledge\n",
            "from data.\n",
            "“\n",
            "—Vasant Dhar3\n",
            "From the article:\n",
            "• The term science implies knowledge gained through systematic study.\n",
            "• A data scientist requires an integrated skill set spanning mathematics,\n",
            "machine learning, artificial intelligence, statistics, databases, and\n",
            "optimization, along with a deep understanding of the craft of problem\n",
            "formulation to engineer effective solutions.\n",
            "3Data Science and Predictions. Communications of the ACM 56(12), 2014.\n",
            "13\n",
            "\n",
            "DATA SCIENCE: SOME DEFINITIONS (2/3)\n",
            "Data science, also known as data-driven science, is an interdisci-\n",
            "plinary field about scientific methods, processes, and systems to\n",
            "extract knowledge or insights from data in various forms, either\n",
            "structured or unstructured.\n",
            "Data science is a ”concept to unify statistics, data analysis and their\n",
            "related methods” in order to ”understand and analyze actual phenom-\n",
            "ena” with data. It employs techniques and theories drawn from many\n",
            "fields within the broad areas of mathematics, statistics, information sci-\n",
            "ence, and computer science.\n",
            "“\n",
            "—Wikipedia\n",
            "14\n",
            "\n",
            "DATA SCIENCE: SOME DEFINITIONS (3/3)\n",
            "Data science is the field that combines statistical analysis, machine\n",
            "learning, and data processing techniques to extract valuable insights\n",
            "from large and complex datasets.\n",
            "It involves collecting, cleaning,\n",
            "analysing, and interpreting data to inform decision-making and solve\n",
            "problems across various industries. Data scientists use programming,\n",
            "mathematics, and domain expertise to uncover patterns and trends in\n",
            "data.\n",
            "“\n",
            "—ChatGPT (02/2025)\n",
            "15\n",
            "\n",
            "CONCLUSION\n",
            "16\n",
            "Recall the course objective:\n",
            "Introduce the fundamental notions,\n",
            "principles, and research results concerning\n",
            "modern, scalable, and fault-tolerant ways for\n",
            "managing and analyzing massive amounts of\n",
            "data using parallel and distributed systems.\n",
            "The course is hence concerned with Big Data\n",
            "and Big Data Analytics, and thus relevant for\n",
            "Data Science.\n",
            "\n",
            "THE CHALLENGES OF BIG DATA\n",
            "Source: http://www.ibmbigdatahub.com/sites/default/files/infographic_file/\n",
            "17\n",
            "\n",
            "HOW IS BIG DATA USED?\n",
            "\n",
            "HOW IS BIG DATA USED ?\n",
            "Slides taken from https://www.slideshare.net/Dell/big-data-use-cases-3601989\n",
            "19\n",
            "\n",
            "1. Optimize Funnel \n",
            "Conversion \n",
            "2. Behavioral \n",
            "Analytics\n",
            "3. Customer \n",
            "Segmentation\n",
            "4. Predictive \n",
            "Support\n",
            "5. Market Basket \n",
            "Analysis and Pricing \n",
            "Optimization\n",
            "6. Predict Security \n",
            "Threats\n",
            "7. Fraud Detection\n",
            "8. industry specific\n",
            "BIG DATA USE CASES:\n",
            "\n",
            "Big data analytics allows \n",
            "companies to track leads through \n",
            "the entire sales conversion process, \n",
            "from a click on an adword ad to the \n",
            "final transaction, in order to uncover \n",
            "insights on how the conversion \n",
            "process can be improved.\n",
            "1. OPTIMIZE FUNNEL\n",
            "CONVERSION\n",
            "\n",
            "TYPE\n",
            "T-mobile uses multiple indicators, such as billing and \n",
            "sentiment analysis, in order to identify customers that can \n",
            "be upgraded to higher quality products, as well as to identify \n",
            "those with a high lifetime customer-value, so its team can \n",
            "focus on retaining those customers.\n",
            "Purpose:\n",
            "INDUSTRY\n",
            "Communication\n",
            "COMPANY\n",
            "T-Mobile\n",
            "EMPLOYEES\n",
            "38,000\n",
            "tweet this\n",
            "Optimize Funnel \n",
            "Conversion\n",
            "\n",
            "TYPE\n",
            "Celcom Axiata Berhad adopted a big data solution in order \n",
            "to improve customer retention and boost its market share by \n",
            "improving the marketing campaign process. The company \n",
            "used real-time data to create a personalized campaign for \n",
            "each customer based on which products or offers the \n",
            "customer would most want or need.\n",
            "Purpose:\n",
            "INDUSTRY\n",
            "Communication\n",
            "COMPANY\n",
            "Celcom Axiata \n",
            "Berhad\n",
            "EMPLOYEES\n",
            "Enterprise\n",
            "tweet this\n",
            "Optimize Funnel \n",
            "Conversion\n",
            "\n",
            "With access to data on consumer \n",
            "behavior, companies can learn what \n",
            "prompts a customer to stick around \n",
            "longer, as well as learn more about their \n",
            "customer’s characteristics and \n",
            "purchasing habits in order to improve \n",
            "marketing efforts and boost profits.\n",
            "2. behavioral \n",
            "analytics\n",
            "\n",
            "With 1.8 billion customers, MasterCard is in the unique \n",
            "position of being able to analyze the behavior of customers \n",
            "in not only their own stores, but also thousands of other \n",
            "retailers. The company teamed up with Mu Sigma to collect \n",
            "and analyze data on shoppers’ behavior, and provide the \n",
            "insights it finds to other retailers in benchmarking reports. \n",
            "Purpose:\n",
            "INDUSTRY\n",
            "Finance\n",
            "COMPANY\n",
            "Mastercard\n",
            "EMPLOYEES\n",
            "67,000\n",
            "TYPE\n",
            "Behavioral \n",
            "Analytics\n",
            "tweet this\n",
            "\n",
            "With services like Hulu and Netflix competing for viewers’ \n",
            "attention, Time Warner collects data on how frequently \n",
            "customers tune in, the effect of bandwidth on consumer \n",
            "behavior, customer engagement and peak usage times in \n",
            "order to improve their service and increase profits. The \n",
            "company also segments its customers for advertisers by \n",
            "correlating viewing habits with public data—such as voter \n",
            "registration information—in order to launch highly targeted \n",
            "campaigns to specific locations or demographics.\n",
            "Purpose:\n",
            "INDUSTRY\n",
            "Entertainment\n",
            "COMPANY\n",
            "Time Warner Cable\n",
            "EMPLOYEES\n",
            "34,000\n",
            "TYPE\n",
            "Behavioral Analytics \n",
            "& Customer \n",
            "Segmentation\n",
            "tweet this\n",
            "\n",
            "TYPE\n",
            "Behavioral \n",
            "Analytics\n",
            "Customer complaints and PR crises have become more \n",
            "difficult to handle thanks to social media. To better keep track \n",
            "of customer sentiment and what is being said about the \n",
            "company online, Nestle created a 24/7 monitoring centre to \n",
            "listen to all of the conversations about the company and its \n",
            "products on social media. The company will actively engage \n",
            "with those that post about them online in order to mitigate \n",
            "damage and build customer loyalty.\n",
            "Purpose:\n",
            "INDUSTRY\n",
            "Food & Beverage\n",
            "COMPANY\n",
            "Nestlé\n",
            "EMPLOYEES\n",
            ">330,000\n",
            "tweet this\n",
            "\n",
            "By accessing data about the \n",
            "consumer from multiple sources, such \n",
            "as social media data and transaction \n",
            "history, companies can better \n",
            "segment and target their customers \n",
            "and start to make personalized offers \n",
            "to those customers.\n",
            "3. CUSTOMER\n",
            "SEGMENTATION\n",
            "\n",
            "Thanks to its partnerships with Google and Facebook, \n",
            "Heineken has access to vasts amounts of data about \n",
            "its customers that it uses to create real-time, \n",
            "personalized marketing messages. One project \n",
            "provides real-time content to fans who happen to be \n",
            "watching a sponsored event.\n",
            "Purpose:\n",
            "INDUSTRY\n",
            "Food & Beverage\n",
            "COMPANY\n",
            "Heineken\n",
            "EMPLOYEES\n",
            "64,252\n",
            "TYPE\n",
            "Customer \n",
            "Segmentation\n",
            "tweet this\n",
            "\n",
            "Spotify uses data from user profiles and users’ playlists, \n",
            "and historical data on music played to provide \n",
            "recommendations for each user. By combining data from \n",
            "millions of users, Spotify is able to make recommendations \n",
            "even if a particular user doesn’t have an extensive history \n",
            "with the site.\n",
            "Purpose:\n",
            "INDUSTRY\n",
            "Entertainment\n",
            "COMPANY\n",
            "Spotify\n",
            "EMPLOYEES\n",
            "5,000\n",
            "tweet this\n",
            "TYPE\n",
            "Customer \n",
            "Segmentation & \n",
            "Behavioral Analytics \n",
            "\n",
            "Through sensors and other \n",
            "machine-generated data, companies can \n",
            "identify when a malfunction is likely to \n",
            "occur. The company can then \n",
            "preemptively order parts and make \n",
            "repairs in order to avoid downtime and \n",
            "lost profits.\n",
            "4. PREDICTIVE SUPPORT\n",
            "\n",
            "Southwest analyses sensor data on their planes in order \n",
            "to identify patterns that indicate a potential malfunction \n",
            "or safety issue. This allows the airline to address potential \n",
            "problems and make necessary repairs without \n",
            "interrupting flights or putting passengers in danger.\n",
            "Purpose:\n",
            "INDUSTRY\n",
            "Travel\n",
            "COMPANY\n",
            "Southwest Airlines\n",
            "EMPLOYEES\n",
            ">45,000\n",
            "TYPE\n",
            "Predictive Support\n",
            "tweet this\n",
            "\n",
            "Engine yard provides big data analytics to its users, so \n",
            "they can monitor the performance of applications in \n",
            "real time, pinpoint problems with the infrastructure and \n",
            "optimize the platform to correct performance issues.\n",
            "Purpose:\n",
            "INDUSTRY\n",
            "Cloud Storage\n",
            "COMPANY\n",
            "Engine Yard\n",
            "EMPLOYEES\n",
            "130\n",
            "TYPE\n",
            "Predictive Support\n",
            "tweet this\n",
            "\n",
            "By quickly pulling data together \n",
            "from multiple sources, retailers can \n",
            "better optimize their product \n",
            "selection and pricing, as well as \n",
            "decide where to target ads.\n",
            "5. marKET BASKET \n",
            "ANALYSIS & PRICING \n",
            "OPTIMIZATION\n",
            "\n",
            "TYPE\n",
            "Coca-Cola uses an algorithm to ensure that its \n",
            "orange juice has a consistent taste throughout the \n",
            "year. The algorithm incorporates satellite imagery, \n",
            "crop yields, consumer preferences and details about \n",
            "the flavours that make up a particular fruit in order \n",
            "to determine how the juice should be blended.\n",
            "Purpose:\n",
            "INDUSTRY\n",
            "Food\n",
            "COMPANY\n",
            "Coca-Cola Co.\n",
            "EMPLOYEES\n",
            "146,200\n",
            "tweet this\n",
            "Market Basket \n",
            "Analysis\n",
            "\n",
            "Big data analytics can track trends in \n",
            "security breaches and allow companies \n",
            "to proactively go after threats before they \n",
            "strike.\n",
            "6. PREDICT SECURITY\n",
            "THREATS\n",
            "\n",
            "TYPE\n",
            "Rabobank analysed criminal activities at ATMs to \n",
            "determine factors that increased the risk of \n",
            "becoming victimized. It discovered that proximity to \n",
            "highways, weather conditions and the season all \n",
            "affect the risk of a security threat.\n",
            "Purpose:\n",
            "INDUSTRY\n",
            "Finance\n",
            "COMPANY\n",
            "Rabobank\n",
            "EMPLOYEES\n",
            "27,000\n",
            "tweet this\n",
            "Predict Security \n",
            "Threats\n",
            "\n",
            "TYPE\n",
            "With more than 1.5 billion items in its catalog, \n",
            "Amazon has a lot of product to keep track of \n",
            "and protect. It uses its cloud system, S3, to \n",
            "predict which items are most likely to be stolen, \n",
            "so it can better secure its warehouses.\n",
            "Purpose:\n",
            "INDUSTRY\n",
            "Online Retail\n",
            "COMPANY\n",
            "Amazon\n",
            "EMPLOYEES\n",
            "110,000\n",
            "tweet this\n",
            "Predict Security \n",
            "Threats\n",
            "\n",
            "Financial firms use big data to \n",
            "help them identify sophisticated \n",
            "fraud schemes by combining \n",
            "multiple points of data.\n",
            "7. FRAUD DETECTION\n",
            "\n",
            "TYPE\n",
            "Zions Bank uses data analytics to detect anomalies \n",
            "across channels that indicate potential fraud. The \n",
            "fraud team receives data from 140 sources—some in \n",
            "real-time—to monitor activity, such as if a customer \n",
            "makes a mobile banking transaction at the same \n",
            "time as a branch transaction.\n",
            "Purpose:\n",
            "INDUSTRY\n",
            "Finance\n",
            "COMPANY\n",
            "Zion’s Bank\n",
            "EMPLOYEES\n",
            "2,700\n",
            "tweet this\n",
            "Fraud Detection\n",
            "\n",
            "TYPE\n",
            "Discovery Health uses big data analytics to identify \n",
            "fraudulent claims and possible fraudulent \n",
            "prescriptions. For example, it can identify if a \n",
            "healthcare provider is charging for a more expensive \n",
            "procedure than was actually performed.\n",
            "Purpose:\n",
            "INDUSTRY\n",
            "Insurance\n",
            "COMPANY\n",
            "Discovery Health\n",
            "EMPLOYEES\n",
            "5,000\n",
            "tweet this\n",
            "Fraud Detection\n",
            "\n",
            "Virtually every industry has invested \n",
            "in big data to help solve specific \n",
            "challenges those industries face. \n",
            "Healthcare, for example, uses big \n",
            "data to improve patient outcomes, \n",
            "and agriculture uses data to boost \n",
            "crop yields.\n",
            "8. industry specific\n",
            "\n",
            "Kayak uses big data analytics to create a predictive model that \n",
            "tells users if the price for a particular flight will go up or down \n",
            "within the next week. The system uses one billion search queries \n",
            "to find the cheapest flights, as well as popular destinations and \n",
            "the busiest airports. The algorithm is constantly improved by \n",
            "tracking the flights to see if its predictions are correct.\n",
            "Purpose:\n",
            "INDUSTRY\n",
            "Travel\n",
            "COMPANY\n",
            "Kayak\n",
            "EMPLOYEES\n",
            "101\n",
            "TYPE\n",
            "Industry Specific\n",
            "tweet this\n",
            "\n",
            "TYPE\n",
            "Aurora collects internal as well as national data in \n",
            "order to create a benchmark for healthcare quality. It \n",
            "also analyzes data on groups of patients with similar \n",
            "medical conditions, to reveal trends in the diseases \n",
            "and to identify the right candidates for medical \n",
            "research. Finally, the real-time data analysis allows \n",
            "Aurora to predict and improve patient outcomes ,and \n",
            "so far has reduced readmissions by 10 percent.\n",
            "Purpose:\n",
            "INDUSTRY\n",
            "Health Care\n",
            "COMPANY\n",
            "Aurora Health Care\n",
            "EMPLOYEES\n",
            "30,000\n",
            "tweet this\n",
            "Industry Specific\n",
            "\n",
            "TYPE\n",
            "Shell uses sensor data to map its oil and gas \n",
            "wells in order to increase output and boost the \n",
            "efficiency of its operations. The data received \n",
            "from the sensors is analyzed by artificial \n",
            "intelligence and rendered in 3D and 4D maps.\n",
            "Purpose:\n",
            "INDUSTRY\n",
            "Oil\n",
            "COMPANY\n",
            "Shell\n",
            "EMPLOYEES\n",
            "87,000\n",
            "tweet this\n",
            "Industry Specific\n",
            "\n",
            "TAKE-AWAY\n",
            "In all of these examples, data is stored, processed, and analyzed, to\n",
            "turn it into value.\n",
            "20\n",
            "\n",
            "BIG DATACENTERS\n",
            "\n",
            "How do you process Big Data ?\n",
            "Assume you’re google at the beginning of the 2000’s \n",
            "and you want to index the web\n",
            "\n",
            "Execution environment - 1997\n",
            "• Just \n",
            "a \n",
            "bunch \n",
            "of \n",
            "computers. \n",
            "• The web index fit on \n",
            "a single computer \n",
            "• Redundancy \n",
            "to \n",
            "ensure availability\n",
            "\n",
            "Execution environment - 1999\n",
            "• Compute \n",
            "servers \n",
            "consist \n",
            "of \n",
            "multiple CPUs (possibly with \n",
            "multiple cores per CPU), and \n",
            "attached hard disks,\n",
            "• Servers are collected in racks. \n",
            "High-speed Ethernet connections \n",
            "(at least 1Gbps) connect servers \n",
            "in a rack together\n",
            "The Google “Corckboard” \n",
            "rack\n",
            " \n",
            "\n",
            "Execution environment - currently\n",
            "• Racks are connected together to central network switches using \n",
            "multi-Gbps redundant links.  \n",
            "• Access of data from other racks is slower than data on same \n",
            "computer/same rack\n",
            "• Many racks together form a data center\n",
            "An image of the google \n",
            "datacenter in Mons \n",
            "(Belgium)\n",
            " \n",
            "\n",
            "Discussion\n",
            "• Each compute node is kept simple by design:\n",
            "– Mid-range computers are much cheaper than \n",
            "powerful high-range computers …\n",
            "– … and consume less energy\n",
            "– … but google has lots of them!\n",
            "\n",
            "Characteristics\n",
            "Figure Source: The Datacenter as a Computer \n",
            "\n",
            "Characteristics\n",
            "Figure Source: The Datacenter as a Computer \n",
            "Capacity: The amount of data we can \n",
            "store per server/rack/datacenter\n",
            "\n",
            "Characteristics\n",
            "Figure Source: The Datacenter as a Computer \n",
            "Latency: The time it takes to fetch a data \n",
            "item, when asked on local \n",
            "machine/another server on the same \n",
            "rack/another server on a different rack\n",
            "\n",
            "Characteristics\n",
            "Figure Source: The Datacenter as a Computer \n",
            "Bandwidth: the speed at which data can \n",
            "be transferred to the same \n",
            "machine/another server on the same \n",
            "rack/another server on a different rack\n",
            "\n",
            "Characteristics\n",
            "Figure Source: The Datacenter as a Computer \n",
            "Conclusion:\n",
            "•\n",
            "Huge storage capacity\n",
            "•\n",
            "Latency between racks \n",
            "= 1/10 latency on rack level \n",
            "≈ 1/10 latency on server level\n",
            "•\n",
            "Bandwidth between racks\n",
            "= 1/10 bandwidth on rack level \n",
            "= ½ to 1/10 bandwidth on server level\n",
            "\n",
            "What do we gain? Parallelism!\n",
            "• Let us consider the maximal aggregate bandwidth: the speed \n",
            "by which we can analyze data in parallel assuming ideal data \n",
            "distribution over servers & disks\n",
            "• “Embarrassingly parallel” example: count the number of times \n",
            "the word “Belgium” appears in documents on the Web.  \n",
            "•\n",
            "Each server has multiple CPUs and \n",
            "can read from multiple disks in \n",
            "parallel. As such, each server can \n",
            "analyze many documents in parallel.\n",
            "•\n",
            "At the end,  sum the per-server \n",
            "counters (which can be done very \n",
            "fast)\n",
            "\n",
            "What do we gain? Parallelism!\n",
            "• Let us consider the maximal aggregate bandwidth: the speed \n",
            "by which we can analyze data in parallel assuming ideal data \n",
            "distribution over servers & disks\n",
            "Component\n",
            "Max Aggr Bandwidth\n",
            "1 Hard Disk\n",
            "100 MB/sec (≈ 1 Gbps)\n",
            "Server\n",
            "= 12  Hard Disks\n",
            "1.2 GB/sec  (≈ 12 Gbps)\n",
            "Rack\n",
            "= 80 servers\n",
            "96 GB/sec   (≈ 768 Gbps)\n",
            "Cluster/datacenter\n",
            "= 30 racks\n",
            "2.88 TB/sec (≈ 23 Tbps)\n",
            "• Scanning 400TB hence takes 138 secs ≈ 2,3 minutes\n",
            "• Scanning 400TB sequentially at 100 MB/sec takes ≈ 46,29 days \n",
            "\n",
            "The challenges (1/2)\n",
            "• Scalable software development: allow growth without \n",
            "requiring re-architecting algorithm/applications\n",
            "Auto scale\n",
            "\n",
            "The challenges (2/2)\n",
            "• Fault-tolerance: if you have 1000’s of machines, failures \n",
            "happen every day.\n",
            "• Fault-tolerance is typically\n",
            "addressed through\n",
            "redundancy and/or\n",
            "re-execution\n",
            "\n",
            "FAULT-TOLERANCE: MOTIVATIONAL EXAMPLE\n",
            "• Mean Time To Failure (MTTF) of typical hard-disk:\n",
            "1, 000, 000 hours = 114 years\n",
            "22\n",
            "\n",
            "FAULT-TOLERANCE: MOTIVATIONAL EXAMPLE\n",
            "• Mean Time To Failure (MTTF) of typical hard-disk:\n",
            "1, 000, 000 hours = 114 years\n",
            "• Annual Failure Rate (AFR) for a given MTTF\n",
            "AFR = #hours in 1 year\n",
            "MTTF\n",
            "= 365 × 24\n",
            "106\n",
            "= 0.876%\n",
            "22\n",
            "\n",
            "FAULT-TOLERANCE: MOTIVATIONAL EXAMPLE\n",
            "• Mean Time To Failure (MTTF) of typical hard-disk:\n",
            "1, 000, 000 hours = 114 years\n",
            "• Annual Failure Rate (AFR) for a given MTTF\n",
            "AFR = #hours in 1 year\n",
            "MTTF\n",
            "= 365 × 24\n",
            "106\n",
            "= 0.876%\n",
            "• Expected number of disk failures per year in a datacenter with 100,000\n",
            "disks:\n",
            "#disks × AFR = 105 × 0.876% = 876disks\n",
            "year = 2disks\n",
            "day\n",
            "22\n",
            "\n",
            "Google’s solutions\n",
            "• New programming models and frameworks \n",
            "for distributed and scalable data analysis\n",
            "Name\n",
            "Purpose\n",
            "Google File System\n",
            "A distributed file system for scalable \n",
            "storage and high-throughput retrieval\n",
            "Map Reduce\n",
            "A programming model + execution \n",
            "environment for general-purpose \n",
            "distributed batch processing\n",
            "BigTable\n",
            "A NoSQL Database\n",
            "Dremel, F1\n",
            "A query language for interactive SQL-\n",
            "like analysis of structured datasets\n",
            "…\n",
            "Lots of research ongoing!\n",
            "Open Source Impl\n",
            "Apache Hadoop\n",
            "•\n",
            "HDFS\n",
            "•\n",
            "M/R\n",
            "Apache HBase\n",
            "Apache Spark/Drill\n",
            "….\n",
            "High-level design described in a series of papers; no implementation available\n",
            "\n",
            "HDFS\n",
            "\n",
            "HDFS: \n",
            "THE HADOOP DISTRIBUTED FILE \n",
            "SYSTEM\n",
            "\n",
            "HDFS Architecture\n",
            "• HDFS has a master/slave architecture\n",
            "• Master = NameNode (NN) manages the file system and \n",
            "regulates access to files by clients.\n",
            "• Slaves = DataNodes (DN), usually one per server in the cluster, \n",
            "manage storage attached to the server that they run on.\n",
            "NameNode\n",
            "DataNodes\n",
            "/users/ds/dat0.txt  r:2     {1,3}\n",
            "/users/ds/dat1.txt  r:3     {2, 4, 5}\n",
            "MetaData(FileName, Replication Factor, Block Ids)\n",
            "1\n",
            "2\n",
            "4\n",
            "5\n",
            "2\n",
            "5\n",
            "3\n",
            "1\n",
            "3\n",
            "4\n",
            "2\n",
            "4\n",
            "5\n",
            "•\n",
            "Files are transparently \n",
            "broken down into blocks \n",
            "(default 128 MB).\n",
            "•\n",
            "Blocks are replicated \n",
            "across datanodes. \n",
            "Replication is rack-aware.\n",
            "dat0.txt\n",
            "1\n",
            "3\n",
            "dat1.txt\n",
            "2\n",
            "4\n",
            "5\n",
            "\n",
            "HDFS Architecture\n",
            "• HDFS has a master/slave architecture\n",
            "• Master = NameNode (NN) manages the file system and \n",
            "regulates access to files by clients.\n",
            "• Slaves = DataNodes (DN), usually one per server in the cluster, \n",
            "manage storage attached to the server that they run on.\n",
            "NameNode\n",
            "DataNodes\n",
            "/users/ds/dat0.txt  r:2     {1,3}\n",
            "/users/ds/dat1.txt  r:3     {2, 4, 5}\n",
            "MetaData(FileName, Replication Factor, Block Ids)\n",
            "1\n",
            "2\n",
            "4\n",
            "5\n",
            "2\n",
            "5\n",
            "3\n",
            "1\n",
            "3\n",
            "4\n",
            "2\n",
            "4\n",
            "5\n",
            "•\n",
            "Optimized for:\n",
            "•\n",
            "Large files\n",
            "•\n",
            "Read throughput\n",
            "•\n",
            "Appending writes\n",
            "(files are append-only)\n",
            "•\n",
            "Replication ensures:\n",
            "•\n",
            "Durability\n",
            "•\n",
            "Availability\n",
            "•\n",
            "Throughput\n",
            "\n",
            "HDFS Common Pitfalls\n",
            "• HDFS = optimized for large files.\n",
            "– A single HDFS block is 128 MB  per default.\n",
            "– It does not make sense to store files smaller than 128 MB in HDFS!\n",
            "• Files are immutable, optimized for sequential access.\n",
            "– New files that are created can only be appended to (no random-\n",
            "access)\n",
            "– Once a file is created, it is immutable. Changing a file requires re-\n",
            "creating it.\n",
            "\n",
            "HDFS Implementation\n",
            "• This implies that clients only need to install a JAR file to access \n",
            "the HDFS\n",
            "“ The NameNode and DataNode are pieces of software designed to run \n",
            "on commodity machines. These machines typically run a GNU/Linux \n",
            "operating system (OS). HDFS is built using the Java language; any \n",
            "machine that supports Java can run the NameNode or the DataNode \n",
            "software. Usage of the highly portable Java language means that \n",
            "HDFS can be deployed on a wide range of machines.\n",
            "Source: Hadoop documentation\n",
            "\n",
            "Typical HDFS commands\n",
            "bin/hadoop fs –ls\n",
            "bin/hadoop fs –mkdir\n",
            "bin/hadoop fs –copyFromLocal\n",
            "bin/hadoop fs –copyToLocal\n",
            "bin/hadoop fs –moveToLocal\n",
            "bin/hadoop fs –rm\n",
            "\n",
            "MAP/REDUCE\n",
            "\n",
            "MAP/REDUCE: SIMPLIFIED DATA \n",
            "PROCESSING ON LARGE CLUSTERS\n",
            "\n",
            "As a reaction to this complexity, we designed a new \n",
            "abstraction that allows us to express the simple \n",
            "computations we were trying to perform but hides the \n",
            "messy details of parallelization, fault tolerance, data \n",
            "distribution and load balancing in a library. Our abstraction \n",
            "is inspired by the map and reduce primitives present in Lisp \n",
            "and many other functional languages. We realized that \n",
            "most of our computations involved applying a map \n",
            "operation to each logical record’ in our input in order to \n",
            "compute a set of intermediate key/value pairs, and then \n",
            "applying a reduce operation to all the values that shared \n",
            "the same key in order to combine the derived data \n",
            "appropriately. Our use of a functional model with user-\n",
            "specified map and reduce operations allows us to \n",
            "parallelize large computations easily and to use reexecution \n",
            "as the primary mechanism for fault tolerance.\n",
            "\n",
            "M/R Computational Model\n",
            "• A M/R job is specified by two functions: map and reduce\n",
            "map: (key1, value1) -> list(key2, value2)\n",
            "The input key/value pair represents a logical record in the input data source. \n",
            "In the case of a file this could be a line, or if the input source is a database \n",
            "table, this could be a record,\n",
            "A single input key/value pair may result in zero \n",
            "or more output key/value pairs.\n",
            "\n",
            "M/R Computational Model\n",
            "• A M/R job is specified by two functions: map and reduce\n",
            "reduce: (key2, list(value2)) -> list(key3, value3)\n",
            "The reduce function is called once per \n",
            "unique map output key, and receives \n",
            "a list of all values emitted for that key\n",
            "Like the map function, reduce can output zero \n",
            "to many key/value pairs. \n",
            "\n",
            "M/R Computational Model\n",
            "• For each word occurring in a document on the Web, count the \n",
            "number of occurrences across all documents. \n",
            "def map(docid, line): \n",
            " \n",
            "for each word in line:\n",
            " \n",
            "   yield (word, 1)\n",
            "def reduce(word, list-of-occ-numbers): \n",
            " \n",
            "   yield (word, sum(list-of-occ-numbers))\n",
            "\n",
            "M/R: opportunity for parallelism\n",
            "• We can spawn multiple copies of the map function (called \n",
            "map tasks) in parallel (at most one for each key/value pair).\n",
            "• Likewise, we can spawn multiple copies of the reduce \n",
            "function (called reduce tasks) in parallel (at most one for each \n",
            "unique key output by the map).\n",
            "Input\n",
            "map\n",
            "map\n",
            "map\n",
            "map output\n",
            "cat, 1\n",
            "dog, 1\n",
            "turtle, 1\n",
            "cat, 1\n",
            "belgium, 1\n",
            "turtle, 1\n",
            "dog, 1\n",
            "doc1\n",
            "doc2\n",
            "doc3\n",
            "shuffle+sort\n",
            "reduce\n",
            "input\n",
            "cat, [1,1]\n",
            "dog, [1,1]\n",
            "turtle, [1,1]\n",
            "belgium, [1]\n",
            "reduce\n",
            "reduce\n",
            "reduce\n",
            "reduce\n",
            "\n",
            "M/R Execution: Hadoop V1 Architecture\n",
            "•\n",
            "Master = JobTracker – accepts jobs, decomposes them into map \n",
            "and reduce tasks, and schedules them for remote execution on \n",
            "child nodes.\n",
            "•\n",
            "Slave = TaskTracker – accepts tasks from Jobtracker and spawns \n",
            "child processes to do the actual work. \n",
            "•\n",
            "Idea: ship computation to data\n",
            "NameNode\n",
            "DataNodes = TaskTrackers\n",
            "1\n",
            "3\n",
            "4\n",
            "2\n",
            "4\n",
            "5\n",
            "•\n",
            "The JobTracker accepts M/R \n",
            "jobs. \n",
            "•\n",
            "If the input is a HDFS file, a \n",
            "map task is created for each \n",
            "block and sent to the node \n",
            "holding that block for \n",
            "execution.\n",
            "•\n",
            "Map output is written to \n",
            "local disk.\n",
            "JobTracker\n",
            "Job1\n",
            "Job2\n",
            "Map task 1\n",
            "Map task 2\n",
            "Reduce task 1\n",
            "Reduce task 2\n",
            "…\n",
            "1\n",
            "2\n",
            "4\n",
            "5\n",
            "2\n",
            "5\n",
            "3\n",
            "\n",
            "M/R Execution: Hadoop V1 Architecture\n",
            "•\n",
            "Master = JobTracker – accepts jobs, decomposes them into map \n",
            "and reduce tasks, and schedules them for remote execution on \n",
            "child nodes.\n",
            "•\n",
            "Slave = TaskTracker – accepts tasks from Jobtracker and spawns \n",
            "child processes to do the actual work. \n",
            "•\n",
            "Idea: ship computation to data\n",
            "NameNode\n",
            "DataNodes = TaskTrackers\n",
            "1\n",
            "3\n",
            "4\n",
            "2\n",
            "4\n",
            "5\n",
            "•\n",
            "The JobTracker creates \n",
            "reduce tasks intelligently\n",
            "•\n",
            "Reduce tasks read the map \n",
            "outputs over the network \n",
            "and write their output \n",
            "back to HDFS.\n",
            "JobTracker\n",
            "Job1\n",
            "Job2\n",
            "Map task 1\n",
            "Map task 2\n",
            "Reduce task 1\n",
            "Reduce task 2\n",
            "…\n",
            "1\n",
            "2\n",
            "4\n",
            "5\n",
            "2\n",
            "5\n",
            "3\n",
            "\n",
            "M/R Execution: Hadoop V1 Architecture\n",
            "•\n",
            "Master = JobTracker – accepts jobs, decomposes them into map \n",
            "and reduce tasks, and schedules them for remote execution on \n",
            "child nodes.\n",
            "•\n",
            "Slave = TaskTracker – accepts tasks from Jobtracker and spawns \n",
            "child processes to do the actual work. \n",
            "•\n",
            "Idea: ship computation to data\n",
            "NameNode\n",
            "DataNodes = TaskTrackers\n",
            "1\n",
            "3\n",
            "4\n",
            "2\n",
            "4\n",
            "5\n",
            "•\n",
            "Load balancing: The \n",
            "JobTracker monitors for \n",
            "stragglers and may spawn \n",
            "additional map on \n",
            "datanodes that hold a \n",
            "block replica. Whichever \n",
            "node completes first is \n",
            "allowed to proceed. The \n",
            "other(s) is/are killed.\n",
            "JobTracker\n",
            "Job1\n",
            "Job2\n",
            "Map task 1\n",
            "Map task 2\n",
            "Reduce task 1\n",
            "Reduce task 2\n",
            "…\n",
            "1\n",
            "2\n",
            "4\n",
            "5\n",
            "2\n",
            "5\n",
            "3\n",
            "\n",
            "M/R Execution: Hadoop V1 Architecture\n",
            "•\n",
            "Master = JobTracker – accepts jobs, decomposes them into map \n",
            "and reduce tasks, and schedules them for remote execution on \n",
            "child nodes.\n",
            "•\n",
            "Slave = TaskTracker – accepts tasks from Jobtracker and spawns \n",
            "child processes to do the actual work. \n",
            "•\n",
            "Idea: ship computation to data\n",
            "NameNode\n",
            "DataNodes = TaskTrackers\n",
            "1\n",
            "3\n",
            "4\n",
            "2\n",
            "4\n",
            "5\n",
            "•\n",
            "Failures: In clusters with \n",
            "1000s of nodes, hardware \n",
            "failures occur frequently. \n",
            "The same mechanism as \n",
            "for load balancing allows \n",
            "to cope with such failures\n",
            "JobTracker\n",
            "Job1\n",
            "Job2\n",
            "Map task 1\n",
            "Map task 2\n",
            "Reduce task 1\n",
            "Reduce task 2\n",
            "…\n",
            "1\n",
            "2\n",
            "4\n",
            "5\n",
            "2\n",
            "5\n",
            "3\n",
            "\n",
            "M/R Scheduling: caveats\n",
            "•\n",
            "Hadoop does its best to run the map task \n",
            "on a node where the input data resides in \n",
            "HDFS, hence exploiting data locality.\n",
            "•\n",
            "Sometimes, however, all nodes hosting the \n",
            "HDFS block replicas for a map task’s input \n",
            "split are busy.\n",
            "•\n",
            "In that case, the job scheduler will look for \n",
            "a free map slot on a node in the same rack \n",
            "as one of the blocks. Very occasionally \n",
            "even this is not possible, so an off-rack \n",
            "node is used, which results in an inter-rack \n",
            "network transfer. \n",
            "\n",
            "M/R Operation: some more detail\n",
            "Figure Source: Hadoop – The definitive Guide\n",
            "•\n",
            "The number of map tasks (Mapper) is actually  equal to the number of splits.\n",
            "•\n",
            "Split is a logical division of the input data while block is a physical division of data.\n",
            "•\n",
            "HDFS default block size is default split size.\n",
            "\n",
            "M/R Operation: some more detail\n",
            "Figure Source: Hadoop – The definitive Guide\n",
            "•\n",
            "Each map task partitions its output; the number of partitions equals the number of \n",
            "reducers that will run later. Each partition is sorted already on the map-side. (One \n",
            "partition can still have many distinct keys!)\n",
            "•\n",
            "The reducers copy their respective partition from the mappers, and merge them \n",
            "into one big sorted file.\n",
            "•\n",
            "The net result is a distributed merge-sort between the map and reduce phases.\n",
            "\n",
            "M/R Operation: some more detail\n",
            "Figure Source: Hadoop – The definitive Guide\n",
            "•\n",
            "Optionally, one can specify a combine function that already performs a partial \n",
            "map-side reduction. The aim is then to decrease the size of data to be transferred.\n",
            "map: (key1, value1) -> list(key2, value2)\n",
            "Combine: (key2, list(value2)) -> list(key2, value2)\n",
            "reduce: (key2, list(value2)) -> list(key3, value3)\n",
            "\n",
            "Combiner example\n",
            "• For each word occurring in a document on the Web, count the \n",
            "number of occurrences across all documents. \n",
            "def map(docid, line): \n",
            " \n",
            "for each word in line:\n",
            " \n",
            "   yield (word, 1)\n",
            "def reduce(word, list-of-occ-numbers): \n",
            " \n",
            "   yield (word, sum(list-of-occ-numbers))\n",
            "def combine(word, list-of-ones): \n",
            " \n",
            "yield (word, sum(list-of-ones))\n",
            "\n",
            "WRITING M/R PROGRAMS\n",
            "• Hadoop M/R is written in Java. As such, the default way to write a M/R\n",
            "job is through its Java API.\n",
            "• Because this can be cumbersome, Hadoop M/R also exposes the\n",
            "streaming interface which allows executables written in an arbitrary\n",
            "language (shell script, python, C, …) to implement the map and reduce\n",
            "logic.\n",
            "• In that case, the executable reads its input from stdin, and writes its\n",
            "output to stdout.\n",
            "25\n",
            "\n",
            "REFERENCES\n",
            "• S. Ghemawate, H. Gobioff, and H.-T. Leung. The Google File System\n",
            "http://research.google.com/archive/gfs-sosp2003.pdf\n",
            "• HDFS architecture. http://hadoop.apache.org/docs/stable/\n",
            "hadoop-project-dist/hadoop-hdfs/HdfsDesign.html\n",
            "• J. Dean and S. Ghemawat. MapReduce: Simplified Data Processing on Large\n",
            "Clusters. Communications of the ACM 2008.\n",
            "• L. A. Barroso, J. Clidaras, U. Hölzle. The datacenter as a computer.\n",
            "http://www.morganclaypool.com/doi/abs/10.2200/\n",
            "S00516ED2V01Y201306CAC024\n",
            "• T. White. Hadoop: The Definitive Guide. O’Reilly Media, 2010.\n",
            "26\n",
            "\n",
            "QUESTIONS?\n",
            "27\n",
            "\n",
            "ACKNOWLEDGEMENTS\n",
            "Based on content created by Jan Hidders and Stijn Vansummeren.\n",
            "28\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_clean_tokens_from_pdf(filepath):\n",
        "    import fitz\n",
        "    doc = fitz.open(filepath)\n",
        "    all_words = []\n",
        "    for page in doc:\n",
        "        word_list = page.get_text(\"words\")  # returns (x0, y0, x1, y1, \"word\", ...)\n",
        "        words = [w[4] for w in word_list]\n",
        "        all_words.extend(words)\n",
        "    return all_words\n"
      ],
      "metadata": {
        "id": "8uXYxdV7s2rl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_words_list = extract_clean_tokens_from_pdf(pdf_path)\n",
        "display(raw_words_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pPzGlF48s58z",
        "outputId": "7a00c6c0-575b-409e-cb1d-c9350825e497"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "['INFO-H515:',\n",
              " 'BIG',\n",
              " 'DATA:',\n",
              " 'DISTRIBUTED',\n",
              " 'MANAGEMENT',\n",
              " 'Lecture',\n",
              " '1:',\n",
              " 'Introduction,',\n",
              " 'HDFS,',\n",
              " 'Map/Reduce',\n",
              " 'Dimitris',\n",
              " 'Sacharidis',\n",
              " 'LECTURE',\n",
              " 'OUTLINE',\n",
              " 'What',\n",
              " 'is',\n",
              " 'Big',\n",
              " 'Data?',\n",
              " 'How',\n",
              " 'is',\n",
              " 'Big',\n",
              " 'Data',\n",
              " 'Used?',\n",
              " 'Big',\n",
              " 'Datacenters',\n",
              " 'HDFS',\n",
              " 'Map/Reduce',\n",
              " '1',\n",
              " 'WHAT',\n",
              " 'IS',\n",
              " 'BIG',\n",
              " 'DATA?',\n",
              " '“Big',\n",
              " 'data',\n",
              " 'is',\n",
              " 'like',\n",
              " 'teenage',\n",
              " 'sex:',\n",
              " 'everyone',\n",
              " 'talks',\n",
              " 'about',\n",
              " 'it,',\n",
              " 'nobody',\n",
              " 'really',\n",
              " 'knows',\n",
              " 'how',\n",
              " 'to',\n",
              " 'do',\n",
              " 'it,',\n",
              " 'everyone',\n",
              " 'thinks',\n",
              " 'everyone',\n",
              " 'else',\n",
              " 'is',\n",
              " 'doing',\n",
              " 'it,',\n",
              " 'so',\n",
              " 'everyone',\n",
              " 'claims',\n",
              " 'they',\n",
              " 'are',\n",
              " 'doing',\n",
              " 'it',\n",
              " '…”',\n",
              " '—Dan',\n",
              " 'Ariely',\n",
              " '3',\n",
              " 'BIG',\n",
              " 'DATA',\n",
              " 'DATA',\n",
              " 'ANALYTICS',\n",
              " 'DATA',\n",
              " 'SCIENCE',\n",
              " 'THE',\n",
              " 'BIG',\n",
              " 'THREE',\n",
              " '4',\n",
              " 'BIG',\n",
              " 'DATA',\n",
              " 'THE',\n",
              " 'BIG',\n",
              " 'THREE',\n",
              " '4',\n",
              " 'BIG',\n",
              " 'DATA',\n",
              " 'Some',\n",
              " 'History',\n",
              " 'THE',\n",
              " 'BIG',\n",
              " 'THREE',\n",
              " '4',\n",
              " 'Source:',\n",
              " 'The',\n",
              " 'Economist,',\n",
              " 'February',\n",
              " '25,',\n",
              " '2010',\n",
              " 'The',\n",
              " 'Data',\n",
              " 'Deluge',\n",
              " '“',\n",
              " 'EIGHTEEN',\n",
              " 'months',\n",
              " 'ago,',\n",
              " 'Li',\n",
              " '&',\n",
              " 'Fung,',\n",
              " 'a',\n",
              " 'firm',\n",
              " 'that',\n",
              " 'manages',\n",
              " 'supply',\n",
              " 'chains',\n",
              " 'for',\n",
              " 'retailers,',\n",
              " 'saw',\n",
              " '100',\n",
              " 'gigabytes',\n",
              " 'of',\n",
              " 'information',\n",
              " 'flow',\n",
              " 'through',\n",
              " 'its',\n",
              " 'network',\n",
              " 'each',\n",
              " 'day.',\n",
              " 'Now',\n",
              " 'the',\n",
              " 'amount',\n",
              " 'has',\n",
              " 'increased',\n",
              " 'tenfold.',\n",
              " 'During',\n",
              " '2009,',\n",
              " 'American',\n",
              " 'drone',\n",
              " 'aircraft',\n",
              " 'flying',\n",
              " 'over',\n",
              " 'Iraq',\n",
              " 'and',\n",
              " 'Afghanistan',\n",
              " 'sent',\n",
              " 'back',\n",
              " 'around',\n",
              " '24',\n",
              " \"years'\",\n",
              " 'worth',\n",
              " 'of',\n",
              " 'video',\n",
              " 'footage.',\n",
              " 'New',\n",
              " 'models',\n",
              " 'being',\n",
              " 'deployed',\n",
              " 'this',\n",
              " 'year',\n",
              " 'will',\n",
              " 'produce',\n",
              " 'ten',\n",
              " 'times',\n",
              " 'as',\n",
              " 'many',\n",
              " 'data',\n",
              " 'streams',\n",
              " 'as',\n",
              " 'their',\n",
              " 'predecessors,',\n",
              " 'and',\n",
              " 'those',\n",
              " 'in',\n",
              " '2011',\n",
              " 'will',\n",
              " 'produce',\n",
              " '30',\n",
              " 'times',\n",
              " 'as',\n",
              " 'many.',\n",
              " 'Source:',\n",
              " 'The',\n",
              " 'Economist,',\n",
              " 'February',\n",
              " '25,',\n",
              " '2010',\n",
              " 'The',\n",
              " 'Data',\n",
              " 'Deluge',\n",
              " '“',\n",
              " 'Everywhere',\n",
              " 'you',\n",
              " 'look,',\n",
              " 'the',\n",
              " 'quantity',\n",
              " 'of',\n",
              " 'information',\n",
              " 'in',\n",
              " 'the',\n",
              " 'world',\n",
              " 'is',\n",
              " 'soaring.',\n",
              " 'According',\n",
              " 'to',\n",
              " 'one',\n",
              " 'estimate,',\n",
              " 'mankind',\n",
              " 'created',\n",
              " '150',\n",
              " 'exabytes',\n",
              " '(billion',\n",
              " 'gigabytes)',\n",
              " 'of',\n",
              " 'data',\n",
              " 'in',\n",
              " '2005.',\n",
              " 'This',\n",
              " 'year,',\n",
              " 'it',\n",
              " 'will',\n",
              " 'create',\n",
              " '1,200',\n",
              " 'exabytes.',\n",
              " 'Merely',\n",
              " 'keeping',\n",
              " 'up',\n",
              " 'with',\n",
              " 'this',\n",
              " 'flood,',\n",
              " 'and',\n",
              " 'storing',\n",
              " 'the',\n",
              " 'bits',\n",
              " 'that',\n",
              " 'might',\n",
              " 'be',\n",
              " 'useful,',\n",
              " 'is',\n",
              " 'difficult',\n",
              " 'enough.',\n",
              " 'Analysing',\n",
              " 'it,',\n",
              " 'to',\n",
              " 'spot',\n",
              " 'patterns',\n",
              " 'and',\n",
              " 'extract',\n",
              " 'useful',\n",
              " 'information,',\n",
              " 'is',\n",
              " 'harder',\n",
              " 'still.',\n",
              " 'Source:',\n",
              " 'The',\n",
              " 'Economist,',\n",
              " 'February',\n",
              " '25,',\n",
              " '2010',\n",
              " 'The',\n",
              " 'Data',\n",
              " 'Deluge',\n",
              " '“',\n",
              " 'By',\n",
              " 'the',\n",
              " '2030s,',\n",
              " 'computer',\n",
              " 'data',\n",
              " 'storage',\n",
              " 'may',\n",
              " 'surpass',\n",
              " '1',\n",
              " 'yottabyte',\n",
              " '(1024),',\n",
              " 'the',\n",
              " 'largest',\n",
              " 'number',\n",
              " 'with',\n",
              " 'an',\n",
              " 'official',\n",
              " 'metric',\n",
              " 'prefix.',\n",
              " 'The',\n",
              " 'International',\n",
              " 'Bureau',\n",
              " 'of',\n",
              " 'Weights',\n",
              " 'and',\n",
              " 'Measures',\n",
              " '(BIPM)',\n",
              " 'in',\n",
              " 'Paris',\n",
              " 'recommends',\n",
              " 'new',\n",
              " 'names—ronna',\n",
              " 'and',\n",
              " 'quecca—as',\n",
              " 'prefixes',\n",
              " 'for',\n",
              " '1027',\n",
              " 'and',\n",
              " '1030.',\n",
              " '1',\n",
              " 'gigabyte',\n",
              " '=',\n",
              " '109',\n",
              " 'bytes',\n",
              " '1',\n",
              " 'terabyte',\n",
              " '=',\n",
              " '1012',\n",
              " 'bytes',\n",
              " '1',\n",
              " 'petabyte',\n",
              " '=',\n",
              " '1015',\n",
              " 'bytes',\n",
              " '1',\n",
              " 'exabyte',\n",
              " '=',\n",
              " '1018',\n",
              " 'bytes',\n",
              " '1',\n",
              " 'zetabyte',\n",
              " '=',\n",
              " '1021',\n",
              " 'bytes',\n",
              " '1',\n",
              " 'yottabyte',\n",
              " '=',\n",
              " '1024',\n",
              " 'bytes',\n",
              " '1',\n",
              " 'ronnabyte',\n",
              " '=',\n",
              " '1027',\n",
              " 'bytes',\n",
              " '1',\n",
              " 'queccabyte',\n",
              " '=',\n",
              " '1030',\n",
              " 'bytes',\n",
              " 'https://www.science.org/content/article/you-know-kilo-mega-and-giga-metric-system-ready-ronna-and-quecca',\n",
              " 'Is',\n",
              " 'this',\n",
              " 'really',\n",
              " 'problematic?',\n",
              " 'Assume',\n",
              " 'you’re',\n",
              " 'google',\n",
              " 'at',\n",
              " 'the',\n",
              " 'beginning',\n",
              " 'of',\n",
              " 'the',\n",
              " '2000’s',\n",
              " 'and',\n",
              " 'you',\n",
              " 'want',\n",
              " 'to',\n",
              " 'index',\n",
              " 'the',\n",
              " 'web',\n",
              " 'Is',\n",
              " 'this',\n",
              " 'really',\n",
              " 'problematic?',\n",
              " '“',\n",
              " 'Although',\n",
              " 'it',\n",
              " 'is',\n",
              " 'hard',\n",
              " 'to',\n",
              " 'accurately',\n",
              " 'determine',\n",
              " 'the',\n",
              " 'size',\n",
              " 'of',\n",
              " 'the',\n",
              " 'Web',\n",
              " 'at',\n",
              " 'any',\n",
              " 'point',\n",
              " 'in',\n",
              " 'time,',\n",
              " 'it',\n",
              " 'is',\n",
              " 'safe',\n",
              " 'to',\n",
              " 'say',\n",
              " 'that',\n",
              " 'it',\n",
              " 'consists',\n",
              " 'of',\n",
              " 'hundreds',\n",
              " 'of',\n",
              " 'billions',\n",
              " 'of',\n",
              " 'individual',\n",
              " 'documents',\n",
              " 'and',\n",
              " 'that',\n",
              " 'it',\n",
              " 'continues',\n",
              " 'to',\n",
              " 'grow.',\n",
              " 'If',\n",
              " 'we',\n",
              " 'assume',\n",
              " 'the',\n",
              " 'Web',\n",
              " 'to',\n",
              " 'contain',\n",
              " '100',\n",
              " 'billion',\n",
              " 'documents,',\n",
              " 'with',\n",
              " 'an',\n",
              " 'average',\n",
              " 'document',\n",
              " 'size',\n",
              " 'of',\n",
              " '4',\n",
              " 'kB',\n",
              " '(after',\n",
              " 'compression),',\n",
              " 'the',\n",
              " 'Web',\n",
              " 'is',\n",
              " 'about',\n",
              " '400',\n",
              " 'TB',\n",
              " '…',\n",
              " '…',\n",
              " 'The',\n",
              " 'size',\n",
              " 'of',\n",
              " 'the',\n",
              " 'resulting',\n",
              " 'inverted',\n",
              " '(web',\n",
              " 'search)',\n",
              " 'index',\n",
              " 'depends',\n",
              " 'on',\n",
              " 'the',\n",
              " 'specific',\n",
              " 'implementation,',\n",
              " 'but',\n",
              " 'it',\n",
              " 'tends',\n",
              " 'to',\n",
              " 'be',\n",
              " 'on',\n",
              " 'the',\n",
              " 'same',\n",
              " 'order',\n",
              " 'of',\n",
              " 'magnitude',\n",
              " 'as',\n",
              " 'the',\n",
              " 'original',\n",
              " 'repository.',\n",
              " 'Source:',\n",
              " 'The',\n",
              " 'Datacenter',\n",
              " 'as',\n",
              " 'a',\n",
              " 'Computer',\n",
              " 'Luiz',\n",
              " 'André',\n",
              " 'Barroso',\n",
              " '(Google)',\n",
              " 'Jimmy',\n",
              " 'Clidaras',\n",
              " '(Google)',\n",
              " 'Urs',\n",
              " 'Hölzle',\n",
              " '(Google)',\n",
              " 'BIG',\n",
              " 'DATA:',\n",
              " 'SOME',\n",
              " 'DEFINITIONS',\n",
              " '(1/4)',\n",
              " 'Big',\n",
              " 'Data',\n",
              " 'is',\n",
              " 'a',\n",
              " 'term',\n",
              " 'encompassing',\n",
              " 'the',\n",
              " 'use',\n",
              " 'of',\n",
              " 'techniques',\n",
              " 'to',\n",
              " 'capture,',\n",
              " 'pro-',\n",
              " 'cess,',\n",
              " 'analyse',\n",
              " 'and',\n",
              " 'visualize',\n",
              " 'potentially',\n",
              " 'large',\n",
              " 'datasets',\n",
              " 'in',\n",
              " 'a',\n",
              " 'reasonable',\n",
              " 'timeframe',\n",
              " 'not',\n",
              " 'accessible',\n",
              " 'to',\n",
              " 'standard',\n",
              " 'IT',\n",
              " 'technologies.',\n",
              " 'By',\n",
              " 'extension,',\n",
              " 'the',\n",
              " 'platform,',\n",
              " 'tools',\n",
              " 'and',\n",
              " 'software',\n",
              " 'used',\n",
              " 'for',\n",
              " 'this',\n",
              " 'purpose',\n",
              " 'are',\n",
              " 'collectively',\n",
              " 'called',\n",
              " 'Big',\n",
              " 'Data',\n",
              " 'technologies.',\n",
              " '“',\n",
              " '–',\n",
              " 'Nessi1',\n",
              " '1The',\n",
              " 'European',\n",
              " 'Technology',\n",
              " 'Platform',\n",
              " 'dedicated',\n",
              " 'to',\n",
              " 'Software,',\n",
              " 'Services',\n",
              " 'and',\n",
              " 'Data',\n",
              " '5',\n",
              " 'BIG',\n",
              " 'DATA:',\n",
              " 'SOME',\n",
              " 'DEFINITIONS',\n",
              " '(2/4)',\n",
              " 'Big',\n",
              " 'data',\n",
              " 'refers',\n",
              " 'to',\n",
              " 'datasets',\n",
              " 'whose',\n",
              " 'size',\n",
              " 'is',\n",
              " 'beyond',\n",
              " 'the',\n",
              " 'ability',\n",
              " 'of',\n",
              " 'typical',\n",
              " 'database',\n",
              " 'software',\n",
              " 'tools',\n",
              " 'to',\n",
              " 'capture,',\n",
              " 'store,',\n",
              " 'manage,',\n",
              " 'and',\n",
              " 'analyze.',\n",
              " 'This',\n",
              " 'definition',\n",
              " 'is',\n",
              " 'intentionally',\n",
              " 'subjective',\n",
              " 'and',\n",
              " 'incorporates',\n",
              " 'a',\n",
              " 'moving',\n",
              " 'defi-',\n",
              " 'nition',\n",
              " 'of',\n",
              " 'how',\n",
              " 'big',\n",
              " 'a',\n",
              " 'dataset',\n",
              " 'needs',\n",
              " 'to',\n",
              " 'be',\n",
              " 'in',\n",
              " 'order',\n",
              " 'to',\n",
              " 'be',\n",
              " 'considered',\n",
              " 'big',\n",
              " 'data',\n",
              " '…',\n",
              " 'We',\n",
              " 'assume',\n",
              " 'that,',\n",
              " 'as',\n",
              " 'technology',\n",
              " 'advances',\n",
              " 'over',\n",
              " 'time,',\n",
              " 'the',\n",
              " 'size',\n",
              " 'of',\n",
              " 'datasets',\n",
              " 'that',\n",
              " 'qualify',\n",
              " 'as',\n",
              " 'big',\n",
              " 'data',\n",
              " 'will',\n",
              " 'also',\n",
              " 'increase.',\n",
              " '“',\n",
              " '–',\n",
              " 'MCKinsey',\n",
              " 'Global',\n",
              " 'Institute',\n",
              " '2',\n",
              " '2Big',\n",
              " 'data:',\n",
              " 'The',\n",
              " 'next',\n",
              " 'frontier',\n",
              " 'for',\n",
              " 'innovation,',\n",
              " 'competition,',\n",
              " 'and',\n",
              " 'productivity,',\n",
              " 'June',\n",
              " '2011',\n",
              " '6',\n",
              " 'BIG',\n",
              " 'DATA:',\n",
              " 'SOME',\n",
              " 'DEFINITIONS',\n",
              " '(3/4)',\n",
              " 'Big',\n",
              " 'data',\n",
              " 'are',\n",
              " 'data',\n",
              " 'sets',\n",
              " 'that',\n",
              " 'are',\n",
              " 'so',\n",
              " 'voluminous',\n",
              " 'and',\n",
              " 'complex',\n",
              " 'that',\n",
              " 'tra-',\n",
              " 'ditional',\n",
              " 'data',\n",
              " 'processing',\n",
              " 'application',\n",
              " 'software',\n",
              " 'are',\n",
              " 'inadequate',\n",
              " 'to',\n",
              " 'deal',\n",
              " 'with',\n",
              " 'them.',\n",
              " 'Big',\n",
              " 'data',\n",
              " 'challenges',\n",
              " 'include',\n",
              " 'capturing',\n",
              " 'data,',\n",
              " 'data',\n",
              " 'storage,',\n",
              " 'data',\n",
              " 'analysis,',\n",
              " 'search,',\n",
              " 'sharing,',\n",
              " 'transfer,',\n",
              " 'visualization,',\n",
              " 'querying,',\n",
              " 'updat-',\n",
              " 'ing',\n",
              " 'and',\n",
              " 'information',\n",
              " 'privacy.',\n",
              " 'Lately,',\n",
              " 'the',\n",
              " 'term',\n",
              " 'big',\n",
              " 'data',\n",
              " 'tends',\n",
              " 'to',\n",
              " 'refer',\n",
              " 'to',\n",
              " 'the',\n",
              " 'use',\n",
              " 'of',\n",
              " 'predictive',\n",
              " 'analyt-',\n",
              " 'ics,',\n",
              " 'user',\n",
              " 'behavior',\n",
              " 'analytics,',\n",
              " 'or',\n",
              " 'certain',\n",
              " 'other',\n",
              " 'advanced',\n",
              " 'data',\n",
              " 'analytics',\n",
              " 'methods',\n",
              " 'that',\n",
              " 'extract',\n",
              " 'value',\n",
              " 'from',\n",
              " 'data,',\n",
              " 'and',\n",
              " 'seldom',\n",
              " 'to',\n",
              " 'a',\n",
              " 'particular',\n",
              " 'size',\n",
              " 'of',\n",
              " 'data',\n",
              " 'set.',\n",
              " '“',\n",
              " '—Wikipedia',\n",
              " '7',\n",
              " 'BIG',\n",
              " 'DATA:',\n",
              " 'SOME',\n",
              " 'DEFINITIONS',\n",
              " '(4/4)',\n",
              " 'Big',\n",
              " 'data',\n",
              " 'refers',\n",
              " 'to',\n",
              " 'extremely',\n",
              " 'large',\n",
              " 'datasets',\n",
              " 'that',\n",
              " 'cannot',\n",
              " 'be',\n",
              " 'easily',\n",
              " 'man-',\n",
              " 'aged,',\n",
              " 'processed,',\n",
              " 'or',\n",
              " 'analyzed',\n",
              " 'using',\n",
              " 'traditional',\n",
              " 'data',\n",
              " 'processing',\n",
              " 'meth-',\n",
              " 'ods.',\n",
              " 'It',\n",
              " 'is',\n",
              " 'characterized',\n",
              " 'by',\n",
              " 'the',\n",
              " 'three',\n",
              " 'V’s:',\n",
              " 'volume,',\n",
              " 'variety,',\n",
              " 'and',\n",
              " 'velocity,',\n",
              " 'which',\n",
              " 'describe',\n",
              " 'the',\n",
              " 'sheer',\n",
              " 'amount',\n",
              " 'of',\n",
              " 'data,',\n",
              " 'the',\n",
              " 'diversity',\n",
              " 'of',\n",
              " 'data',\n",
              " 'types,',\n",
              " 'and',\n",
              " 'the',\n",
              " 'speed',\n",
              " 'at',\n",
              " 'which',\n",
              " 'it',\n",
              " 'is',\n",
              " 'generated.',\n",
              " 'Big',\n",
              " 'data',\n",
              " 'technologies',\n",
              " 'enable',\n",
              " 'businesses',\n",
              " 'and',\n",
              " 'researchers',\n",
              " 'to',\n",
              " 'gain',\n",
              " 'insights',\n",
              " 'and',\n",
              " 'make',\n",
              " 'data-driven',\n",
              " 'de-',\n",
              " 'cisions',\n",
              " 'from',\n",
              " 'these',\n",
              " 'vast,',\n",
              " 'dynamic',\n",
              " 'datasets.',\n",
              " '“',\n",
              " '—ChatGPT',\n",
              " '(02/2025)',\n",
              " '8',\n",
              " 'BIG',\n",
              " 'DATA',\n",
              " 'DATA',\n",
              " 'ANALYTICS',\n",
              " 'DATA',\n",
              " 'SCIENCE',\n",
              " 'THE',\n",
              " 'BIG',\n",
              " 'THREE',\n",
              " '9',\n",
              " 'DATA',\n",
              " 'ANALYTICS',\n",
              " 'THE',\n",
              " 'BIG',\n",
              " 'THREE',\n",
              " '9',\n",
              " 'DATA',\n",
              " 'ANALYTICS:',\n",
              " 'A',\n",
              " 'DEFINITION',\n",
              " 'Data',\n",
              " 'analysis,',\n",
              " 'also',\n",
              " 'known',\n",
              " 'as',\n",
              " 'analysis',\n",
              " 'of',\n",
              " 'data',\n",
              " 'or',\n",
              " 'data',\n",
              " 'analytics,',\n",
              " 'is',\n",
              " 'a',\n",
              " 'process',\n",
              " 'of',\n",
              " 'inspecting,',\n",
              " 'cleansing,',\n",
              " 'transforming,',\n",
              " 'and',\n",
              " 'modeling',\n",
              " 'data',\n",
              " 'with',\n",
              " 'the',\n",
              " 'goal',\n",
              " 'of',\n",
              " 'discovering',\n",
              " 'useful',\n",
              " 'information,',\n",
              " 'suggesting',\n",
              " 'conclu-',\n",
              " 'sions,',\n",
              " 'and',\n",
              " 'supporting',\n",
              " 'decision-making.',\n",
              " '“',\n",
              " '—Wikipedia',\n",
              " '10',\n",
              " '11',\n",
              " 'BIG',\n",
              " 'DATA',\n",
              " 'DATA',\n",
              " 'ANALYTICS',\n",
              " 'DATA',\n",
              " 'SCIENCE',\n",
              " 'THE',\n",
              " 'BIG',\n",
              " 'THREE',\n",
              " '12',\n",
              " 'DATA',\n",
              " 'SCIENCE',\n",
              " 'THE',\n",
              " 'BIG',\n",
              " 'THREE',\n",
              " '12',\n",
              " 'DATA',\n",
              " 'SCIENCE:',\n",
              " 'SOME',\n",
              " 'DEFINITIONS',\n",
              " '(1/3)',\n",
              " 'Data',\n",
              " 'science',\n",
              " 'is',\n",
              " 'the',\n",
              " 'study',\n",
              " 'of',\n",
              " 'the',\n",
              " 'generalizable',\n",
              " 'extraction',\n",
              " 'of',\n",
              " 'knowledge',\n",
              " 'from',\n",
              " 'data.',\n",
              " '“',\n",
              " '—Vasant',\n",
              " 'Dhar3',\n",
              " 'From',\n",
              " 'the',\n",
              " 'article:',\n",
              " '•',\n",
              " 'The',\n",
              " 'term',\n",
              " 'science',\n",
              " 'implies',\n",
              " 'knowledge',\n",
              " 'gained',\n",
              " 'through',\n",
              " 'systematic',\n",
              " 'study.',\n",
              " '•',\n",
              " 'A',\n",
              " 'data',\n",
              " 'scientist',\n",
              " 'requires',\n",
              " 'an',\n",
              " 'integrated',\n",
              " 'skill',\n",
              " 'set',\n",
              " 'spanning',\n",
              " 'mathematics,',\n",
              " 'machine',\n",
              " 'learning,',\n",
              " 'artificial',\n",
              " 'intelligence,',\n",
              " 'statistics,',\n",
              " 'databases,',\n",
              " 'and',\n",
              " 'optimization,',\n",
              " 'along',\n",
              " 'with',\n",
              " 'a',\n",
              " 'deep',\n",
              " 'understanding',\n",
              " 'of',\n",
              " 'the',\n",
              " 'craft',\n",
              " 'of',\n",
              " 'problem',\n",
              " 'formulation',\n",
              " 'to',\n",
              " 'engineer',\n",
              " 'effective',\n",
              " 'solutions.',\n",
              " '3Data',\n",
              " 'Science',\n",
              " 'and',\n",
              " 'Predictions.',\n",
              " 'Communications',\n",
              " 'of',\n",
              " 'the',\n",
              " 'ACM',\n",
              " '56(12),',\n",
              " '2014.',\n",
              " '13',\n",
              " 'DATA',\n",
              " 'SCIENCE:',\n",
              " 'SOME',\n",
              " 'DEFINITIONS',\n",
              " '(2/3)',\n",
              " 'Data',\n",
              " 'science,',\n",
              " 'also',\n",
              " 'known',\n",
              " 'as',\n",
              " 'data-driven',\n",
              " 'science,',\n",
              " 'is',\n",
              " 'an',\n",
              " 'interdisci-',\n",
              " 'plinary',\n",
              " 'field',\n",
              " 'about',\n",
              " 'scientific',\n",
              " 'methods,',\n",
              " 'processes,',\n",
              " 'and',\n",
              " 'systems',\n",
              " 'to',\n",
              " 'extract',\n",
              " 'knowledge',\n",
              " 'or',\n",
              " 'insights',\n",
              " 'from',\n",
              " 'data',\n",
              " 'in',\n",
              " 'various',\n",
              " 'forms,',\n",
              " 'either',\n",
              " 'structured',\n",
              " 'or',\n",
              " 'unstructured.',\n",
              " 'Data',\n",
              " 'science',\n",
              " ...]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lowercase_tokens(tokens):\n",
        "    \"\"\"\n",
        "    Converts all tokens to lowercase.\n",
        "\n",
        "    Args:\n",
        "        tokens (list of str): List of word tokens.\n",
        "\n",
        "    Returns:\n",
        "        list of str: Same list with all words in lowercase.\n",
        "    \"\"\"\n",
        "    return [token.lower() for token in tokens]\n"
      ],
      "metadata": {
        "id": "O-YOyxnfte1x"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_words_list_lowered = lowercase_tokens(raw_words_list)"
      ],
      "metadata": {
        "id": "nFcYeNo3tgZf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(raw_words_list_lowered)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vZrsmobDuK4E",
        "outputId": "210f9eca-bed7-46d7-c333-bb23df815765"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "['info-h515:',\n",
              " 'big',\n",
              " 'data:',\n",
              " 'distributed',\n",
              " 'management',\n",
              " 'lecture',\n",
              " '1:',\n",
              " 'introduction,',\n",
              " 'hdfs,',\n",
              " 'map/reduce',\n",
              " 'dimitris',\n",
              " 'sacharidis',\n",
              " 'lecture',\n",
              " 'outline',\n",
              " 'what',\n",
              " 'is',\n",
              " 'big',\n",
              " 'data?',\n",
              " 'how',\n",
              " 'is',\n",
              " 'big',\n",
              " 'data',\n",
              " 'used?',\n",
              " 'big',\n",
              " 'datacenters',\n",
              " 'hdfs',\n",
              " 'map/reduce',\n",
              " '1',\n",
              " 'what',\n",
              " 'is',\n",
              " 'big',\n",
              " 'data?',\n",
              " '“big',\n",
              " 'data',\n",
              " 'is',\n",
              " 'like',\n",
              " 'teenage',\n",
              " 'sex:',\n",
              " 'everyone',\n",
              " 'talks',\n",
              " 'about',\n",
              " 'it,',\n",
              " 'nobody',\n",
              " 'really',\n",
              " 'knows',\n",
              " 'how',\n",
              " 'to',\n",
              " 'do',\n",
              " 'it,',\n",
              " 'everyone',\n",
              " 'thinks',\n",
              " 'everyone',\n",
              " 'else',\n",
              " 'is',\n",
              " 'doing',\n",
              " 'it,',\n",
              " 'so',\n",
              " 'everyone',\n",
              " 'claims',\n",
              " 'they',\n",
              " 'are',\n",
              " 'doing',\n",
              " 'it',\n",
              " '…”',\n",
              " '—dan',\n",
              " 'ariely',\n",
              " '3',\n",
              " 'big',\n",
              " 'data',\n",
              " 'data',\n",
              " 'analytics',\n",
              " 'data',\n",
              " 'science',\n",
              " 'the',\n",
              " 'big',\n",
              " 'three',\n",
              " '4',\n",
              " 'big',\n",
              " 'data',\n",
              " 'the',\n",
              " 'big',\n",
              " 'three',\n",
              " '4',\n",
              " 'big',\n",
              " 'data',\n",
              " 'some',\n",
              " 'history',\n",
              " 'the',\n",
              " 'big',\n",
              " 'three',\n",
              " '4',\n",
              " 'source:',\n",
              " 'the',\n",
              " 'economist,',\n",
              " 'february',\n",
              " '25,',\n",
              " '2010',\n",
              " 'the',\n",
              " 'data',\n",
              " 'deluge',\n",
              " '“',\n",
              " 'eighteen',\n",
              " 'months',\n",
              " 'ago,',\n",
              " 'li',\n",
              " '&',\n",
              " 'fung,',\n",
              " 'a',\n",
              " 'firm',\n",
              " 'that',\n",
              " 'manages',\n",
              " 'supply',\n",
              " 'chains',\n",
              " 'for',\n",
              " 'retailers,',\n",
              " 'saw',\n",
              " '100',\n",
              " 'gigabytes',\n",
              " 'of',\n",
              " 'information',\n",
              " 'flow',\n",
              " 'through',\n",
              " 'its',\n",
              " 'network',\n",
              " 'each',\n",
              " 'day.',\n",
              " 'now',\n",
              " 'the',\n",
              " 'amount',\n",
              " 'has',\n",
              " 'increased',\n",
              " 'tenfold.',\n",
              " 'during',\n",
              " '2009,',\n",
              " 'american',\n",
              " 'drone',\n",
              " 'aircraft',\n",
              " 'flying',\n",
              " 'over',\n",
              " 'iraq',\n",
              " 'and',\n",
              " 'afghanistan',\n",
              " 'sent',\n",
              " 'back',\n",
              " 'around',\n",
              " '24',\n",
              " \"years'\",\n",
              " 'worth',\n",
              " 'of',\n",
              " 'video',\n",
              " 'footage.',\n",
              " 'new',\n",
              " 'models',\n",
              " 'being',\n",
              " 'deployed',\n",
              " 'this',\n",
              " 'year',\n",
              " 'will',\n",
              " 'produce',\n",
              " 'ten',\n",
              " 'times',\n",
              " 'as',\n",
              " 'many',\n",
              " 'data',\n",
              " 'streams',\n",
              " 'as',\n",
              " 'their',\n",
              " 'predecessors,',\n",
              " 'and',\n",
              " 'those',\n",
              " 'in',\n",
              " '2011',\n",
              " 'will',\n",
              " 'produce',\n",
              " '30',\n",
              " 'times',\n",
              " 'as',\n",
              " 'many.',\n",
              " 'source:',\n",
              " 'the',\n",
              " 'economist,',\n",
              " 'february',\n",
              " '25,',\n",
              " '2010',\n",
              " 'the',\n",
              " 'data',\n",
              " 'deluge',\n",
              " '“',\n",
              " 'everywhere',\n",
              " 'you',\n",
              " 'look,',\n",
              " 'the',\n",
              " 'quantity',\n",
              " 'of',\n",
              " 'information',\n",
              " 'in',\n",
              " 'the',\n",
              " 'world',\n",
              " 'is',\n",
              " 'soaring.',\n",
              " 'according',\n",
              " 'to',\n",
              " 'one',\n",
              " 'estimate,',\n",
              " 'mankind',\n",
              " 'created',\n",
              " '150',\n",
              " 'exabytes',\n",
              " '(billion',\n",
              " 'gigabytes)',\n",
              " 'of',\n",
              " 'data',\n",
              " 'in',\n",
              " '2005.',\n",
              " 'this',\n",
              " 'year,',\n",
              " 'it',\n",
              " 'will',\n",
              " 'create',\n",
              " '1,200',\n",
              " 'exabytes.',\n",
              " 'merely',\n",
              " 'keeping',\n",
              " 'up',\n",
              " 'with',\n",
              " 'this',\n",
              " 'flood,',\n",
              " 'and',\n",
              " 'storing',\n",
              " 'the',\n",
              " 'bits',\n",
              " 'that',\n",
              " 'might',\n",
              " 'be',\n",
              " 'useful,',\n",
              " 'is',\n",
              " 'difficult',\n",
              " 'enough.',\n",
              " 'analysing',\n",
              " 'it,',\n",
              " 'to',\n",
              " 'spot',\n",
              " 'patterns',\n",
              " 'and',\n",
              " 'extract',\n",
              " 'useful',\n",
              " 'information,',\n",
              " 'is',\n",
              " 'harder',\n",
              " 'still.',\n",
              " 'source:',\n",
              " 'the',\n",
              " 'economist,',\n",
              " 'february',\n",
              " '25,',\n",
              " '2010',\n",
              " 'the',\n",
              " 'data',\n",
              " 'deluge',\n",
              " '“',\n",
              " 'by',\n",
              " 'the',\n",
              " '2030s,',\n",
              " 'computer',\n",
              " 'data',\n",
              " 'storage',\n",
              " 'may',\n",
              " 'surpass',\n",
              " '1',\n",
              " 'yottabyte',\n",
              " '(1024),',\n",
              " 'the',\n",
              " 'largest',\n",
              " 'number',\n",
              " 'with',\n",
              " 'an',\n",
              " 'official',\n",
              " 'metric',\n",
              " 'prefix.',\n",
              " 'the',\n",
              " 'international',\n",
              " 'bureau',\n",
              " 'of',\n",
              " 'weights',\n",
              " 'and',\n",
              " 'measures',\n",
              " '(bipm)',\n",
              " 'in',\n",
              " 'paris',\n",
              " 'recommends',\n",
              " 'new',\n",
              " 'names—ronna',\n",
              " 'and',\n",
              " 'quecca—as',\n",
              " 'prefixes',\n",
              " 'for',\n",
              " '1027',\n",
              " 'and',\n",
              " '1030.',\n",
              " '1',\n",
              " 'gigabyte',\n",
              " '=',\n",
              " '109',\n",
              " 'bytes',\n",
              " '1',\n",
              " 'terabyte',\n",
              " '=',\n",
              " '1012',\n",
              " 'bytes',\n",
              " '1',\n",
              " 'petabyte',\n",
              " '=',\n",
              " '1015',\n",
              " 'bytes',\n",
              " '1',\n",
              " 'exabyte',\n",
              " '=',\n",
              " '1018',\n",
              " 'bytes',\n",
              " '1',\n",
              " 'zetabyte',\n",
              " '=',\n",
              " '1021',\n",
              " 'bytes',\n",
              " '1',\n",
              " 'yottabyte',\n",
              " '=',\n",
              " '1024',\n",
              " 'bytes',\n",
              " '1',\n",
              " 'ronnabyte',\n",
              " '=',\n",
              " '1027',\n",
              " 'bytes',\n",
              " '1',\n",
              " 'queccabyte',\n",
              " '=',\n",
              " '1030',\n",
              " 'bytes',\n",
              " 'https://www.science.org/content/article/you-know-kilo-mega-and-giga-metric-system-ready-ronna-and-quecca',\n",
              " 'is',\n",
              " 'this',\n",
              " 'really',\n",
              " 'problematic?',\n",
              " 'assume',\n",
              " 'you’re',\n",
              " 'google',\n",
              " 'at',\n",
              " 'the',\n",
              " 'beginning',\n",
              " 'of',\n",
              " 'the',\n",
              " '2000’s',\n",
              " 'and',\n",
              " 'you',\n",
              " 'want',\n",
              " 'to',\n",
              " 'index',\n",
              " 'the',\n",
              " 'web',\n",
              " 'is',\n",
              " 'this',\n",
              " 'really',\n",
              " 'problematic?',\n",
              " '“',\n",
              " 'although',\n",
              " 'it',\n",
              " 'is',\n",
              " 'hard',\n",
              " 'to',\n",
              " 'accurately',\n",
              " 'determine',\n",
              " 'the',\n",
              " 'size',\n",
              " 'of',\n",
              " 'the',\n",
              " 'web',\n",
              " 'at',\n",
              " 'any',\n",
              " 'point',\n",
              " 'in',\n",
              " 'time,',\n",
              " 'it',\n",
              " 'is',\n",
              " 'safe',\n",
              " 'to',\n",
              " 'say',\n",
              " 'that',\n",
              " 'it',\n",
              " 'consists',\n",
              " 'of',\n",
              " 'hundreds',\n",
              " 'of',\n",
              " 'billions',\n",
              " 'of',\n",
              " 'individual',\n",
              " 'documents',\n",
              " 'and',\n",
              " 'that',\n",
              " 'it',\n",
              " 'continues',\n",
              " 'to',\n",
              " 'grow.',\n",
              " 'if',\n",
              " 'we',\n",
              " 'assume',\n",
              " 'the',\n",
              " 'web',\n",
              " 'to',\n",
              " 'contain',\n",
              " '100',\n",
              " 'billion',\n",
              " 'documents,',\n",
              " 'with',\n",
              " 'an',\n",
              " 'average',\n",
              " 'document',\n",
              " 'size',\n",
              " 'of',\n",
              " '4',\n",
              " 'kb',\n",
              " '(after',\n",
              " 'compression),',\n",
              " 'the',\n",
              " 'web',\n",
              " 'is',\n",
              " 'about',\n",
              " '400',\n",
              " 'tb',\n",
              " '…',\n",
              " '…',\n",
              " 'the',\n",
              " 'size',\n",
              " 'of',\n",
              " 'the',\n",
              " 'resulting',\n",
              " 'inverted',\n",
              " '(web',\n",
              " 'search)',\n",
              " 'index',\n",
              " 'depends',\n",
              " 'on',\n",
              " 'the',\n",
              " 'specific',\n",
              " 'implementation,',\n",
              " 'but',\n",
              " 'it',\n",
              " 'tends',\n",
              " 'to',\n",
              " 'be',\n",
              " 'on',\n",
              " 'the',\n",
              " 'same',\n",
              " 'order',\n",
              " 'of',\n",
              " 'magnitude',\n",
              " 'as',\n",
              " 'the',\n",
              " 'original',\n",
              " 'repository.',\n",
              " 'source:',\n",
              " 'the',\n",
              " 'datacenter',\n",
              " 'as',\n",
              " 'a',\n",
              " 'computer',\n",
              " 'luiz',\n",
              " 'andré',\n",
              " 'barroso',\n",
              " '(google)',\n",
              " 'jimmy',\n",
              " 'clidaras',\n",
              " '(google)',\n",
              " 'urs',\n",
              " 'hölzle',\n",
              " '(google)',\n",
              " 'big',\n",
              " 'data:',\n",
              " 'some',\n",
              " 'definitions',\n",
              " '(1/4)',\n",
              " 'big',\n",
              " 'data',\n",
              " 'is',\n",
              " 'a',\n",
              " 'term',\n",
              " 'encompassing',\n",
              " 'the',\n",
              " 'use',\n",
              " 'of',\n",
              " 'techniques',\n",
              " 'to',\n",
              " 'capture,',\n",
              " 'pro-',\n",
              " 'cess,',\n",
              " 'analyse',\n",
              " 'and',\n",
              " 'visualize',\n",
              " 'potentially',\n",
              " 'large',\n",
              " 'datasets',\n",
              " 'in',\n",
              " 'a',\n",
              " 'reasonable',\n",
              " 'timeframe',\n",
              " 'not',\n",
              " 'accessible',\n",
              " 'to',\n",
              " 'standard',\n",
              " 'it',\n",
              " 'technologies.',\n",
              " 'by',\n",
              " 'extension,',\n",
              " 'the',\n",
              " 'platform,',\n",
              " 'tools',\n",
              " 'and',\n",
              " 'software',\n",
              " 'used',\n",
              " 'for',\n",
              " 'this',\n",
              " 'purpose',\n",
              " 'are',\n",
              " 'collectively',\n",
              " 'called',\n",
              " 'big',\n",
              " 'data',\n",
              " 'technologies.',\n",
              " '“',\n",
              " '–',\n",
              " 'nessi1',\n",
              " '1the',\n",
              " 'european',\n",
              " 'technology',\n",
              " 'platform',\n",
              " 'dedicated',\n",
              " 'to',\n",
              " 'software,',\n",
              " 'services',\n",
              " 'and',\n",
              " 'data',\n",
              " '5',\n",
              " 'big',\n",
              " 'data:',\n",
              " 'some',\n",
              " 'definitions',\n",
              " '(2/4)',\n",
              " 'big',\n",
              " 'data',\n",
              " 'refers',\n",
              " 'to',\n",
              " 'datasets',\n",
              " 'whose',\n",
              " 'size',\n",
              " 'is',\n",
              " 'beyond',\n",
              " 'the',\n",
              " 'ability',\n",
              " 'of',\n",
              " 'typical',\n",
              " 'database',\n",
              " 'software',\n",
              " 'tools',\n",
              " 'to',\n",
              " 'capture,',\n",
              " 'store,',\n",
              " 'manage,',\n",
              " 'and',\n",
              " 'analyze.',\n",
              " 'this',\n",
              " 'definition',\n",
              " 'is',\n",
              " 'intentionally',\n",
              " 'subjective',\n",
              " 'and',\n",
              " 'incorporates',\n",
              " 'a',\n",
              " 'moving',\n",
              " 'defi-',\n",
              " 'nition',\n",
              " 'of',\n",
              " 'how',\n",
              " 'big',\n",
              " 'a',\n",
              " 'dataset',\n",
              " 'needs',\n",
              " 'to',\n",
              " 'be',\n",
              " 'in',\n",
              " 'order',\n",
              " 'to',\n",
              " 'be',\n",
              " 'considered',\n",
              " 'big',\n",
              " 'data',\n",
              " '…',\n",
              " 'we',\n",
              " 'assume',\n",
              " 'that,',\n",
              " 'as',\n",
              " 'technology',\n",
              " 'advances',\n",
              " 'over',\n",
              " 'time,',\n",
              " 'the',\n",
              " 'size',\n",
              " 'of',\n",
              " 'datasets',\n",
              " 'that',\n",
              " 'qualify',\n",
              " 'as',\n",
              " 'big',\n",
              " 'data',\n",
              " 'will',\n",
              " 'also',\n",
              " 'increase.',\n",
              " '“',\n",
              " '–',\n",
              " 'mckinsey',\n",
              " 'global',\n",
              " 'institute',\n",
              " '2',\n",
              " '2big',\n",
              " 'data:',\n",
              " 'the',\n",
              " 'next',\n",
              " 'frontier',\n",
              " 'for',\n",
              " 'innovation,',\n",
              " 'competition,',\n",
              " 'and',\n",
              " 'productivity,',\n",
              " 'june',\n",
              " '2011',\n",
              " '6',\n",
              " 'big',\n",
              " 'data:',\n",
              " 'some',\n",
              " 'definitions',\n",
              " '(3/4)',\n",
              " 'big',\n",
              " 'data',\n",
              " 'are',\n",
              " 'data',\n",
              " 'sets',\n",
              " 'that',\n",
              " 'are',\n",
              " 'so',\n",
              " 'voluminous',\n",
              " 'and',\n",
              " 'complex',\n",
              " 'that',\n",
              " 'tra-',\n",
              " 'ditional',\n",
              " 'data',\n",
              " 'processing',\n",
              " 'application',\n",
              " 'software',\n",
              " 'are',\n",
              " 'inadequate',\n",
              " 'to',\n",
              " 'deal',\n",
              " 'with',\n",
              " 'them.',\n",
              " 'big',\n",
              " 'data',\n",
              " 'challenges',\n",
              " 'include',\n",
              " 'capturing',\n",
              " 'data,',\n",
              " 'data',\n",
              " 'storage,',\n",
              " 'data',\n",
              " 'analysis,',\n",
              " 'search,',\n",
              " 'sharing,',\n",
              " 'transfer,',\n",
              " 'visualization,',\n",
              " 'querying,',\n",
              " 'updat-',\n",
              " 'ing',\n",
              " 'and',\n",
              " 'information',\n",
              " 'privacy.',\n",
              " 'lately,',\n",
              " 'the',\n",
              " 'term',\n",
              " 'big',\n",
              " 'data',\n",
              " 'tends',\n",
              " 'to',\n",
              " 'refer',\n",
              " 'to',\n",
              " 'the',\n",
              " 'use',\n",
              " 'of',\n",
              " 'predictive',\n",
              " 'analyt-',\n",
              " 'ics,',\n",
              " 'user',\n",
              " 'behavior',\n",
              " 'analytics,',\n",
              " 'or',\n",
              " 'certain',\n",
              " 'other',\n",
              " 'advanced',\n",
              " 'data',\n",
              " 'analytics',\n",
              " 'methods',\n",
              " 'that',\n",
              " 'extract',\n",
              " 'value',\n",
              " 'from',\n",
              " 'data,',\n",
              " 'and',\n",
              " 'seldom',\n",
              " 'to',\n",
              " 'a',\n",
              " 'particular',\n",
              " 'size',\n",
              " 'of',\n",
              " 'data',\n",
              " 'set.',\n",
              " '“',\n",
              " '—wikipedia',\n",
              " '7',\n",
              " 'big',\n",
              " 'data:',\n",
              " 'some',\n",
              " 'definitions',\n",
              " '(4/4)',\n",
              " 'big',\n",
              " 'data',\n",
              " 'refers',\n",
              " 'to',\n",
              " 'extremely',\n",
              " 'large',\n",
              " 'datasets',\n",
              " 'that',\n",
              " 'cannot',\n",
              " 'be',\n",
              " 'easily',\n",
              " 'man-',\n",
              " 'aged,',\n",
              " 'processed,',\n",
              " 'or',\n",
              " 'analyzed',\n",
              " 'using',\n",
              " 'traditional',\n",
              " 'data',\n",
              " 'processing',\n",
              " 'meth-',\n",
              " 'ods.',\n",
              " 'it',\n",
              " 'is',\n",
              " 'characterized',\n",
              " 'by',\n",
              " 'the',\n",
              " 'three',\n",
              " 'v’s:',\n",
              " 'volume,',\n",
              " 'variety,',\n",
              " 'and',\n",
              " 'velocity,',\n",
              " 'which',\n",
              " 'describe',\n",
              " 'the',\n",
              " 'sheer',\n",
              " 'amount',\n",
              " 'of',\n",
              " 'data,',\n",
              " 'the',\n",
              " 'diversity',\n",
              " 'of',\n",
              " 'data',\n",
              " 'types,',\n",
              " 'and',\n",
              " 'the',\n",
              " 'speed',\n",
              " 'at',\n",
              " 'which',\n",
              " 'it',\n",
              " 'is',\n",
              " 'generated.',\n",
              " 'big',\n",
              " 'data',\n",
              " 'technologies',\n",
              " 'enable',\n",
              " 'businesses',\n",
              " 'and',\n",
              " 'researchers',\n",
              " 'to',\n",
              " 'gain',\n",
              " 'insights',\n",
              " 'and',\n",
              " 'make',\n",
              " 'data-driven',\n",
              " 'de-',\n",
              " 'cisions',\n",
              " 'from',\n",
              " 'these',\n",
              " 'vast,',\n",
              " 'dynamic',\n",
              " 'datasets.',\n",
              " '“',\n",
              " '—chatgpt',\n",
              " '(02/2025)',\n",
              " '8',\n",
              " 'big',\n",
              " 'data',\n",
              " 'data',\n",
              " 'analytics',\n",
              " 'data',\n",
              " 'science',\n",
              " 'the',\n",
              " 'big',\n",
              " 'three',\n",
              " '9',\n",
              " 'data',\n",
              " 'analytics',\n",
              " 'the',\n",
              " 'big',\n",
              " 'three',\n",
              " '9',\n",
              " 'data',\n",
              " 'analytics:',\n",
              " 'a',\n",
              " 'definition',\n",
              " 'data',\n",
              " 'analysis,',\n",
              " 'also',\n",
              " 'known',\n",
              " 'as',\n",
              " 'analysis',\n",
              " 'of',\n",
              " 'data',\n",
              " 'or',\n",
              " 'data',\n",
              " 'analytics,',\n",
              " 'is',\n",
              " 'a',\n",
              " 'process',\n",
              " 'of',\n",
              " 'inspecting,',\n",
              " 'cleansing,',\n",
              " 'transforming,',\n",
              " 'and',\n",
              " 'modeling',\n",
              " 'data',\n",
              " 'with',\n",
              " 'the',\n",
              " 'goal',\n",
              " 'of',\n",
              " 'discovering',\n",
              " 'useful',\n",
              " 'information,',\n",
              " 'suggesting',\n",
              " 'conclu-',\n",
              " 'sions,',\n",
              " 'and',\n",
              " 'supporting',\n",
              " 'decision-making.',\n",
              " '“',\n",
              " '—wikipedia',\n",
              " '10',\n",
              " '11',\n",
              " 'big',\n",
              " 'data',\n",
              " 'data',\n",
              " 'analytics',\n",
              " 'data',\n",
              " 'science',\n",
              " 'the',\n",
              " 'big',\n",
              " 'three',\n",
              " '12',\n",
              " 'data',\n",
              " 'science',\n",
              " 'the',\n",
              " 'big',\n",
              " 'three',\n",
              " '12',\n",
              " 'data',\n",
              " 'science:',\n",
              " 'some',\n",
              " 'definitions',\n",
              " '(1/3)',\n",
              " 'data',\n",
              " 'science',\n",
              " 'is',\n",
              " 'the',\n",
              " 'study',\n",
              " 'of',\n",
              " 'the',\n",
              " 'generalizable',\n",
              " 'extraction',\n",
              " 'of',\n",
              " 'knowledge',\n",
              " 'from',\n",
              " 'data.',\n",
              " '“',\n",
              " '—vasant',\n",
              " 'dhar3',\n",
              " 'from',\n",
              " 'the',\n",
              " 'article:',\n",
              " '•',\n",
              " 'the',\n",
              " 'term',\n",
              " 'science',\n",
              " 'implies',\n",
              " 'knowledge',\n",
              " 'gained',\n",
              " 'through',\n",
              " 'systematic',\n",
              " 'study.',\n",
              " '•',\n",
              " 'a',\n",
              " 'data',\n",
              " 'scientist',\n",
              " 'requires',\n",
              " 'an',\n",
              " 'integrated',\n",
              " 'skill',\n",
              " 'set',\n",
              " 'spanning',\n",
              " 'mathematics,',\n",
              " 'machine',\n",
              " 'learning,',\n",
              " 'artificial',\n",
              " 'intelligence,',\n",
              " 'statistics,',\n",
              " 'databases,',\n",
              " 'and',\n",
              " 'optimization,',\n",
              " 'along',\n",
              " 'with',\n",
              " 'a',\n",
              " 'deep',\n",
              " 'understanding',\n",
              " 'of',\n",
              " 'the',\n",
              " 'craft',\n",
              " 'of',\n",
              " 'problem',\n",
              " 'formulation',\n",
              " 'to',\n",
              " 'engineer',\n",
              " 'effective',\n",
              " 'solutions.',\n",
              " '3data',\n",
              " 'science',\n",
              " 'and',\n",
              " 'predictions.',\n",
              " 'communications',\n",
              " 'of',\n",
              " 'the',\n",
              " 'acm',\n",
              " '56(12),',\n",
              " '2014.',\n",
              " '13',\n",
              " 'data',\n",
              " 'science:',\n",
              " 'some',\n",
              " 'definitions',\n",
              " '(2/3)',\n",
              " 'data',\n",
              " 'science,',\n",
              " 'also',\n",
              " 'known',\n",
              " 'as',\n",
              " 'data-driven',\n",
              " 'science,',\n",
              " 'is',\n",
              " 'an',\n",
              " 'interdisci-',\n",
              " 'plinary',\n",
              " 'field',\n",
              " 'about',\n",
              " 'scientific',\n",
              " 'methods,',\n",
              " 'processes,',\n",
              " 'and',\n",
              " 'systems',\n",
              " 'to',\n",
              " 'extract',\n",
              " 'knowledge',\n",
              " 'or',\n",
              " 'insights',\n",
              " 'from',\n",
              " 'data',\n",
              " 'in',\n",
              " 'various',\n",
              " 'forms,',\n",
              " 'either',\n",
              " 'structured',\n",
              " 'or',\n",
              " 'unstructured.',\n",
              " 'data',\n",
              " 'science',\n",
              " ...]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_special_characters(tokens):\n",
        "    \"\"\"\n",
        "    Removes tokens that contain only special characters (non-alphanumeric).\n",
        "\n",
        "    Args:\n",
        "        tokens (list of str): List of word tokens.\n",
        "\n",
        "    Returns:\n",
        "        list of str: Filtered list containing only alphanumeric tokens.\n",
        "    \"\"\"\n",
        "    return [token for token in tokens if re.fullmatch(r'\\w+', token)]\n"
      ],
      "metadata": {
        "id": "4MxCHyJWt2iG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_special_characters(tokens):\n",
        "    \"\"\"\n",
        "    Removes special characters from inside tokens, but keeps the cleaned word.\n",
        "\n",
        "    Args:\n",
        "        tokens (list of str): List of word tokens.\n",
        "\n",
        "    Returns:\n",
        "        list of str: Cleaned tokens with special characters removed.\n",
        "    \"\"\"\n",
        "    return [re.sub(r'\\W+', '', token) for token in tokens if re.sub(r'\\W+', '', token)]\n"
      ],
      "metadata": {
        "id": "awjX4PRQuueb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_words_list_lowered_characters_cleaned = clean_special_characters(raw_words_list_lowered)"
      ],
      "metadata": {
        "id": "jyFGR2oduDdz"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(raw_words_list_lowered_characters_cleaned)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lqkG6OKztlL1",
        "outputId": "6ef0e42a-6808-4fea-9955-d213978b7765"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "['infoh515',\n",
              " 'big',\n",
              " 'data',\n",
              " 'distributed',\n",
              " 'management',\n",
              " 'lecture',\n",
              " '1',\n",
              " 'introduction',\n",
              " 'hdfs',\n",
              " 'mapreduce',\n",
              " 'dimitris',\n",
              " 'sacharidis',\n",
              " 'lecture',\n",
              " 'outline',\n",
              " 'what',\n",
              " 'is',\n",
              " 'big',\n",
              " 'data',\n",
              " 'how',\n",
              " 'is',\n",
              " 'big',\n",
              " 'data',\n",
              " 'used',\n",
              " 'big',\n",
              " 'datacenters',\n",
              " 'hdfs',\n",
              " 'mapreduce',\n",
              " '1',\n",
              " 'what',\n",
              " 'is',\n",
              " 'big',\n",
              " 'data',\n",
              " 'big',\n",
              " 'data',\n",
              " 'is',\n",
              " 'like',\n",
              " 'teenage',\n",
              " 'sex',\n",
              " 'everyone',\n",
              " 'talks',\n",
              " 'about',\n",
              " 'it',\n",
              " 'nobody',\n",
              " 'really',\n",
              " 'knows',\n",
              " 'how',\n",
              " 'to',\n",
              " 'do',\n",
              " 'it',\n",
              " 'everyone',\n",
              " 'thinks',\n",
              " 'everyone',\n",
              " 'else',\n",
              " 'is',\n",
              " 'doing',\n",
              " 'it',\n",
              " 'so',\n",
              " 'everyone',\n",
              " 'claims',\n",
              " 'they',\n",
              " 'are',\n",
              " 'doing',\n",
              " 'it',\n",
              " 'dan',\n",
              " 'ariely',\n",
              " '3',\n",
              " 'big',\n",
              " 'data',\n",
              " 'data',\n",
              " 'analytics',\n",
              " 'data',\n",
              " 'science',\n",
              " 'the',\n",
              " 'big',\n",
              " 'three',\n",
              " '4',\n",
              " 'big',\n",
              " 'data',\n",
              " 'the',\n",
              " 'big',\n",
              " 'three',\n",
              " '4',\n",
              " 'big',\n",
              " 'data',\n",
              " 'some',\n",
              " 'history',\n",
              " 'the',\n",
              " 'big',\n",
              " 'three',\n",
              " '4',\n",
              " 'source',\n",
              " 'the',\n",
              " 'economist',\n",
              " 'february',\n",
              " '25',\n",
              " '2010',\n",
              " 'the',\n",
              " 'data',\n",
              " 'deluge',\n",
              " 'eighteen',\n",
              " 'months',\n",
              " 'ago',\n",
              " 'li',\n",
              " 'fung',\n",
              " 'a',\n",
              " 'firm',\n",
              " 'that',\n",
              " 'manages',\n",
              " 'supply',\n",
              " 'chains',\n",
              " 'for',\n",
              " 'retailers',\n",
              " 'saw',\n",
              " '100',\n",
              " 'gigabytes',\n",
              " 'of',\n",
              " 'information',\n",
              " 'flow',\n",
              " 'through',\n",
              " 'its',\n",
              " 'network',\n",
              " 'each',\n",
              " 'day',\n",
              " 'now',\n",
              " 'the',\n",
              " 'amount',\n",
              " 'has',\n",
              " 'increased',\n",
              " 'tenfold',\n",
              " 'during',\n",
              " '2009',\n",
              " 'american',\n",
              " 'drone',\n",
              " 'aircraft',\n",
              " 'flying',\n",
              " 'over',\n",
              " 'iraq',\n",
              " 'and',\n",
              " 'afghanistan',\n",
              " 'sent',\n",
              " 'back',\n",
              " 'around',\n",
              " '24',\n",
              " 'years',\n",
              " 'worth',\n",
              " 'of',\n",
              " 'video',\n",
              " 'footage',\n",
              " 'new',\n",
              " 'models',\n",
              " 'being',\n",
              " 'deployed',\n",
              " 'this',\n",
              " 'year',\n",
              " 'will',\n",
              " 'produce',\n",
              " 'ten',\n",
              " 'times',\n",
              " 'as',\n",
              " 'many',\n",
              " 'data',\n",
              " 'streams',\n",
              " 'as',\n",
              " 'their',\n",
              " 'predecessors',\n",
              " 'and',\n",
              " 'those',\n",
              " 'in',\n",
              " '2011',\n",
              " 'will',\n",
              " 'produce',\n",
              " '30',\n",
              " 'times',\n",
              " 'as',\n",
              " 'many',\n",
              " 'source',\n",
              " 'the',\n",
              " 'economist',\n",
              " 'february',\n",
              " '25',\n",
              " '2010',\n",
              " 'the',\n",
              " 'data',\n",
              " 'deluge',\n",
              " 'everywhere',\n",
              " 'you',\n",
              " 'look',\n",
              " 'the',\n",
              " 'quantity',\n",
              " 'of',\n",
              " 'information',\n",
              " 'in',\n",
              " 'the',\n",
              " 'world',\n",
              " 'is',\n",
              " 'soaring',\n",
              " 'according',\n",
              " 'to',\n",
              " 'one',\n",
              " 'estimate',\n",
              " 'mankind',\n",
              " 'created',\n",
              " '150',\n",
              " 'exabytes',\n",
              " 'billion',\n",
              " 'gigabytes',\n",
              " 'of',\n",
              " 'data',\n",
              " 'in',\n",
              " '2005',\n",
              " 'this',\n",
              " 'year',\n",
              " 'it',\n",
              " 'will',\n",
              " 'create',\n",
              " '1200',\n",
              " 'exabytes',\n",
              " 'merely',\n",
              " 'keeping',\n",
              " 'up',\n",
              " 'with',\n",
              " 'this',\n",
              " 'flood',\n",
              " 'and',\n",
              " 'storing',\n",
              " 'the',\n",
              " 'bits',\n",
              " 'that',\n",
              " 'might',\n",
              " 'be',\n",
              " 'useful',\n",
              " 'is',\n",
              " 'difficult',\n",
              " 'enough',\n",
              " 'analysing',\n",
              " 'it',\n",
              " 'to',\n",
              " 'spot',\n",
              " 'patterns',\n",
              " 'and',\n",
              " 'extract',\n",
              " 'useful',\n",
              " 'information',\n",
              " 'is',\n",
              " 'harder',\n",
              " 'still',\n",
              " 'source',\n",
              " 'the',\n",
              " 'economist',\n",
              " 'february',\n",
              " '25',\n",
              " '2010',\n",
              " 'the',\n",
              " 'data',\n",
              " 'deluge',\n",
              " 'by',\n",
              " 'the',\n",
              " '2030s',\n",
              " 'computer',\n",
              " 'data',\n",
              " 'storage',\n",
              " 'may',\n",
              " 'surpass',\n",
              " '1',\n",
              " 'yottabyte',\n",
              " '1024',\n",
              " 'the',\n",
              " 'largest',\n",
              " 'number',\n",
              " 'with',\n",
              " 'an',\n",
              " 'official',\n",
              " 'metric',\n",
              " 'prefix',\n",
              " 'the',\n",
              " 'international',\n",
              " 'bureau',\n",
              " 'of',\n",
              " 'weights',\n",
              " 'and',\n",
              " 'measures',\n",
              " 'bipm',\n",
              " 'in',\n",
              " 'paris',\n",
              " 'recommends',\n",
              " 'new',\n",
              " 'namesronna',\n",
              " 'and',\n",
              " 'queccaas',\n",
              " 'prefixes',\n",
              " 'for',\n",
              " '1027',\n",
              " 'and',\n",
              " '1030',\n",
              " '1',\n",
              " 'gigabyte',\n",
              " '109',\n",
              " 'bytes',\n",
              " '1',\n",
              " 'terabyte',\n",
              " '1012',\n",
              " 'bytes',\n",
              " '1',\n",
              " 'petabyte',\n",
              " '1015',\n",
              " 'bytes',\n",
              " '1',\n",
              " 'exabyte',\n",
              " '1018',\n",
              " 'bytes',\n",
              " '1',\n",
              " 'zetabyte',\n",
              " '1021',\n",
              " 'bytes',\n",
              " '1',\n",
              " 'yottabyte',\n",
              " '1024',\n",
              " 'bytes',\n",
              " '1',\n",
              " 'ronnabyte',\n",
              " '1027',\n",
              " 'bytes',\n",
              " '1',\n",
              " 'queccabyte',\n",
              " '1030',\n",
              " 'bytes',\n",
              " 'httpswwwscienceorgcontentarticleyouknowkilomegaandgigametricsystemreadyronnaandquecca',\n",
              " 'is',\n",
              " 'this',\n",
              " 'really',\n",
              " 'problematic',\n",
              " 'assume',\n",
              " 'youre',\n",
              " 'google',\n",
              " 'at',\n",
              " 'the',\n",
              " 'beginning',\n",
              " 'of',\n",
              " 'the',\n",
              " '2000s',\n",
              " 'and',\n",
              " 'you',\n",
              " 'want',\n",
              " 'to',\n",
              " 'index',\n",
              " 'the',\n",
              " 'web',\n",
              " 'is',\n",
              " 'this',\n",
              " 'really',\n",
              " 'problematic',\n",
              " 'although',\n",
              " 'it',\n",
              " 'is',\n",
              " 'hard',\n",
              " 'to',\n",
              " 'accurately',\n",
              " 'determine',\n",
              " 'the',\n",
              " 'size',\n",
              " 'of',\n",
              " 'the',\n",
              " 'web',\n",
              " 'at',\n",
              " 'any',\n",
              " 'point',\n",
              " 'in',\n",
              " 'time',\n",
              " 'it',\n",
              " 'is',\n",
              " 'safe',\n",
              " 'to',\n",
              " 'say',\n",
              " 'that',\n",
              " 'it',\n",
              " 'consists',\n",
              " 'of',\n",
              " 'hundreds',\n",
              " 'of',\n",
              " 'billions',\n",
              " 'of',\n",
              " 'individual',\n",
              " 'documents',\n",
              " 'and',\n",
              " 'that',\n",
              " 'it',\n",
              " 'continues',\n",
              " 'to',\n",
              " 'grow',\n",
              " 'if',\n",
              " 'we',\n",
              " 'assume',\n",
              " 'the',\n",
              " 'web',\n",
              " 'to',\n",
              " 'contain',\n",
              " '100',\n",
              " 'billion',\n",
              " 'documents',\n",
              " 'with',\n",
              " 'an',\n",
              " 'average',\n",
              " 'document',\n",
              " 'size',\n",
              " 'of',\n",
              " '4',\n",
              " 'kb',\n",
              " 'after',\n",
              " 'compression',\n",
              " 'the',\n",
              " 'web',\n",
              " 'is',\n",
              " 'about',\n",
              " '400',\n",
              " 'tb',\n",
              " 'the',\n",
              " 'size',\n",
              " 'of',\n",
              " 'the',\n",
              " 'resulting',\n",
              " 'inverted',\n",
              " 'web',\n",
              " 'search',\n",
              " 'index',\n",
              " 'depends',\n",
              " 'on',\n",
              " 'the',\n",
              " 'specific',\n",
              " 'implementation',\n",
              " 'but',\n",
              " 'it',\n",
              " 'tends',\n",
              " 'to',\n",
              " 'be',\n",
              " 'on',\n",
              " 'the',\n",
              " 'same',\n",
              " 'order',\n",
              " 'of',\n",
              " 'magnitude',\n",
              " 'as',\n",
              " 'the',\n",
              " 'original',\n",
              " 'repository',\n",
              " 'source',\n",
              " 'the',\n",
              " 'datacenter',\n",
              " 'as',\n",
              " 'a',\n",
              " 'computer',\n",
              " 'luiz',\n",
              " 'andré',\n",
              " 'barroso',\n",
              " 'google',\n",
              " 'jimmy',\n",
              " 'clidaras',\n",
              " 'google',\n",
              " 'urs',\n",
              " 'hölzle',\n",
              " 'google',\n",
              " 'big',\n",
              " 'data',\n",
              " 'some',\n",
              " 'definitions',\n",
              " '14',\n",
              " 'big',\n",
              " 'data',\n",
              " 'is',\n",
              " 'a',\n",
              " 'term',\n",
              " 'encompassing',\n",
              " 'the',\n",
              " 'use',\n",
              " 'of',\n",
              " 'techniques',\n",
              " 'to',\n",
              " 'capture',\n",
              " 'pro',\n",
              " 'cess',\n",
              " 'analyse',\n",
              " 'and',\n",
              " 'visualize',\n",
              " 'potentially',\n",
              " 'large',\n",
              " 'datasets',\n",
              " 'in',\n",
              " 'a',\n",
              " 'reasonable',\n",
              " 'timeframe',\n",
              " 'not',\n",
              " 'accessible',\n",
              " 'to',\n",
              " 'standard',\n",
              " 'it',\n",
              " 'technologies',\n",
              " 'by',\n",
              " 'extension',\n",
              " 'the',\n",
              " 'platform',\n",
              " 'tools',\n",
              " 'and',\n",
              " 'software',\n",
              " 'used',\n",
              " 'for',\n",
              " 'this',\n",
              " 'purpose',\n",
              " 'are',\n",
              " 'collectively',\n",
              " 'called',\n",
              " 'big',\n",
              " 'data',\n",
              " 'technologies',\n",
              " 'nessi1',\n",
              " '1the',\n",
              " 'european',\n",
              " 'technology',\n",
              " 'platform',\n",
              " 'dedicated',\n",
              " 'to',\n",
              " 'software',\n",
              " 'services',\n",
              " 'and',\n",
              " 'data',\n",
              " '5',\n",
              " 'big',\n",
              " 'data',\n",
              " 'some',\n",
              " 'definitions',\n",
              " '24',\n",
              " 'big',\n",
              " 'data',\n",
              " 'refers',\n",
              " 'to',\n",
              " 'datasets',\n",
              " 'whose',\n",
              " 'size',\n",
              " 'is',\n",
              " 'beyond',\n",
              " 'the',\n",
              " 'ability',\n",
              " 'of',\n",
              " 'typical',\n",
              " 'database',\n",
              " 'software',\n",
              " 'tools',\n",
              " 'to',\n",
              " 'capture',\n",
              " 'store',\n",
              " 'manage',\n",
              " 'and',\n",
              " 'analyze',\n",
              " 'this',\n",
              " 'definition',\n",
              " 'is',\n",
              " 'intentionally',\n",
              " 'subjective',\n",
              " 'and',\n",
              " 'incorporates',\n",
              " 'a',\n",
              " 'moving',\n",
              " 'defi',\n",
              " 'nition',\n",
              " 'of',\n",
              " 'how',\n",
              " 'big',\n",
              " 'a',\n",
              " 'dataset',\n",
              " 'needs',\n",
              " 'to',\n",
              " 'be',\n",
              " 'in',\n",
              " 'order',\n",
              " 'to',\n",
              " 'be',\n",
              " 'considered',\n",
              " 'big',\n",
              " 'data',\n",
              " 'we',\n",
              " 'assume',\n",
              " 'that',\n",
              " 'as',\n",
              " 'technology',\n",
              " 'advances',\n",
              " 'over',\n",
              " 'time',\n",
              " 'the',\n",
              " 'size',\n",
              " 'of',\n",
              " 'datasets',\n",
              " 'that',\n",
              " 'qualify',\n",
              " 'as',\n",
              " 'big',\n",
              " 'data',\n",
              " 'will',\n",
              " 'also',\n",
              " 'increase',\n",
              " 'mckinsey',\n",
              " 'global',\n",
              " 'institute',\n",
              " '2',\n",
              " '2big',\n",
              " 'data',\n",
              " 'the',\n",
              " 'next',\n",
              " 'frontier',\n",
              " 'for',\n",
              " 'innovation',\n",
              " 'competition',\n",
              " 'and',\n",
              " 'productivity',\n",
              " 'june',\n",
              " '2011',\n",
              " '6',\n",
              " 'big',\n",
              " 'data',\n",
              " 'some',\n",
              " 'definitions',\n",
              " '34',\n",
              " 'big',\n",
              " 'data',\n",
              " 'are',\n",
              " 'data',\n",
              " 'sets',\n",
              " 'that',\n",
              " 'are',\n",
              " 'so',\n",
              " 'voluminous',\n",
              " 'and',\n",
              " 'complex',\n",
              " 'that',\n",
              " 'tra',\n",
              " 'ditional',\n",
              " 'data',\n",
              " 'processing',\n",
              " 'application',\n",
              " 'software',\n",
              " 'are',\n",
              " 'inadequate',\n",
              " 'to',\n",
              " 'deal',\n",
              " 'with',\n",
              " 'them',\n",
              " 'big',\n",
              " 'data',\n",
              " 'challenges',\n",
              " 'include',\n",
              " 'capturing',\n",
              " 'data',\n",
              " 'data',\n",
              " 'storage',\n",
              " 'data',\n",
              " 'analysis',\n",
              " 'search',\n",
              " 'sharing',\n",
              " 'transfer',\n",
              " 'visualization',\n",
              " 'querying',\n",
              " 'updat',\n",
              " 'ing',\n",
              " 'and',\n",
              " 'information',\n",
              " 'privacy',\n",
              " 'lately',\n",
              " 'the',\n",
              " 'term',\n",
              " 'big',\n",
              " 'data',\n",
              " 'tends',\n",
              " 'to',\n",
              " 'refer',\n",
              " 'to',\n",
              " 'the',\n",
              " 'use',\n",
              " 'of',\n",
              " 'predictive',\n",
              " 'analyt',\n",
              " 'ics',\n",
              " 'user',\n",
              " 'behavior',\n",
              " 'analytics',\n",
              " 'or',\n",
              " 'certain',\n",
              " 'other',\n",
              " 'advanced',\n",
              " 'data',\n",
              " 'analytics',\n",
              " 'methods',\n",
              " 'that',\n",
              " 'extract',\n",
              " 'value',\n",
              " 'from',\n",
              " 'data',\n",
              " 'and',\n",
              " 'seldom',\n",
              " 'to',\n",
              " 'a',\n",
              " 'particular',\n",
              " 'size',\n",
              " 'of',\n",
              " 'data',\n",
              " 'set',\n",
              " 'wikipedia',\n",
              " '7',\n",
              " 'big',\n",
              " 'data',\n",
              " 'some',\n",
              " 'definitions',\n",
              " '44',\n",
              " 'big',\n",
              " 'data',\n",
              " 'refers',\n",
              " 'to',\n",
              " 'extremely',\n",
              " 'large',\n",
              " 'datasets',\n",
              " 'that',\n",
              " 'cannot',\n",
              " 'be',\n",
              " 'easily',\n",
              " 'man',\n",
              " 'aged',\n",
              " 'processed',\n",
              " 'or',\n",
              " 'analyzed',\n",
              " 'using',\n",
              " 'traditional',\n",
              " 'data',\n",
              " 'processing',\n",
              " 'meth',\n",
              " 'ods',\n",
              " 'it',\n",
              " 'is',\n",
              " 'characterized',\n",
              " 'by',\n",
              " 'the',\n",
              " 'three',\n",
              " 'vs',\n",
              " 'volume',\n",
              " 'variety',\n",
              " 'and',\n",
              " 'velocity',\n",
              " 'which',\n",
              " 'describe',\n",
              " 'the',\n",
              " 'sheer',\n",
              " 'amount',\n",
              " 'of',\n",
              " 'data',\n",
              " 'the',\n",
              " 'diversity',\n",
              " 'of',\n",
              " 'data',\n",
              " 'types',\n",
              " 'and',\n",
              " 'the',\n",
              " 'speed',\n",
              " 'at',\n",
              " 'which',\n",
              " 'it',\n",
              " 'is',\n",
              " 'generated',\n",
              " 'big',\n",
              " 'data',\n",
              " 'technologies',\n",
              " 'enable',\n",
              " 'businesses',\n",
              " 'and',\n",
              " 'researchers',\n",
              " 'to',\n",
              " 'gain',\n",
              " 'insights',\n",
              " 'and',\n",
              " 'make',\n",
              " 'datadriven',\n",
              " 'de',\n",
              " 'cisions',\n",
              " 'from',\n",
              " 'these',\n",
              " 'vast',\n",
              " 'dynamic',\n",
              " 'datasets',\n",
              " 'chatgpt',\n",
              " '022025',\n",
              " '8',\n",
              " 'big',\n",
              " 'data',\n",
              " 'data',\n",
              " 'analytics',\n",
              " 'data',\n",
              " 'science',\n",
              " 'the',\n",
              " 'big',\n",
              " 'three',\n",
              " '9',\n",
              " 'data',\n",
              " 'analytics',\n",
              " 'the',\n",
              " 'big',\n",
              " 'three',\n",
              " '9',\n",
              " 'data',\n",
              " 'analytics',\n",
              " 'a',\n",
              " 'definition',\n",
              " 'data',\n",
              " 'analysis',\n",
              " 'also',\n",
              " 'known',\n",
              " 'as',\n",
              " 'analysis',\n",
              " 'of',\n",
              " 'data',\n",
              " 'or',\n",
              " 'data',\n",
              " 'analytics',\n",
              " 'is',\n",
              " 'a',\n",
              " 'process',\n",
              " 'of',\n",
              " 'inspecting',\n",
              " 'cleansing',\n",
              " 'transforming',\n",
              " 'and',\n",
              " 'modeling',\n",
              " 'data',\n",
              " 'with',\n",
              " 'the',\n",
              " 'goal',\n",
              " 'of',\n",
              " 'discovering',\n",
              " 'useful',\n",
              " 'information',\n",
              " 'suggesting',\n",
              " 'conclu',\n",
              " 'sions',\n",
              " 'and',\n",
              " 'supporting',\n",
              " 'decisionmaking',\n",
              " 'wikipedia',\n",
              " '10',\n",
              " '11',\n",
              " 'big',\n",
              " 'data',\n",
              " 'data',\n",
              " 'analytics',\n",
              " 'data',\n",
              " 'science',\n",
              " 'the',\n",
              " 'big',\n",
              " 'three',\n",
              " '12',\n",
              " 'data',\n",
              " 'science',\n",
              " 'the',\n",
              " 'big',\n",
              " 'three',\n",
              " '12',\n",
              " 'data',\n",
              " 'science',\n",
              " 'some',\n",
              " 'definitions',\n",
              " '13',\n",
              " 'data',\n",
              " 'science',\n",
              " 'is',\n",
              " 'the',\n",
              " 'study',\n",
              " 'of',\n",
              " 'the',\n",
              " 'generalizable',\n",
              " 'extraction',\n",
              " 'of',\n",
              " 'knowledge',\n",
              " 'from',\n",
              " 'data',\n",
              " 'vasant',\n",
              " 'dhar3',\n",
              " 'from',\n",
              " 'the',\n",
              " 'article',\n",
              " 'the',\n",
              " 'term',\n",
              " 'science',\n",
              " 'implies',\n",
              " 'knowledge',\n",
              " 'gained',\n",
              " 'through',\n",
              " 'systematic',\n",
              " 'study',\n",
              " 'a',\n",
              " 'data',\n",
              " 'scientist',\n",
              " 'requires',\n",
              " 'an',\n",
              " 'integrated',\n",
              " 'skill',\n",
              " 'set',\n",
              " 'spanning',\n",
              " 'mathematics',\n",
              " 'machine',\n",
              " 'learning',\n",
              " 'artificial',\n",
              " 'intelligence',\n",
              " 'statistics',\n",
              " 'databases',\n",
              " 'and',\n",
              " 'optimization',\n",
              " 'along',\n",
              " 'with',\n",
              " 'a',\n",
              " 'deep',\n",
              " 'understanding',\n",
              " 'of',\n",
              " 'the',\n",
              " 'craft',\n",
              " 'of',\n",
              " 'problem',\n",
              " 'formulation',\n",
              " 'to',\n",
              " 'engineer',\n",
              " 'effective',\n",
              " 'solutions',\n",
              " '3data',\n",
              " 'science',\n",
              " 'and',\n",
              " 'predictions',\n",
              " 'communications',\n",
              " 'of',\n",
              " 'the',\n",
              " 'acm',\n",
              " '5612',\n",
              " '2014',\n",
              " '13',\n",
              " 'data',\n",
              " 'science',\n",
              " 'some',\n",
              " 'definitions',\n",
              " '23',\n",
              " 'data',\n",
              " 'science',\n",
              " 'also',\n",
              " 'known',\n",
              " 'as',\n",
              " 'datadriven',\n",
              " 'science',\n",
              " 'is',\n",
              " 'an',\n",
              " 'interdisci',\n",
              " 'plinary',\n",
              " 'field',\n",
              " 'about',\n",
              " 'scientific',\n",
              " 'methods',\n",
              " 'processes',\n",
              " 'and',\n",
              " 'systems',\n",
              " 'to',\n",
              " 'extract',\n",
              " 'knowledge',\n",
              " 'or',\n",
              " 'insights',\n",
              " 'from',\n",
              " 'data',\n",
              " 'in',\n",
              " 'various',\n",
              " 'forms',\n",
              " 'either',\n",
              " 'structured',\n",
              " 'or',\n",
              " 'unstructured',\n",
              " 'data',\n",
              " 'science',\n",
              " 'is',\n",
              " 'a',\n",
              " 'concept',\n",
              " 'to',\n",
              " 'unify',\n",
              " 'statistics',\n",
              " 'data',\n",
              " 'analysis',\n",
              " 'and',\n",
              " 'their',\n",
              " 'related',\n",
              " 'methods',\n",
              " 'in',\n",
              " 'order',\n",
              " 'to',\n",
              " 'understand',\n",
              " 'and',\n",
              " 'analyze',\n",
              " 'actual',\n",
              " 'phenom',\n",
              " 'ena',\n",
              " 'with',\n",
              " 'data',\n",
              " 'it',\n",
              " 'employs',\n",
              " 'techniques',\n",
              " 'and',\n",
              " ...]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def remove_stop_words(tokens):\n",
        "    \"\"\"\n",
        "    Removes stop words from a list of tokens using spaCy's built-in stop words.\n",
        "\n",
        "    Args:\n",
        "        tokens (list of str): List of tokens (strings).\n",
        "\n",
        "    Returns:\n",
        "        list of str: Tokens with stop words removed.\n",
        "    \"\"\"\n",
        "    return [token for token in tokens if token not in nlp.Defaults.stop_words]\n"
      ],
      "metadata": {
        "id": "DbkJQ4tMwK0z"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spacy_stopwords = nlp.Defaults.stop_words\n",
        "\n",
        "print(sorted(spacy_stopwords))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3B5whcdSzSVH",
        "outputId": "a624d16e-8d6f-4ed2-892c-0e14f632c185"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", 'a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amount', 'an', 'and', 'another', 'any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'are', 'around', 'as', 'at', 'back', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'both', 'bottom', 'but', 'by', 'ca', 'call', 'can', 'cannot', 'could', 'did', 'do', 'does', 'doing', 'done', 'down', 'due', 'during', 'each', 'eight', 'either', 'eleven', 'else', 'elsewhere', 'empty', 'enough', 'even', 'ever', 'every', 'everyone', 'everything', 'everywhere', 'except', 'few', 'fifteen', 'fifty', 'first', 'five', 'for', 'former', 'formerly', 'forty', 'four', 'from', 'front', 'full', 'further', 'get', 'give', 'go', 'had', 'has', 'have', 'he', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'however', 'hundred', 'i', 'if', 'in', 'indeed', 'into', 'is', 'it', 'its', 'itself', 'just', 'keep', 'last', 'latter', 'latterly', 'least', 'less', 'made', 'make', 'many', 'may', 'me', 'meanwhile', 'might', 'mine', 'more', 'moreover', 'most', 'mostly', 'move', 'much', 'must', 'my', 'myself', \"n't\", 'name', 'namely', 'neither', 'never', 'nevertheless', 'next', 'nine', 'no', 'nobody', 'none', 'noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'n‘t', 'n’t', 'of', 'off', 'often', 'on', 'once', 'one', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'part', 'per', 'perhaps', 'please', 'put', 'quite', 'rather', 're', 'really', 'regarding', 'same', 'say', 'see', 'seem', 'seemed', 'seeming', 'seems', 'serious', 'several', 'she', 'should', 'show', 'side', 'since', 'six', 'sixty', 'so', 'some', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhere', 'still', 'such', 'take', 'ten', 'than', 'that', 'the', 'their', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein', 'thereupon', 'these', 'they', 'third', 'this', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'top', 'toward', 'towards', 'twelve', 'twenty', 'two', 'under', 'unless', 'until', 'up', 'upon', 'us', 'used', 'using', 'various', 'very', 'via', 'was', 'we', 'well', 'were', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'with', 'within', 'without', 'would', 'yet', 'you', 'your', 'yours', 'yourself', 'yourselves', '‘d', '‘ll', '‘m', '‘re', '‘s', '‘ve', '’d', '’ll', '’m', '’re', '’s', '’ve']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_filtered_from_stopwords = remove_stop_words(raw_words_list_lowered_characters_cleaned)"
      ],
      "metadata": {
        "id": "7gwBwtVqwVkb"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(list_filtered_from_stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iUY6TMgpw82o",
        "outputId": "bdfd5ccc-83ac-4e87-bda0-293a224bbb73"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "['infoh515',\n",
              " 'big',\n",
              " 'data',\n",
              " 'distributed',\n",
              " 'management',\n",
              " 'lecture',\n",
              " '1',\n",
              " 'introduction',\n",
              " 'hdfs',\n",
              " 'mapreduce',\n",
              " 'dimitris',\n",
              " 'sacharidis',\n",
              " 'lecture',\n",
              " 'outline',\n",
              " 'big',\n",
              " 'data',\n",
              " 'big',\n",
              " 'data',\n",
              " 'big',\n",
              " 'datacenters',\n",
              " 'hdfs',\n",
              " 'mapreduce',\n",
              " '1',\n",
              " 'big',\n",
              " 'data',\n",
              " 'big',\n",
              " 'data',\n",
              " 'like',\n",
              " 'teenage',\n",
              " 'sex',\n",
              " 'talks',\n",
              " 'knows',\n",
              " 'thinks',\n",
              " 'claims',\n",
              " 'dan',\n",
              " 'ariely',\n",
              " '3',\n",
              " 'big',\n",
              " 'data',\n",
              " 'data',\n",
              " 'analytics',\n",
              " 'data',\n",
              " 'science',\n",
              " 'big',\n",
              " '4',\n",
              " 'big',\n",
              " 'data',\n",
              " 'big',\n",
              " '4',\n",
              " 'big',\n",
              " 'data',\n",
              " 'history',\n",
              " 'big',\n",
              " '4',\n",
              " 'source',\n",
              " 'economist',\n",
              " 'february',\n",
              " '25',\n",
              " '2010',\n",
              " 'data',\n",
              " 'deluge',\n",
              " 'eighteen',\n",
              " 'months',\n",
              " 'ago',\n",
              " 'li',\n",
              " 'fung',\n",
              " 'firm',\n",
              " 'manages',\n",
              " 'supply',\n",
              " 'chains',\n",
              " 'retailers',\n",
              " 'saw',\n",
              " '100',\n",
              " 'gigabytes',\n",
              " 'information',\n",
              " 'flow',\n",
              " 'network',\n",
              " 'day',\n",
              " 'increased',\n",
              " 'tenfold',\n",
              " '2009',\n",
              " 'american',\n",
              " 'drone',\n",
              " 'aircraft',\n",
              " 'flying',\n",
              " 'iraq',\n",
              " 'afghanistan',\n",
              " 'sent',\n",
              " '24',\n",
              " 'years',\n",
              " 'worth',\n",
              " 'video',\n",
              " 'footage',\n",
              " 'new',\n",
              " 'models',\n",
              " 'deployed',\n",
              " 'year',\n",
              " 'produce',\n",
              " 'times',\n",
              " 'data',\n",
              " 'streams',\n",
              " 'predecessors',\n",
              " '2011',\n",
              " 'produce',\n",
              " '30',\n",
              " 'times',\n",
              " 'source',\n",
              " 'economist',\n",
              " 'february',\n",
              " '25',\n",
              " '2010',\n",
              " 'data',\n",
              " 'deluge',\n",
              " 'look',\n",
              " 'quantity',\n",
              " 'information',\n",
              " 'world',\n",
              " 'soaring',\n",
              " 'according',\n",
              " 'estimate',\n",
              " 'mankind',\n",
              " 'created',\n",
              " '150',\n",
              " 'exabytes',\n",
              " 'billion',\n",
              " 'gigabytes',\n",
              " 'data',\n",
              " '2005',\n",
              " 'year',\n",
              " 'create',\n",
              " '1200',\n",
              " 'exabytes',\n",
              " 'merely',\n",
              " 'keeping',\n",
              " 'flood',\n",
              " 'storing',\n",
              " 'bits',\n",
              " 'useful',\n",
              " 'difficult',\n",
              " 'analysing',\n",
              " 'spot',\n",
              " 'patterns',\n",
              " 'extract',\n",
              " 'useful',\n",
              " 'information',\n",
              " 'harder',\n",
              " 'source',\n",
              " 'economist',\n",
              " 'february',\n",
              " '25',\n",
              " '2010',\n",
              " 'data',\n",
              " 'deluge',\n",
              " '2030s',\n",
              " 'computer',\n",
              " 'data',\n",
              " 'storage',\n",
              " 'surpass',\n",
              " '1',\n",
              " 'yottabyte',\n",
              " '1024',\n",
              " 'largest',\n",
              " 'number',\n",
              " 'official',\n",
              " 'metric',\n",
              " 'prefix',\n",
              " 'international',\n",
              " 'bureau',\n",
              " 'weights',\n",
              " 'measures',\n",
              " 'bipm',\n",
              " 'paris',\n",
              " 'recommends',\n",
              " 'new',\n",
              " 'namesronna',\n",
              " 'queccaas',\n",
              " 'prefixes',\n",
              " '1027',\n",
              " '1030',\n",
              " '1',\n",
              " 'gigabyte',\n",
              " '109',\n",
              " 'bytes',\n",
              " '1',\n",
              " 'terabyte',\n",
              " '1012',\n",
              " 'bytes',\n",
              " '1',\n",
              " 'petabyte',\n",
              " '1015',\n",
              " 'bytes',\n",
              " '1',\n",
              " 'exabyte',\n",
              " '1018',\n",
              " 'bytes',\n",
              " '1',\n",
              " 'zetabyte',\n",
              " '1021',\n",
              " 'bytes',\n",
              " '1',\n",
              " 'yottabyte',\n",
              " '1024',\n",
              " 'bytes',\n",
              " '1',\n",
              " 'ronnabyte',\n",
              " '1027',\n",
              " 'bytes',\n",
              " '1',\n",
              " 'queccabyte',\n",
              " '1030',\n",
              " 'bytes',\n",
              " 'httpswwwscienceorgcontentarticleyouknowkilomegaandgigametricsystemreadyronnaandquecca',\n",
              " 'problematic',\n",
              " 'assume',\n",
              " 'youre',\n",
              " 'google',\n",
              " 'beginning',\n",
              " '2000s',\n",
              " 'want',\n",
              " 'index',\n",
              " 'web',\n",
              " 'problematic',\n",
              " 'hard',\n",
              " 'accurately',\n",
              " 'determine',\n",
              " 'size',\n",
              " 'web',\n",
              " 'point',\n",
              " 'time',\n",
              " 'safe',\n",
              " 'consists',\n",
              " 'hundreds',\n",
              " 'billions',\n",
              " 'individual',\n",
              " 'documents',\n",
              " 'continues',\n",
              " 'grow',\n",
              " 'assume',\n",
              " 'web',\n",
              " 'contain',\n",
              " '100',\n",
              " 'billion',\n",
              " 'documents',\n",
              " 'average',\n",
              " 'document',\n",
              " 'size',\n",
              " '4',\n",
              " 'kb',\n",
              " 'compression',\n",
              " 'web',\n",
              " '400',\n",
              " 'tb',\n",
              " 'size',\n",
              " 'resulting',\n",
              " 'inverted',\n",
              " 'web',\n",
              " 'search',\n",
              " 'index',\n",
              " 'depends',\n",
              " 'specific',\n",
              " 'implementation',\n",
              " 'tends',\n",
              " 'order',\n",
              " 'magnitude',\n",
              " 'original',\n",
              " 'repository',\n",
              " 'source',\n",
              " 'datacenter',\n",
              " 'computer',\n",
              " 'luiz',\n",
              " 'andré',\n",
              " 'barroso',\n",
              " 'google',\n",
              " 'jimmy',\n",
              " 'clidaras',\n",
              " 'google',\n",
              " 'urs',\n",
              " 'hölzle',\n",
              " 'google',\n",
              " 'big',\n",
              " 'data',\n",
              " 'definitions',\n",
              " '14',\n",
              " 'big',\n",
              " 'data',\n",
              " 'term',\n",
              " 'encompassing',\n",
              " 'use',\n",
              " 'techniques',\n",
              " 'capture',\n",
              " 'pro',\n",
              " 'cess',\n",
              " 'analyse',\n",
              " 'visualize',\n",
              " 'potentially',\n",
              " 'large',\n",
              " 'datasets',\n",
              " 'reasonable',\n",
              " 'timeframe',\n",
              " 'accessible',\n",
              " 'standard',\n",
              " 'technologies',\n",
              " 'extension',\n",
              " 'platform',\n",
              " 'tools',\n",
              " 'software',\n",
              " 'purpose',\n",
              " 'collectively',\n",
              " 'called',\n",
              " 'big',\n",
              " 'data',\n",
              " 'technologies',\n",
              " 'nessi1',\n",
              " '1the',\n",
              " 'european',\n",
              " 'technology',\n",
              " 'platform',\n",
              " 'dedicated',\n",
              " 'software',\n",
              " 'services',\n",
              " 'data',\n",
              " '5',\n",
              " 'big',\n",
              " 'data',\n",
              " 'definitions',\n",
              " '24',\n",
              " 'big',\n",
              " 'data',\n",
              " 'refers',\n",
              " 'datasets',\n",
              " 'size',\n",
              " 'ability',\n",
              " 'typical',\n",
              " 'database',\n",
              " 'software',\n",
              " 'tools',\n",
              " 'capture',\n",
              " 'store',\n",
              " 'manage',\n",
              " 'analyze',\n",
              " 'definition',\n",
              " 'intentionally',\n",
              " 'subjective',\n",
              " 'incorporates',\n",
              " 'moving',\n",
              " 'defi',\n",
              " 'nition',\n",
              " 'big',\n",
              " 'dataset',\n",
              " 'needs',\n",
              " 'order',\n",
              " 'considered',\n",
              " 'big',\n",
              " 'data',\n",
              " 'assume',\n",
              " 'technology',\n",
              " 'advances',\n",
              " 'time',\n",
              " 'size',\n",
              " 'datasets',\n",
              " 'qualify',\n",
              " 'big',\n",
              " 'data',\n",
              " 'increase',\n",
              " 'mckinsey',\n",
              " 'global',\n",
              " 'institute',\n",
              " '2',\n",
              " '2big',\n",
              " 'data',\n",
              " 'frontier',\n",
              " 'innovation',\n",
              " 'competition',\n",
              " 'productivity',\n",
              " 'june',\n",
              " '2011',\n",
              " '6',\n",
              " 'big',\n",
              " 'data',\n",
              " 'definitions',\n",
              " '34',\n",
              " 'big',\n",
              " 'data',\n",
              " 'data',\n",
              " 'sets',\n",
              " 'voluminous',\n",
              " 'complex',\n",
              " 'tra',\n",
              " 'ditional',\n",
              " 'data',\n",
              " 'processing',\n",
              " 'application',\n",
              " 'software',\n",
              " 'inadequate',\n",
              " 'deal',\n",
              " 'big',\n",
              " 'data',\n",
              " 'challenges',\n",
              " 'include',\n",
              " 'capturing',\n",
              " 'data',\n",
              " 'data',\n",
              " 'storage',\n",
              " 'data',\n",
              " 'analysis',\n",
              " 'search',\n",
              " 'sharing',\n",
              " 'transfer',\n",
              " 'visualization',\n",
              " 'querying',\n",
              " 'updat',\n",
              " 'ing',\n",
              " 'information',\n",
              " 'privacy',\n",
              " 'lately',\n",
              " 'term',\n",
              " 'big',\n",
              " 'data',\n",
              " 'tends',\n",
              " 'refer',\n",
              " 'use',\n",
              " 'predictive',\n",
              " 'analyt',\n",
              " 'ics',\n",
              " 'user',\n",
              " 'behavior',\n",
              " 'analytics',\n",
              " 'certain',\n",
              " 'advanced',\n",
              " 'data',\n",
              " 'analytics',\n",
              " 'methods',\n",
              " 'extract',\n",
              " 'value',\n",
              " 'data',\n",
              " 'seldom',\n",
              " 'particular',\n",
              " 'size',\n",
              " 'data',\n",
              " 'set',\n",
              " 'wikipedia',\n",
              " '7',\n",
              " 'big',\n",
              " 'data',\n",
              " 'definitions',\n",
              " '44',\n",
              " 'big',\n",
              " 'data',\n",
              " 'refers',\n",
              " 'extremely',\n",
              " 'large',\n",
              " 'datasets',\n",
              " 'easily',\n",
              " 'man',\n",
              " 'aged',\n",
              " 'processed',\n",
              " 'analyzed',\n",
              " 'traditional',\n",
              " 'data',\n",
              " 'processing',\n",
              " 'meth',\n",
              " 'ods',\n",
              " 'characterized',\n",
              " 'vs',\n",
              " 'volume',\n",
              " 'variety',\n",
              " 'velocity',\n",
              " 'describe',\n",
              " 'sheer',\n",
              " 'data',\n",
              " 'diversity',\n",
              " 'data',\n",
              " 'types',\n",
              " 'speed',\n",
              " 'generated',\n",
              " 'big',\n",
              " 'data',\n",
              " 'technologies',\n",
              " 'enable',\n",
              " 'businesses',\n",
              " 'researchers',\n",
              " 'gain',\n",
              " 'insights',\n",
              " 'datadriven',\n",
              " 'de',\n",
              " 'cisions',\n",
              " 'vast',\n",
              " 'dynamic',\n",
              " 'datasets',\n",
              " 'chatgpt',\n",
              " '022025',\n",
              " '8',\n",
              " 'big',\n",
              " 'data',\n",
              " 'data',\n",
              " 'analytics',\n",
              " 'data',\n",
              " 'science',\n",
              " 'big',\n",
              " '9',\n",
              " 'data',\n",
              " 'analytics',\n",
              " 'big',\n",
              " '9',\n",
              " 'data',\n",
              " 'analytics',\n",
              " 'definition',\n",
              " 'data',\n",
              " 'analysis',\n",
              " 'known',\n",
              " 'analysis',\n",
              " 'data',\n",
              " 'data',\n",
              " 'analytics',\n",
              " 'process',\n",
              " 'inspecting',\n",
              " 'cleansing',\n",
              " 'transforming',\n",
              " 'modeling',\n",
              " 'data',\n",
              " 'goal',\n",
              " 'discovering',\n",
              " 'useful',\n",
              " 'information',\n",
              " 'suggesting',\n",
              " 'conclu',\n",
              " 'sions',\n",
              " 'supporting',\n",
              " 'decisionmaking',\n",
              " 'wikipedia',\n",
              " '10',\n",
              " '11',\n",
              " 'big',\n",
              " 'data',\n",
              " 'data',\n",
              " 'analytics',\n",
              " 'data',\n",
              " 'science',\n",
              " 'big',\n",
              " '12',\n",
              " 'data',\n",
              " 'science',\n",
              " 'big',\n",
              " '12',\n",
              " 'data',\n",
              " 'science',\n",
              " 'definitions',\n",
              " '13',\n",
              " 'data',\n",
              " 'science',\n",
              " 'study',\n",
              " 'generalizable',\n",
              " 'extraction',\n",
              " 'knowledge',\n",
              " 'data',\n",
              " 'vasant',\n",
              " 'dhar3',\n",
              " 'article',\n",
              " 'term',\n",
              " 'science',\n",
              " 'implies',\n",
              " 'knowledge',\n",
              " 'gained',\n",
              " 'systematic',\n",
              " 'study',\n",
              " 'data',\n",
              " 'scientist',\n",
              " 'requires',\n",
              " 'integrated',\n",
              " 'skill',\n",
              " 'set',\n",
              " 'spanning',\n",
              " 'mathematics',\n",
              " 'machine',\n",
              " 'learning',\n",
              " 'artificial',\n",
              " 'intelligence',\n",
              " 'statistics',\n",
              " 'databases',\n",
              " 'optimization',\n",
              " 'deep',\n",
              " 'understanding',\n",
              " 'craft',\n",
              " 'problem',\n",
              " 'formulation',\n",
              " 'engineer',\n",
              " 'effective',\n",
              " 'solutions',\n",
              " '3data',\n",
              " 'science',\n",
              " 'predictions',\n",
              " 'communications',\n",
              " 'acm',\n",
              " '5612',\n",
              " '2014',\n",
              " '13',\n",
              " 'data',\n",
              " 'science',\n",
              " 'definitions',\n",
              " '23',\n",
              " 'data',\n",
              " 'science',\n",
              " 'known',\n",
              " 'datadriven',\n",
              " 'science',\n",
              " 'interdisci',\n",
              " 'plinary',\n",
              " 'field',\n",
              " 'scientific',\n",
              " 'methods',\n",
              " 'processes',\n",
              " 'systems',\n",
              " 'extract',\n",
              " 'knowledge',\n",
              " 'insights',\n",
              " 'data',\n",
              " 'forms',\n",
              " 'structured',\n",
              " 'unstructured',\n",
              " 'data',\n",
              " 'science',\n",
              " 'concept',\n",
              " 'unify',\n",
              " 'statistics',\n",
              " 'data',\n",
              " 'analysis',\n",
              " 'related',\n",
              " 'methods',\n",
              " 'order',\n",
              " 'understand',\n",
              " 'analyze',\n",
              " 'actual',\n",
              " 'phenom',\n",
              " 'ena',\n",
              " 'data',\n",
              " 'employs',\n",
              " 'techniques',\n",
              " 'theories',\n",
              " 'drawn',\n",
              " 'fields',\n",
              " 'broad',\n",
              " 'areas',\n",
              " 'mathematics',\n",
              " 'statistics',\n",
              " 'information',\n",
              " 'sci',\n",
              " 'ence',\n",
              " 'computer',\n",
              " 'science',\n",
              " 'wikipedia',\n",
              " '14',\n",
              " 'data',\n",
              " 'science',\n",
              " 'definitions',\n",
              " '33',\n",
              " 'data',\n",
              " 'science',\n",
              " 'field',\n",
              " 'combines',\n",
              " 'statistical',\n",
              " 'analysis',\n",
              " 'machine',\n",
              " 'learning',\n",
              " 'data',\n",
              " 'processing',\n",
              " 'techniques',\n",
              " 'extract',\n",
              " 'valuable',\n",
              " 'insights',\n",
              " 'large',\n",
              " 'complex',\n",
              " 'datasets',\n",
              " 'involves',\n",
              " 'collecting',\n",
              " 'cleaning',\n",
              " 'analysing',\n",
              " 'interpreting',\n",
              " 'data',\n",
              " 'inform',\n",
              " 'decisionmaking',\n",
              " 'solve',\n",
              " 'problems',\n",
              " 'industries',\n",
              " 'data',\n",
              " 'scientists',\n",
              " 'use',\n",
              " 'programming',\n",
              " 'mathematics',\n",
              " 'domain',\n",
              " 'expertise',\n",
              " 'uncover',\n",
              " 'patterns',\n",
              " 'trends',\n",
              " 'data',\n",
              " 'chatgpt',\n",
              " '022025',\n",
              " '15',\n",
              " 'conclusion',\n",
              " '16',\n",
              " 'recall',\n",
              " 'course',\n",
              " 'objective',\n",
              " 'introduce',\n",
              " 'fundamental',\n",
              " 'notions',\n",
              " 'principles',\n",
              " 'research',\n",
              " 'results',\n",
              " 'concerning',\n",
              " 'modern',\n",
              " 'scalable',\n",
              " 'faulttolerant',\n",
              " 'ways',\n",
              " 'managing',\n",
              " 'analyzing',\n",
              " 'massive',\n",
              " 'amounts',\n",
              " 'data',\n",
              " 'parallel',\n",
              " 'distributed',\n",
              " 'systems',\n",
              " 'course',\n",
              " 'concerned',\n",
              " 'big',\n",
              " 'data',\n",
              " 'big',\n",
              " 'data',\n",
              " 'analytics',\n",
              " 'relevant',\n",
              " 'data',\n",
              " 'science',\n",
              " 'challenges',\n",
              " 'big',\n",
              " 'data',\n",
              " 'source',\n",
              " 'httpwwwibmbigdatahubcomsitesdefaultfilesinfographic_file',\n",
              " '17',\n",
              " 'big',\n",
              " 'data',\n",
              " 'big',\n",
              " 'data',\n",
              " 'slides',\n",
              " 'taken',\n",
              " 'httpswwwslidesharenetdellbigdatausecases3601989',\n",
              " '19',\n",
              " '1',\n",
              " 'optimize',\n",
              " 'funnel',\n",
              " 'conversion',\n",
              " '2',\n",
              " 'behavioral',\n",
              " 'analytics',\n",
              " '3',\n",
              " 'customer',\n",
              " 'segmentation',\n",
              " '4',\n",
              " 'predictive',\n",
              " 'support',\n",
              " '5',\n",
              " 'market',\n",
              " 'basket',\n",
              " 'analysis',\n",
              " 'pricing',\n",
              " 'optimization',\n",
              " '6',\n",
              " 'predict',\n",
              " 'security',\n",
              " 'threats',\n",
              " '7',\n",
              " 'fraud',\n",
              " 'detection',\n",
              " '8',\n",
              " 'industry',\n",
              " 'specific',\n",
              " 'big',\n",
              " 'data',\n",
              " 'use',\n",
              " 'cases',\n",
              " 'big',\n",
              " 'data',\n",
              " 'analytics',\n",
              " 'allows',\n",
              " 'companies',\n",
              " 'track',\n",
              " 'leads',\n",
              " 'entire',\n",
              " 'sales',\n",
              " 'conversion',\n",
              " 'process',\n",
              " 'click',\n",
              " 'adword',\n",
              " 'ad',\n",
              " 'final',\n",
              " 'transaction',\n",
              " 'order',\n",
              " 'uncover',\n",
              " 'insights',\n",
              " 'conversion',\n",
              " 'process',\n",
              " 'improved',\n",
              " '1',\n",
              " 'optimize',\n",
              " 'funnel',\n",
              " 'conversion',\n",
              " 'type',\n",
              " 'tmobile',\n",
              " 'uses',\n",
              " 'multiple',\n",
              " 'indicators',\n",
              " 'billing',\n",
              " 'sentiment',\n",
              " 'analysis',\n",
              " 'order',\n",
              " 'identify',\n",
              " 'customers',\n",
              " 'upgraded',\n",
              " 'higher',\n",
              " 'quality',\n",
              " 'products',\n",
              " 'identify',\n",
              " 'high',\n",
              " 'lifetime',\n",
              " 'customervalue',\n",
              " 'team',\n",
              " 'focus',\n",
              " 'retaining',\n",
              " 'customers',\n",
              " 'purpose',\n",
              " 'industry',\n",
              " 'communication',\n",
              " 'company',\n",
              " 'tmobile',\n",
              " 'employees',\n",
              " '38000',\n",
              " 'tweet',\n",
              " 'optimize',\n",
              " 'funnel',\n",
              " 'conversion',\n",
              " 'type',\n",
              " 'celcom',\n",
              " 'axiata',\n",
              " 'berhad',\n",
              " 'adopted',\n",
              " 'big',\n",
              " 'data',\n",
              " 'solution',\n",
              " 'order',\n",
              " 'improve',\n",
              " 'customer',\n",
              " 'retention',\n",
              " 'boost',\n",
              " 'market',\n",
              " 'share',\n",
              " 'improving',\n",
              " 'marketing',\n",
              " 'campaign',\n",
              " 'process',\n",
              " 'company',\n",
              " 'realtime',\n",
              " 'data',\n",
              " 'create',\n",
              " 'personalized',\n",
              " 'campaign',\n",
              " 'customer',\n",
              " 'based',\n",
              " 'products',\n",
              " 'offers',\n",
              " 'customer',\n",
              " 'want',\n",
              " 'need',\n",
              " 'purpose',\n",
              " 'industry',\n",
              " 'communication',\n",
              " 'company',\n",
              " 'celcom',\n",
              " 'axiata',\n",
              " 'berhad',\n",
              " 'employees',\n",
              " 'enterprise',\n",
              " 'tweet',\n",
              " 'optimize',\n",
              " 'funnel',\n",
              " 'conversion',\n",
              " 'access',\n",
              " 'data',\n",
              " 'consumer',\n",
              " 'behavior',\n",
              " 'companies',\n",
              " 'learn',\n",
              " 'prompts',\n",
              " 'customer',\n",
              " 'stick',\n",
              " 'longer',\n",
              " 'learn',\n",
              " 'customers',\n",
              " 'characteristics',\n",
              " 'purchasing',\n",
              " 'habits',\n",
              " 'order',\n",
              " 'improve',\n",
              " 'marketing',\n",
              " 'efforts',\n",
              " 'boost',\n",
              " 'profits',\n",
              " '2',\n",
              " 'behavioral',\n",
              " 'analytics',\n",
              " '18',\n",
              " 'billion',\n",
              " 'customers',\n",
              " 'mastercard',\n",
              " 'unique',\n",
              " 'position',\n",
              " 'able',\n",
              " 'analyze',\n",
              " 'behavior',\n",
              " 'customers',\n",
              " 'stores',\n",
              " 'thousands',\n",
              " 'retailers',\n",
              " 'company',\n",
              " 'teamed',\n",
              " 'mu',\n",
              " 'sigma',\n",
              " 'collect',\n",
              " 'analyze',\n",
              " 'data',\n",
              " 'shoppers',\n",
              " 'behavior',\n",
              " 'provide',\n",
              " 'insights',\n",
              " 'finds',\n",
              " 'retailers',\n",
              " 'benchmarking',\n",
              " 'reports',\n",
              " 'purpose',\n",
              " 'industry',\n",
              " 'finance',\n",
              " 'company',\n",
              " 'mastercard',\n",
              " 'employees',\n",
              " '67000',\n",
              " 'type',\n",
              " 'behavioral',\n",
              " 'analytics',\n",
              " 'tweet',\n",
              " 'services',\n",
              " 'like',\n",
              " 'hulu',\n",
              " 'netflix',\n",
              " 'competing',\n",
              " 'viewers',\n",
              " 'attention',\n",
              " 'time',\n",
              " 'warner',\n",
              " 'collects',\n",
              " 'data',\n",
              " 'frequently',\n",
              " 'customers',\n",
              " 'tune',\n",
              " 'effect',\n",
              " 'bandwidth',\n",
              " 'consumer',\n",
              " 'behavior',\n",
              " 'customer',\n",
              " 'engagement',\n",
              " 'peak',\n",
              " 'usage',\n",
              " 'times',\n",
              " 'order',\n",
              " 'improve',\n",
              " 'service',\n",
              " 'increase',\n",
              " 'profits',\n",
              " 'company',\n",
              " 'segments',\n",
              " 'customers',\n",
              " 'advertisers',\n",
              " 'correlating',\n",
              " 'viewing',\n",
              " 'habits',\n",
              " 'public',\n",
              " 'datasuch',\n",
              " 'voter',\n",
              " 'registration',\n",
              " 'informationin',\n",
              " 'order',\n",
              " 'launch',\n",
              " 'highly',\n",
              " 'targeted',\n",
              " 'campaigns',\n",
              " 'specific',\n",
              " 'locations',\n",
              " 'demographics',\n",
              " 'purpose',\n",
              " 'industry',\n",
              " 'entertainment',\n",
              " 'company',\n",
              " 'time',\n",
              " 'warner',\n",
              " ...]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize(tokens):\n",
        "    \"\"\"\n",
        "    Lemmatizes a list of tokens using spaCy with POS awareness.\n",
        "\n",
        "    Args:\n",
        "        tokens (list of str): List of tokens.\n",
        "\n",
        "    Returns:\n",
        "        list of str: Lemmatized tokens.\n",
        "    \"\"\"\n",
        "    doc = nlp(\" \".join(tokens))  # join for contextual tagging\n",
        "    return [token.lemma_ for token in doc if token.lemma_.strip()]\n"
      ],
      "metadata": {
        "id": "cjGg2SS99WPJ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_lemmatized = lemmatize(list_filtered_from_stopwords)"
      ],
      "metadata": {
        "id": "ZV2xAVLE9aSe"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(list_lemmatized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "E-r1t-ld-NSq",
        "outputId": "cae82f55-c2a3-475b-fc76-d9c50fe8329d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "['infoh515',\n",
              " 'big',\n",
              " 'datum',\n",
              " 'distribute',\n",
              " 'management',\n",
              " 'lecture',\n",
              " '1',\n",
              " 'introduction',\n",
              " 'hdfs',\n",
              " 'mapreduce',\n",
              " 'dimitris',\n",
              " 'sacharidis',\n",
              " 'lecture',\n",
              " 'outline',\n",
              " 'big',\n",
              " 'datum',\n",
              " 'big',\n",
              " 'datum',\n",
              " 'big',\n",
              " 'datacenter',\n",
              " 'hdfs',\n",
              " 'mapreduce',\n",
              " '1',\n",
              " 'big',\n",
              " 'datum',\n",
              " 'big',\n",
              " 'datum',\n",
              " 'like',\n",
              " 'teenage',\n",
              " 'sex',\n",
              " 'talk',\n",
              " 'know',\n",
              " 'think',\n",
              " 'claim',\n",
              " 'dan',\n",
              " 'ariely',\n",
              " '3',\n",
              " 'big',\n",
              " 'data',\n",
              " 'datum',\n",
              " 'analytic',\n",
              " 'datum',\n",
              " 'science',\n",
              " 'big',\n",
              " '4',\n",
              " 'big',\n",
              " 'datum',\n",
              " 'big',\n",
              " '4',\n",
              " 'big',\n",
              " 'data',\n",
              " 'history',\n",
              " 'big',\n",
              " '4',\n",
              " 'source',\n",
              " 'economist',\n",
              " 'february',\n",
              " '25',\n",
              " '2010',\n",
              " 'datum',\n",
              " 'deluge',\n",
              " 'eighteen',\n",
              " 'month',\n",
              " 'ago',\n",
              " 'li',\n",
              " 'fung',\n",
              " 'firm',\n",
              " 'manage',\n",
              " 'supply',\n",
              " 'chain',\n",
              " 'retailer',\n",
              " 'see',\n",
              " '100',\n",
              " 'gigabyte',\n",
              " 'information',\n",
              " 'flow',\n",
              " 'network',\n",
              " 'day',\n",
              " 'increase',\n",
              " 'tenfold',\n",
              " '2009',\n",
              " 'american',\n",
              " 'drone',\n",
              " 'aircraft',\n",
              " 'fly',\n",
              " 'iraq',\n",
              " 'afghanistan',\n",
              " 'send',\n",
              " '24',\n",
              " 'year',\n",
              " 'worth',\n",
              " 'video',\n",
              " 'footage',\n",
              " 'new',\n",
              " 'model',\n",
              " 'deploy',\n",
              " 'year',\n",
              " 'produce',\n",
              " 'time',\n",
              " 'datum',\n",
              " 'stream',\n",
              " 'predecessor',\n",
              " '2011',\n",
              " 'produce',\n",
              " '30',\n",
              " 'time',\n",
              " 'source',\n",
              " 'economist',\n",
              " 'february',\n",
              " '25',\n",
              " '2010',\n",
              " 'datum',\n",
              " 'deluge',\n",
              " 'look',\n",
              " 'quantity',\n",
              " 'information',\n",
              " 'world',\n",
              " 'soar',\n",
              " 'accord',\n",
              " 'estimate',\n",
              " 'mankind',\n",
              " 'create',\n",
              " '150',\n",
              " 'exabyte',\n",
              " 'billion',\n",
              " 'gigabyte',\n",
              " 'datum',\n",
              " '2005',\n",
              " 'year',\n",
              " 'create',\n",
              " '1200',\n",
              " 'exabyte',\n",
              " 'merely',\n",
              " 'keep',\n",
              " 'flood',\n",
              " 'store',\n",
              " 'bit',\n",
              " 'useful',\n",
              " 'difficult',\n",
              " 'analyse',\n",
              " 'spot',\n",
              " 'pattern',\n",
              " 'extract',\n",
              " 'useful',\n",
              " 'information',\n",
              " 'hard',\n",
              " 'source',\n",
              " 'economist',\n",
              " 'february',\n",
              " '25',\n",
              " '2010',\n",
              " 'datum',\n",
              " 'deluge',\n",
              " '2030s',\n",
              " 'computer',\n",
              " 'datum',\n",
              " 'storage',\n",
              " 'surpass',\n",
              " '1',\n",
              " 'yottabyte',\n",
              " '1024',\n",
              " 'large',\n",
              " 'number',\n",
              " 'official',\n",
              " 'metric',\n",
              " 'prefix',\n",
              " 'international',\n",
              " 'bureau',\n",
              " 'weight',\n",
              " 'measure',\n",
              " 'bipm',\n",
              " 'paris',\n",
              " 'recommend',\n",
              " 'new',\n",
              " 'namesronna',\n",
              " 'queccaas',\n",
              " 'prefix',\n",
              " '1027',\n",
              " '1030',\n",
              " '1',\n",
              " 'gigabyte',\n",
              " '109',\n",
              " 'byte',\n",
              " '1',\n",
              " 'terabyte',\n",
              " '1012',\n",
              " 'byte',\n",
              " '1',\n",
              " 'petabyte',\n",
              " '1015',\n",
              " 'byte',\n",
              " '1',\n",
              " 'exabyte',\n",
              " '1018',\n",
              " 'byte',\n",
              " '1',\n",
              " 'zetabyte',\n",
              " '1021',\n",
              " 'byte',\n",
              " '1',\n",
              " 'yottabyte',\n",
              " '1024',\n",
              " 'byte',\n",
              " '1',\n",
              " 'ronnabyte',\n",
              " '1027',\n",
              " 'byte',\n",
              " '1',\n",
              " 'queccabyte',\n",
              " '1030',\n",
              " 'byte',\n",
              " 'httpswwwscienceorgcontentarticleyouknowkilomegaandgigametricsystemreadyronnaandquecca',\n",
              " 'problematic',\n",
              " 'assume',\n",
              " 'you',\n",
              " 're',\n",
              " 'google',\n",
              " 'begin',\n",
              " '2000s',\n",
              " 'want',\n",
              " 'index',\n",
              " 'web',\n",
              " 'problematic',\n",
              " 'hard',\n",
              " 'accurately',\n",
              " 'determine',\n",
              " 'size',\n",
              " 'web',\n",
              " 'point',\n",
              " 'time',\n",
              " 'safe',\n",
              " 'consist',\n",
              " 'hundred',\n",
              " 'billion',\n",
              " 'individual',\n",
              " 'document',\n",
              " 'continue',\n",
              " 'grow',\n",
              " 'assume',\n",
              " 'web',\n",
              " 'contain',\n",
              " '100',\n",
              " 'billion',\n",
              " 'document',\n",
              " 'average',\n",
              " 'document',\n",
              " 'size',\n",
              " '4',\n",
              " 'kb',\n",
              " 'compression',\n",
              " 'web',\n",
              " '400',\n",
              " 'tb',\n",
              " 'size',\n",
              " 'result',\n",
              " 'invert',\n",
              " 'web',\n",
              " 'search',\n",
              " 'index',\n",
              " 'depend',\n",
              " 'specific',\n",
              " 'implementation',\n",
              " 'tend',\n",
              " 'order',\n",
              " 'magnitude',\n",
              " 'original',\n",
              " 'repository',\n",
              " 'source',\n",
              " 'datacenter',\n",
              " 'computer',\n",
              " 'luiz',\n",
              " 'andré',\n",
              " 'barroso',\n",
              " 'google',\n",
              " 'jimmy',\n",
              " 'clidaras',\n",
              " 'google',\n",
              " 'urs',\n",
              " 'hölzle',\n",
              " 'google',\n",
              " 'big',\n",
              " 'data',\n",
              " 'definition',\n",
              " '14',\n",
              " 'big',\n",
              " 'data',\n",
              " 'term',\n",
              " 'encompass',\n",
              " 'use',\n",
              " 'technique',\n",
              " 'capture',\n",
              " 'pro',\n",
              " 'cess',\n",
              " 'analyse',\n",
              " 'visualize',\n",
              " 'potentially',\n",
              " 'large',\n",
              " 'dataset',\n",
              " 'reasonable',\n",
              " 'timeframe',\n",
              " 'accessible',\n",
              " 'standard',\n",
              " 'technology',\n",
              " 'extension',\n",
              " 'platform',\n",
              " 'tool',\n",
              " 'software',\n",
              " 'purpose',\n",
              " 'collectively',\n",
              " 'call',\n",
              " 'big',\n",
              " 'datum',\n",
              " 'technology',\n",
              " 'nessi1',\n",
              " '1the',\n",
              " 'european',\n",
              " 'technology',\n",
              " 'platform',\n",
              " 'dedicate',\n",
              " 'software',\n",
              " 'service',\n",
              " 'data',\n",
              " '5',\n",
              " 'big',\n",
              " 'data',\n",
              " 'definition',\n",
              " '24',\n",
              " 'big',\n",
              " 'datum',\n",
              " 'refer',\n",
              " 'dataset',\n",
              " 'size',\n",
              " 'ability',\n",
              " 'typical',\n",
              " 'database',\n",
              " 'software',\n",
              " 'tool',\n",
              " 'capture',\n",
              " 'store',\n",
              " 'manage',\n",
              " 'analyze',\n",
              " 'definition',\n",
              " 'intentionally',\n",
              " 'subjective',\n",
              " 'incorporate',\n",
              " 'move',\n",
              " 'defi',\n",
              " 'nition',\n",
              " 'big',\n",
              " 'dataset',\n",
              " 'need',\n",
              " 'order',\n",
              " 'consider',\n",
              " 'big',\n",
              " 'datum',\n",
              " 'assume',\n",
              " 'technology',\n",
              " 'advance',\n",
              " 'time',\n",
              " 'size',\n",
              " 'dataset',\n",
              " 'qualify',\n",
              " 'big',\n",
              " 'datum',\n",
              " 'increase',\n",
              " 'mckinsey',\n",
              " 'global',\n",
              " 'institute',\n",
              " '2',\n",
              " '2big',\n",
              " 'data',\n",
              " 'frontier',\n",
              " 'innovation',\n",
              " 'competition',\n",
              " 'productivity',\n",
              " 'june',\n",
              " '2011',\n",
              " '6',\n",
              " 'big',\n",
              " 'data',\n",
              " 'definition',\n",
              " '34',\n",
              " 'big',\n",
              " 'data',\n",
              " 'data',\n",
              " 'set',\n",
              " 'voluminous',\n",
              " 'complex',\n",
              " 'tra',\n",
              " 'ditional',\n",
              " 'datum',\n",
              " 'processing',\n",
              " 'application',\n",
              " 'software',\n",
              " 'inadequate',\n",
              " 'deal',\n",
              " 'big',\n",
              " 'datum',\n",
              " 'challenge',\n",
              " 'include',\n",
              " 'capture',\n",
              " 'data',\n",
              " 'datum',\n",
              " 'storage',\n",
              " 'datum',\n",
              " 'analysis',\n",
              " 'search',\n",
              " 'sharing',\n",
              " 'transfer',\n",
              " 'visualization',\n",
              " 'query',\n",
              " 'updat',\n",
              " 'ing',\n",
              " 'information',\n",
              " 'privacy',\n",
              " 'lately',\n",
              " 'term',\n",
              " 'big',\n",
              " 'datum',\n",
              " 'tends',\n",
              " 'refer',\n",
              " 'use',\n",
              " 'predictive',\n",
              " 'analyt',\n",
              " 'ics',\n",
              " 'user',\n",
              " 'behavior',\n",
              " 'analytic',\n",
              " 'certain',\n",
              " 'advanced',\n",
              " 'data',\n",
              " 'analytic',\n",
              " 'method',\n",
              " 'extract',\n",
              " 'value',\n",
              " 'datum',\n",
              " 'seldom',\n",
              " 'particular',\n",
              " 'size',\n",
              " 'datum',\n",
              " 'set',\n",
              " 'wikipedia',\n",
              " '7',\n",
              " 'big',\n",
              " 'data',\n",
              " 'definition',\n",
              " '44',\n",
              " 'big',\n",
              " 'datum',\n",
              " 'refer',\n",
              " 'extremely',\n",
              " 'large',\n",
              " 'dataset',\n",
              " 'easily',\n",
              " 'man',\n",
              " 'age',\n",
              " 'process',\n",
              " 'analyze',\n",
              " 'traditional',\n",
              " 'datum',\n",
              " 'process',\n",
              " 'meth',\n",
              " 'od',\n",
              " 'characterize',\n",
              " 'vs',\n",
              " 'volume',\n",
              " 'variety',\n",
              " 'velocity',\n",
              " 'describe',\n",
              " 'sheer',\n",
              " 'datum',\n",
              " 'diversity',\n",
              " 'datum',\n",
              " 'type',\n",
              " 'speed',\n",
              " 'generate',\n",
              " 'big',\n",
              " 'datum',\n",
              " 'technology',\n",
              " 'enable',\n",
              " 'business',\n",
              " 'researcher',\n",
              " 'gain',\n",
              " 'insight',\n",
              " 'datadriven',\n",
              " 'de',\n",
              " 'cision',\n",
              " 'vast',\n",
              " 'dynamic',\n",
              " 'dataset',\n",
              " 'chatgpt',\n",
              " '022025',\n",
              " '8',\n",
              " 'big',\n",
              " 'data',\n",
              " 'datum',\n",
              " 'analytic',\n",
              " 'datum',\n",
              " 'science',\n",
              " 'big',\n",
              " '9',\n",
              " 'datum',\n",
              " 'analytic',\n",
              " 'big',\n",
              " '9',\n",
              " 'datum',\n",
              " 'analytic',\n",
              " 'definition',\n",
              " 'datum',\n",
              " 'analysis',\n",
              " 'know',\n",
              " 'analysis',\n",
              " 'data',\n",
              " 'datum',\n",
              " 'analytic',\n",
              " 'process',\n",
              " 'inspect',\n",
              " 'cleansing',\n",
              " 'transform',\n",
              " 'modeling',\n",
              " 'datum',\n",
              " 'goal',\n",
              " 'discover',\n",
              " 'useful',\n",
              " 'information',\n",
              " 'suggest',\n",
              " 'conclu',\n",
              " 'sion',\n",
              " 'support',\n",
              " 'decisionmake',\n",
              " 'wikipedia',\n",
              " '10',\n",
              " '11',\n",
              " 'big',\n",
              " 'data',\n",
              " 'datum',\n",
              " 'analytic',\n",
              " 'datum',\n",
              " 'science',\n",
              " 'big',\n",
              " '12',\n",
              " 'datum',\n",
              " 'science',\n",
              " 'big',\n",
              " '12',\n",
              " 'datum',\n",
              " 'science',\n",
              " 'definition',\n",
              " '13',\n",
              " 'datum',\n",
              " 'science',\n",
              " 'study',\n",
              " 'generalizable',\n",
              " 'extraction',\n",
              " 'knowledge',\n",
              " 'datum',\n",
              " 'vasant',\n",
              " 'dhar3',\n",
              " 'article',\n",
              " 'term',\n",
              " 'science',\n",
              " 'imply',\n",
              " 'knowledge',\n",
              " 'gain',\n",
              " 'systematic',\n",
              " 'study',\n",
              " 'datum',\n",
              " 'scientist',\n",
              " 'require',\n",
              " 'integrate',\n",
              " 'skill',\n",
              " 'set',\n",
              " 'span',\n",
              " 'mathematic',\n",
              " 'machine',\n",
              " 'learn',\n",
              " 'artificial',\n",
              " 'intelligence',\n",
              " 'statistic',\n",
              " 'database',\n",
              " 'optimization',\n",
              " 'deep',\n",
              " 'understanding',\n",
              " 'craft',\n",
              " 'problem',\n",
              " 'formulation',\n",
              " 'engineer',\n",
              " 'effective',\n",
              " 'solution',\n",
              " '3data',\n",
              " 'science',\n",
              " 'prediction',\n",
              " 'communication',\n",
              " 'acm',\n",
              " '5612',\n",
              " '2014',\n",
              " '13',\n",
              " 'datum',\n",
              " 'science',\n",
              " 'definition',\n",
              " '23',\n",
              " 'datum',\n",
              " 'science',\n",
              " 'know',\n",
              " 'datadriven',\n",
              " 'science',\n",
              " 'interdisci',\n",
              " 'plinary',\n",
              " 'field',\n",
              " 'scientific',\n",
              " 'method',\n",
              " 'process',\n",
              " 'system',\n",
              " 'extract',\n",
              " 'knowledge',\n",
              " 'insight',\n",
              " 'data',\n",
              " 'form',\n",
              " 'structure',\n",
              " 'unstructured',\n",
              " 'data',\n",
              " 'science',\n",
              " 'concept',\n",
              " 'unify',\n",
              " 'statistic',\n",
              " 'datum',\n",
              " 'analysis',\n",
              " 'relate',\n",
              " 'method',\n",
              " 'order',\n",
              " 'understand',\n",
              " 'analyze',\n",
              " 'actual',\n",
              " 'phenom',\n",
              " 'ena',\n",
              " 'datum',\n",
              " 'employ',\n",
              " 'technique',\n",
              " 'theory',\n",
              " 'draw',\n",
              " 'field',\n",
              " 'broad',\n",
              " 'area',\n",
              " 'mathematic',\n",
              " 'statistic',\n",
              " 'information',\n",
              " 'sci',\n",
              " 'ence',\n",
              " 'computer',\n",
              " 'science',\n",
              " 'wikipedia',\n",
              " '14',\n",
              " 'datum',\n",
              " 'science',\n",
              " 'definition',\n",
              " '33',\n",
              " 'datum',\n",
              " 'science',\n",
              " 'field',\n",
              " 'combine',\n",
              " 'statistical',\n",
              " 'analysis',\n",
              " 'machine',\n",
              " 'learn',\n",
              " 'datum',\n",
              " 'processing',\n",
              " 'technique',\n",
              " 'extract',\n",
              " 'valuable',\n",
              " 'insight',\n",
              " 'large',\n",
              " 'complex',\n",
              " 'dataset',\n",
              " 'involve',\n",
              " 'collect',\n",
              " 'clean',\n",
              " 'analyse',\n",
              " 'interpret',\n",
              " 'datum',\n",
              " 'inform',\n",
              " 'decisionmake',\n",
              " 'solve',\n",
              " 'problem',\n",
              " 'industry',\n",
              " 'data',\n",
              " 'scientist',\n",
              " 'use',\n",
              " 'programming',\n",
              " 'mathematic',\n",
              " 'domain',\n",
              " 'expertise',\n",
              " 'uncover',\n",
              " 'pattern',\n",
              " 'trend',\n",
              " 'datum',\n",
              " 'chatgpt',\n",
              " '022025',\n",
              " '15',\n",
              " 'conclusion',\n",
              " '16',\n",
              " 'recall',\n",
              " 'course',\n",
              " 'objective',\n",
              " 'introduce',\n",
              " 'fundamental',\n",
              " 'notion',\n",
              " 'principle',\n",
              " 'research',\n",
              " 'result',\n",
              " 'concern',\n",
              " 'modern',\n",
              " 'scalable',\n",
              " 'faulttolerant',\n",
              " 'way',\n",
              " 'manage',\n",
              " 'analyze',\n",
              " 'massive',\n",
              " 'amount',\n",
              " 'datum',\n",
              " 'parallel',\n",
              " 'distribute',\n",
              " 'system',\n",
              " 'course',\n",
              " 'concern',\n",
              " 'big',\n",
              " 'datum',\n",
              " 'big',\n",
              " 'data',\n",
              " 'analytic',\n",
              " 'relevant',\n",
              " 'data',\n",
              " 'science',\n",
              " 'challenge',\n",
              " 'big',\n",
              " 'datum',\n",
              " 'source',\n",
              " 'httpwwwibmbigdatahubcomsitesdefaultfilesinfographic_file',\n",
              " '17',\n",
              " 'big',\n",
              " 'datum',\n",
              " 'big',\n",
              " 'datum',\n",
              " 'slide',\n",
              " 'take',\n",
              " 'httpswwwslidesharenetdellbigdatausecases3601989',\n",
              " '19',\n",
              " '1',\n",
              " 'optimize',\n",
              " 'funnel',\n",
              " 'conversion',\n",
              " '2',\n",
              " 'behavioral',\n",
              " 'analytic',\n",
              " '3',\n",
              " 'customer',\n",
              " 'segmentation',\n",
              " '4',\n",
              " 'predictive',\n",
              " 'support',\n",
              " '5',\n",
              " 'market',\n",
              " 'basket',\n",
              " 'analysis',\n",
              " 'pricing',\n",
              " 'optimization',\n",
              " '6',\n",
              " 'predict',\n",
              " 'security',\n",
              " 'threat',\n",
              " '7',\n",
              " 'fraud',\n",
              " 'detection',\n",
              " '8',\n",
              " 'industry',\n",
              " 'specific',\n",
              " 'big',\n",
              " 'datum',\n",
              " 'use',\n",
              " 'case',\n",
              " 'big',\n",
              " 'data',\n",
              " 'analytic',\n",
              " 'allow',\n",
              " 'company',\n",
              " 'track',\n",
              " 'lead',\n",
              " 'entire',\n",
              " 'sale',\n",
              " 'conversion',\n",
              " 'process',\n",
              " 'click',\n",
              " 'adword',\n",
              " 'ad',\n",
              " 'final',\n",
              " 'transaction',\n",
              " 'order',\n",
              " 'uncover',\n",
              " 'insight',\n",
              " 'conversion',\n",
              " 'process',\n",
              " 'improve',\n",
              " '1',\n",
              " 'optimize',\n",
              " 'funnel',\n",
              " 'conversion',\n",
              " 'type',\n",
              " 'tmobile',\n",
              " 'use',\n",
              " 'multiple',\n",
              " 'indicator',\n",
              " 'bill',\n",
              " 'sentiment',\n",
              " 'analysis',\n",
              " 'order',\n",
              " 'identify',\n",
              " 'customer',\n",
              " 'upgrade',\n",
              " 'high',\n",
              " 'quality',\n",
              " 'product',\n",
              " 'identify',\n",
              " 'high',\n",
              " 'lifetime',\n",
              " 'customervalue',\n",
              " 'team',\n",
              " 'focus',\n",
              " 'retain',\n",
              " 'customer',\n",
              " 'purpose',\n",
              " 'industry',\n",
              " 'communication',\n",
              " 'company',\n",
              " 'tmobile',\n",
              " 'employee',\n",
              " '38000',\n",
              " 'tweet',\n",
              " 'optimize',\n",
              " 'funnel',\n",
              " 'conversion',\n",
              " 'type',\n",
              " 'celcom',\n",
              " 'axiata',\n",
              " 'berhad',\n",
              " 'adopt',\n",
              " 'big',\n",
              " 'datum',\n",
              " 'solution',\n",
              " 'order',\n",
              " 'improve',\n",
              " 'customer',\n",
              " 'retention',\n",
              " 'boost',\n",
              " 'market',\n",
              " 'share',\n",
              " 'improve',\n",
              " 'marketing',\n",
              " 'campaign',\n",
              " 'process',\n",
              " 'company',\n",
              " 'realtime',\n",
              " 'datum',\n",
              " 'create',\n",
              " 'personalized',\n",
              " 'campaign',\n",
              " 'customer',\n",
              " 'base',\n",
              " 'product',\n",
              " 'offer',\n",
              " 'customer',\n",
              " 'want',\n",
              " 'need',\n",
              " 'purpose',\n",
              " 'industry',\n",
              " 'communication',\n",
              " 'company',\n",
              " 'celcom',\n",
              " 'axiata',\n",
              " 'berhad',\n",
              " 'employee',\n",
              " 'enterprise',\n",
              " 'tweet',\n",
              " 'optimize',\n",
              " 'funnel',\n",
              " 'conversion',\n",
              " 'access',\n",
              " 'data',\n",
              " 'consumer',\n",
              " 'behavior',\n",
              " 'company',\n",
              " 'learn',\n",
              " 'prompt',\n",
              " 'customer',\n",
              " 'stick',\n",
              " 'long',\n",
              " 'learn',\n",
              " 'customer',\n",
              " 'characteristic',\n",
              " 'purchasing',\n",
              " 'habit',\n",
              " 'order',\n",
              " 'improve',\n",
              " 'marketing',\n",
              " 'effort',\n",
              " 'boost',\n",
              " 'profit',\n",
              " '2',\n",
              " 'behavioral',\n",
              " 'analytic',\n",
              " '18',\n",
              " 'billion',\n",
              " 'customer',\n",
              " 'mastercard',\n",
              " 'unique',\n",
              " 'position',\n",
              " 'able',\n",
              " 'analyze',\n",
              " 'behavior',\n",
              " 'customer',\n",
              " 'store',\n",
              " 'thousand',\n",
              " 'retailer',\n",
              " 'company',\n",
              " 'team',\n",
              " 'mu',\n",
              " 'sigma',\n",
              " 'collect',\n",
              " 'analyze',\n",
              " 'datum',\n",
              " 'shopper',\n",
              " 'behavior',\n",
              " 'provide',\n",
              " 'insight',\n",
              " 'find',\n",
              " 'retailer',\n",
              " 'benchmarking',\n",
              " 'report',\n",
              " 'purpose',\n",
              " 'industry',\n",
              " 'finance',\n",
              " 'company',\n",
              " 'mastercard',\n",
              " 'employee',\n",
              " '67000',\n",
              " 'type',\n",
              " 'behavioral',\n",
              " 'analytic',\n",
              " 'tweet',\n",
              " 'service',\n",
              " 'like',\n",
              " 'hulu',\n",
              " 'netflix',\n",
              " 'compete',\n",
              " 'viewer',\n",
              " 'attention',\n",
              " 'time',\n",
              " 'warner',\n",
              " 'collect',\n",
              " 'datum',\n",
              " 'frequently',\n",
              " 'customer',\n",
              " 'tune',\n",
              " 'effect',\n",
              " 'bandwidth',\n",
              " 'consumer',\n",
              " 'behavior',\n",
              " 'customer',\n",
              " 'engagement',\n",
              " 'peak',\n",
              " 'usage',\n",
              " 'time',\n",
              " 'order',\n",
              " 'improve',\n",
              " 'service',\n",
              " 'increase',\n",
              " 'profit',\n",
              " 'company',\n",
              " 'segment',\n",
              " 'customer',\n",
              " 'advertiser',\n",
              " 'correlate',\n",
              " 'viewing',\n",
              " 'habit',\n",
              " 'public',\n",
              " 'datasuch',\n",
              " 'voter',\n",
              " 'registration',\n",
              " 'informationin',\n",
              " 'order',\n",
              " 'launch',\n",
              " 'highly',\n",
              " 'target',\n",
              " 'campaign',\n",
              " 'specific',\n",
              " 'location',\n",
              " 'demographic',\n",
              " 'purpose',\n",
              " 'industry',\n",
              " 'entertainment',\n",
              " 'company',\n",
              " 'time',\n",
              " ...]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_tokens(tokens, chunk_size):\n",
        "    \"\"\"\n",
        "    Splits a list of tokens into chunks of fixed size.\n",
        "\n",
        "    Args:\n",
        "        tokens (list of str): List of tokens to be chunked.\n",
        "        chunk_size (int): Number of tokens per chunk (set externally).\n",
        "\n",
        "    Returns:\n",
        "        list of str: List of text chunks (each chunk is a single string).\n",
        "    \"\"\"\n",
        "    return [\n",
        "        \" \".join(tokens[i:i + chunk_size])\n",
        "        for i in range(0, len(tokens), chunk_size)\n",
        "    ]\n"
      ],
      "metadata": {
        "id": "MPLPIytj_bOw"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_chunks = chunk_tokens(list_lemmatized, chunk_size)\n"
      ],
      "metadata": {
        "id": "YHIRI13o_zQ_"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(list_chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Tw-Trr8nAAGB",
        "outputId": "d3ab0970-d177-47c3-fedf-bf77978d2d4c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "['infoh515 big datum distribute management lecture 1 introduction hdfs mapreduce dimitris sacharidis lecture outline big datum big datum big datacenter hdfs mapreduce 1 big datum big datum like teenage sex talk know think claim dan ariely 3 big data datum analytic datum science big 4 big datum big 4 big data history big 4 source economist february 25 2010 datum deluge eighteen month ago li fung firm manage supply chain retailer see 100 gigabyte information flow network day increase tenfold 2009 american drone aircraft fly iraq afghanistan send 24 year worth video footage new model deploy year produce time datum stream predecessor 2011 produce 30 time source economist february 25 2010 datum deluge look quantity information world soar accord estimate mankind create 150 exabyte billion gigabyte datum 2005 year create 1200 exabyte merely keep flood store bit useful difficult analyse spot pattern extract useful information hard source economist february 25 2010 datum deluge 2030s computer datum storage surpass 1 yottabyte 1024 large number official metric prefix international bureau weight measure bipm paris recommend new namesronna queccaas prefix 1027 1030 1 gigabyte 109 byte 1 terabyte 1012 byte 1 petabyte 1015 byte 1 exabyte 1018 byte 1 zetabyte 1021 byte 1 yottabyte 1024 byte 1 ronnabyte 1027 byte 1 queccabyte 1030 byte httpswwwscienceorgcontentarticleyouknowkilomegaandgigametricsystemreadyronnaandquecca problematic assume you re google begin 2000s want index web problematic hard accurately determine size web point time safe consist hundred billion individual document continue grow assume web contain 100 billion document average document size 4 kb compression web 400 tb size result invert web search index depend specific implementation tend order magnitude original repository source datacenter computer luiz andré barroso google jimmy clidaras google urs hölzle google big data definition 14 big data term encompass use technique capture pro cess analyse visualize potentially large dataset reasonable timeframe',\n",
              " 'accessible standard technology extension platform tool software purpose collectively call big datum technology nessi1 1the european technology platform dedicate software service data 5 big data definition 24 big datum refer dataset size ability typical database software tool capture store manage analyze definition intentionally subjective incorporate move defi nition big dataset need order consider big datum assume technology advance time size dataset qualify big datum increase mckinsey global institute 2 2big data frontier innovation competition productivity june 2011 6 big data definition 34 big data data set voluminous complex tra ditional datum processing application software inadequate deal big datum challenge include capture data datum storage datum analysis search sharing transfer visualization query updat ing information privacy lately term big datum tends refer use predictive analyt ics user behavior analytic certain advanced data analytic method extract value datum seldom particular size datum set wikipedia 7 big data definition 44 big datum refer extremely large dataset easily man age process analyze traditional datum process meth od characterize vs volume variety velocity describe sheer datum diversity datum type speed generate big datum technology enable business researcher gain insight datadriven de cision vast dynamic dataset chatgpt 022025 8 big data datum analytic datum science big 9 datum analytic big 9 datum analytic definition datum analysis know analysis data datum analytic process inspect cleansing transform modeling datum goal discover useful information suggest conclu sion support decisionmake wikipedia 10 11 big data datum analytic datum science big 12 datum science big 12 datum science definition 13 datum science study generalizable extraction knowledge datum vasant dhar3 article term science imply knowledge gain systematic study datum scientist require integrate skill set span mathematic machine learn artificial intelligence statistic database optimization deep understanding craft problem formulation engineer effective solution 3data science prediction communication acm 5612 2014 13 datum science definition',\n",
              " '23 datum science know datadriven science interdisci plinary field scientific method process system extract knowledge insight data form structure unstructured data science concept unify statistic datum analysis relate method order understand analyze actual phenom ena datum employ technique theory draw field broad area mathematic statistic information sci ence computer science wikipedia 14 datum science definition 33 datum science field combine statistical analysis machine learn datum processing technique extract valuable insight large complex dataset involve collect clean analyse interpret datum inform decisionmake solve problem industry data scientist use programming mathematic domain expertise uncover pattern trend datum chatgpt 022025 15 conclusion 16 recall course objective introduce fundamental notion principle research result concern modern scalable faulttolerant way manage analyze massive amount datum parallel distribute system course concern big datum big data analytic relevant data science challenge big datum source httpwwwibmbigdatahubcomsitesdefaultfilesinfographic_file 17 big datum big datum slide take httpswwwslidesharenetdellbigdatausecases3601989 19 1 optimize funnel conversion 2 behavioral analytic 3 customer segmentation 4 predictive support 5 market basket analysis pricing optimization 6 predict security threat 7 fraud detection 8 industry specific big datum use case big data analytic allow company track lead entire sale conversion process click adword ad final transaction order uncover insight conversion process improve 1 optimize funnel conversion type tmobile use multiple indicator bill sentiment analysis order identify customer upgrade high quality product identify high lifetime customervalue team focus retain customer purpose industry communication company tmobile employee 38000 tweet optimize funnel conversion type celcom axiata berhad adopt big datum solution order improve customer retention boost market share improve marketing campaign process company realtime datum create personalized campaign customer base product offer customer want need purpose industry communication company celcom axiata berhad employee enterprise tweet optimize funnel conversion access data consumer behavior company learn prompt customer stick long learn customer characteristic purchasing habit order',\n",
              " 'improve marketing effort boost profit 2 behavioral analytic 18 billion customer mastercard unique position able analyze behavior customer store thousand retailer company team mu sigma collect analyze datum shopper behavior provide insight find retailer benchmarking report purpose industry finance company mastercard employee 67000 type behavioral analytic tweet service like hulu netflix compete viewer attention time warner collect datum frequently customer tune effect bandwidth consumer behavior customer engagement peak usage time order improve service increase profit company segment customer advertiser correlate viewing habit public datasuch voter registration informationin order launch highly target campaign specific location demographic purpose industry entertainment company time warner cable employee 34000 type behavioral analytic customer segmentation tweet type behavioral analytic customer complaint pr crisis difficult handle thank social medium well track customer sentiment say company online nestle create 247 monitoring centre listen conversation company product social medium company actively engage post online order mitigate damage build customer loyalty purpose industry food beverage company nestlé employee 330000 tweet access datum consumer multiple source social medium datum transaction history company well segment target customer start personalize offer customer 3 customer segmentation thank partnership google facebook heineken access vast amount data customer use create realtime personalize marketing message project provide realtime content fan happen watch sponsor event purpose industry food beverage company heineken employee 64252 type customer segmentation tweet spotify use data user profile user playlist historical datum music play provide recommendation user combine datum million user spotify able recommendation particular user do not extensive history site purpose industry entertainment company spotify employee 5000 tweet type customer segmentation behavioral analytic sensor machinegenerated datum company identify malfunction likely occur company preemptively order part repair order avoid downtime lose profit 4 predictive support southwest analysis sensor datum plane order identify pattern indicate potential malfunction safety issue allow airline address potential problem necessary',\n",
              " 'repair interrupt flight put passenger danger purpose industry travel company southwest airline employee 45000 type predictive support tweet engine yard provide big data analytic user monitor performance application real time pinpoint problem infrastructure optimize platform correct performance issue purpose industry cloud storage company engine yard employee 130 type predictive support tweet quickly pull datum multiple source retailer well optimize product selection pricing decide target ad 5 market basket analysis pricing optimization type cocacola use algorithm ensure orange juice consistent taste year algorithm incorporate satellite imagery crop yield consumer preference detail flavour particular fruit order determine juice blend purpose industry food company cocacola co employee 146200 tweet market basket analysis big data analytic track trend security breach allow company proactively threat strike 6 predict security threat type rabobank analyse criminal activity atms determine factor increase risk victimize discover proximity highway weather condition season affect risk security threat purpose industry finance company rabobank employee 27000 tweet predict security threat type 15 billion item catalog amazon lot product track protect use cloud system s3 predict item likely steal well secure warehouse purpose industry online retail company amazon employee 110000 tweet predict security threat financial firm use big datum help identify sophisticated fraud scheme combine multiple point datum 7 fraud detection type zion bank use data analytic detect anomaly channel indicate potential fraud fraud team receive datum 140 sourcessome realtimeto monitor activity customer make mobile banking transaction time branch transaction purpose industry finance company zion bank employee 2700 tweet fraud detection type discovery health use big data analytic identify fraudulent claim possible fraudulent prescription example identify healthcare provider charge expensive procedure actually perform purpose industry insurance company discovery health employee 5000 tweet fraud detection virtually industry invest big datum help solve specific challenge industry face healthcare example use big datum improve patient outcome agriculture',\n",
              " 'use datum boost crop yield 8 industry specific kayak use big data analytic create predictive model tell user price particular flight week system use billion search query find cheap flight popular destination busy airport algorithm constantly improve track flight prediction correct purpose industry travel company kayak employee 101 type industry specific tweet type aurora collect internal national datum order create benchmark healthcare quality analyze datum group patient similar medical condition reveal trend disease identify right candidate medical research finally realtime datum analysis allow aurora predict improve patient outcome far reduce readmission 10 percent purpose industry health care company aurora health care employee 30000 tweet industry specific type shell use sensor datum map oil gas well order increase output boost efficiency operation datum receive sensor analyze artificial intelligence render 3d 4d map purpose industry oil company shell employee 87000 tweet industry specific takeaway example datum store process analyze turn value 20 big datacenter process big datum assume you re google begin 2000s want index web execution environment 1997 bunch computer web index fit single computer redundancy ensure availability execution environment 1999 compute server consist multiple cpus possibly multiple core cpu attach hard disk server collect rack highspeed ethernet connection 1gbps connect server rack google corckboard rack execution environment currently rack connected central network switch multigbps redundant link access datum rack slow datum computersame rack rack form datum center image google datacenter mons belgium discussion compute node keep simple design midrange computer cheap powerful highrange computer consume energy google lot characteristic figure source datacenter computer characteristic figure source datacenter computer capacity datum store serverrackdatacenter characteristic figure source datacenter computer latency time take fetch datum item ask local machineanother server rackanother server different rack characteristic figure source datacenter computer bandwidth speed datum transfer machineanother server rackanother server different rack characteristic figure source datacenter',\n",
              " 'computer conclusion huge storage capacity latency rack 110 latency rack level 110 latency server level bandwidth rack 110 bandwidth rack level ½ 110 bandwidth server level gain parallelism let consider maximal aggregate bandwidth speed analyze datum parallel assume ideal datum distribution server disk embarrassingly parallel example count number time word belgium appear document web server multiple cpus read multiple disk parallel server analyze document parallel end sum perserver counter fast gain parallelism let consider maximal aggregate bandwidth speed analyze datum parallel assume ideal datum distribution server disk component max aggr bandwidth 1 hard disk 100 mbsec 1 gbps server 12 hard disk 12 gbsec 12 gbps rack 80 server 96 gbsec 768 gbps clusterdatacenter 30 rack 288 tbsec 23 tbps scan 400 tb take 138 sec 23 minute scan 400 tb sequentially 100 mbsec take 4629 day challenge 12 scalable software development allow growth require rearchitecting algorithmapplication auto scale challenge 22 faulttolerance 1000 machine failure happen day faulttolerance typically address redundancy andor reexecution faulttolerance motivational example mean time failure mttf typical harddisk 1 000 000 hour 114 year 22 faulttolerance motivational example mean time failure mttf typical harddisk 1 000 000 hour 114 year annual failure rate afr give mttf afr hour 1 year mttf 365 24 106 0876 22 faulttolerance motivational example mean time failure mttf typical harddisk 1 000 000 hour 114 year annual failure rate afr give mttf afr hour 1 year mttf 365 24 106 0876 expect number disk failure year datacenter 100000 disk disk afr 105 0876 876disks year 2disks day 22 google solution new programming model framework distribute scalable datum analysis purpose google file system distribute file system scalable storage highthroughput retrieval map reduce programming model execution environment generalpurpose distribute batch processing bigtable nosql database dremel f1 query language interactive sql like analysis structure',\n",
              " 'dataset lot research ongoing open source impl apache hadoop hdfs mr apache hbase apache sparkdrill highlevel design describe series paper implementation available hdfs hdfs hadoop distribute file system hdfs architecture hdfs masterslave architecture master namenode nn manage file system regulate access file client slave datanode dn usually server cluster manage storage attach server run namenode datanode usersdsdat0txt r2 13 usersdsdat1txt r3 2 4 5 metadatafilename replication factor block id 1 2 4 5 2 5 3 1 3 4 2 4 5 file transparently break block default 128 mb block replicate datanode replication rackaware dat0txt 1 3 dat1txt 2 4 5 hdfs architecture hdfs masterslave architecture master namenode nn manage file system regulate access file client slave datanode dn usually server cluster manage storage attach server run namenode datanode usersdsdat0txt r2 13 usersdsdat1txt r3 2 4 5 metadatafilename replication factor block id 1 2 4 5 2 5 3 1 3 4 2 4 5 optimize large file read throughput appending write file appendonly replication ensure durability availability throughput hdfs common pitfall hdfs optimize large file single hdfs block 128 mb default sense store file small 128 mb hdfs file immutable optimize sequential access new file create append random access file create immutable change file require create hdfs implementation imply client need install jar file access hdfs namenode datanode piece software design run commodity machine machine typically run gnulinux operating system os hdfs build java language machine support java run namenode datanode software usage highly portable java language mean hdfs deploy wide range machine source hadoop documentation typical hdfs command binhadoop fs ls binhadoop fs mkdir binhadoop fs copyfromlocal binhadoop fs copytolocal binhadoop fs movetolocal binhadoop fs rm mapreduce mapreduce simplify datum process large cluster reaction complexity design new abstraction allow express simple computation try perform hide messy detail parallelization',\n",
              " 'fault tolerance datum distribution load balance library abstraction inspire map reduce primitive present lisp functional language realize computation involve apply map operation logical record input order compute set intermediate keyvalue pair apply reduce operation value share key order combine derive datum appropriately use functional model user specify map reduce operation allow parallelize large computation easily use reexecution primary mechanism fault tolerance mr computational model mr job specify function map reduce map key1 value1 listkey2 value2 input keyvalue pair represent logical record input datum source case file line input source database table record single input keyvalue pair result zero output keyvalue pair mr computational model mr job specify function map reduce reduce key2 listvalue2 listkey3 value3 reduce function call unique map output key receive list value emit key like map function reduce output zero keyvalue pair mr computational model word occur document web count number occurrence document def mapdocid line word line yield word 1 def reduceword listofoccnumber yield word sumlistofoccnumber mr opportunity parallelism spawn multiple copy map function call map task parallel keyvalue pair likewise spawn multiple copy reduce function call reduce task parallel unique key output map input map map map map output cat 1 dog 1 turtle 1 cat 1 belgium 1 turtle 1 dog 1 doc1 doc2 doc3 shufflesort reduce input cat 11 dog 11 turtle 11 belgium 1 reduce reduce reduce reduce mr execution hadoop v1 architecture master jobtracker accept job decompose map reduce task schedule remote execution child node slave tasktracker accept task jobtracker spawn child process actual work idea ship computation datum namenode datanode tasktracker 1 3 4 2 4 5 jobtracker accept mr job input hdfs file map task create block send node hold block execution map output write local disk jobtracker job1 job2 map task 1 map task 2 reduce task 1',\n",
              " 'reduce task 2 1 2 4 5 2 5 3 mr execution hadoop v1 architecture master jobtracker accept job decompose map reduce task schedule remote execution child node slave tasktracker accept task jobtracker spawn child process actual work idea ship computation datum namenode datanode tasktracker 1 3 4 2 4 5 jobtracker create reduce task intelligently reduce task read map output network write output hdfs jobtracker job1 job2 map task 1 map task 2 reduce task 1 reduce task 2 1 2 4 5 2 5 3 mr execution hadoop v1 architecture master jobtracker accept job decompose map reduce task schedule remote execution child node slave tasktracker accept task jobtracker spawn child process actual work idea ship computation datum namenode datanode tasktracker 1 3 4 2 4 5 load balance jobtracker monitor straggler spawn additional map datanode hold block replica whichever node complete allow proceed isare kill jobtracker job1 job2 map task 1 map task 2 reduce task 1 reduce task 2 1 2 4 5 2 5 3 mr execution hadoop v1 architecture master jobtracker accept job decompose map reduce task schedule remote execution child node slave tasktracker accept task jobtracker spawn child process actual work idea ship computation datum namenode datanode tasktracker 1 3 4 2 4 5 failure cluster 1000 node hardware failure occur frequently mechanism load balancing allow cope failure jobtracker job1 job2 map task 1 map task 2 reduce task 1 reduce task 2 1 2 4 5 2 5 3 mr scheduling caveat hadoop well run map task node input datum reside hdfs exploit datum locality node host hdfs block replicas map task input split busy case job scheduler look free map slot node rack block occasionally possible offrack node result interrack network transfer mr operation detail figure source hadoop definitive guide number map',\n",
              " 'task mapper actually equal number split split logical division input datum block physical division datum hdfs default block size default split size mr operation detail figure source hadoop definitive guide map task partition output number partition equal number reducer run late partition sort mapside partition distinct key reducer copy respective partition mapper merge big sorted file net result distribute mergesort map reduce phase mr operation detail figure source hadoop definitive guide optionally specify combine function perform partial mapside reduction aim decrease size datum transfer map key1 value1 listkey2 value2 combine key2 listvalue2 listkey2 value2 reduce key2 listvalue2 listkey3 value3 combiner example word occur document web count number occurrence document def mapdocid line word line yield word 1 def reduceword listofoccnumber yield word sumlistofoccnumber def combineword listofone yield word sumlistofone write mr programs hadoop mr write java default way write mr job java api cumbersome hadoop mr expose streaming interface allow executable write arbitrary language shell script python c implement map reduce logic case executable read input stdin write output stdout 25 reference s ghemawate h gobioff ht leung google file system httpresearchgooglecomarchivegfssosp2003pdf hdfs architecture httphadoopapacheorgdocsstable hadoopprojectdisthadoophdfshdfsdesignhtml j dean s ghemawat mapreduce simplify datum process large cluster communication acm 2008 l barroso j clidaras u hölzle datacenter computer httpwwwmorganclaypoolcomdoiabs102200 s00516ed2v01y201306cac024 t white hadoop definitive guide oreilly media 2010 26 question 27 acknowledgement base content create jan hidder stijn vansummeren 28']"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}